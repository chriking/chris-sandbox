<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//IBM//DTD DITA IBM Reference//EN" "ibm-reference.dtd">
<reference id="tpch_r_entities.dita" xml:lang="en-us">
<!--
22 Feb 18 mm 172495 SI Pro/SI name change. In Namespace in emc_si_advanced, added otherprops  to offering name variables. 
27 Feb 18 mm 173196 Name change. Updated 'Time Zone Determination' note to use SI variable. 
08 Mar 18 jw 172488 Added Flash System 9100 to 'fs_authentication'
14 June 2018 sd added DRAID support, story 173121.
26 FEB 2020 Capacity Limit Metrics Added  537_198366-->
<title>Entities that are used in the help and documentation</title>
<abstract>
<ol>
<li otherprops="5217_170692new"><keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_9100_short" /></li>
</ol>
</abstract>
<refbody>
<section><title>CONTENT COMMON TO SC AND SI</title><ph id="mcafee_atp_settings_sc">If McAfee
Adaptive Threat Protection is enabled on the server where IBM Spectrum Control is installed, it
might prevent some services from starting or stopping. To help avoid this issue, open McAfee
Adaptive Threat Protection and go to settings. In the <uicontrol>Real Protect Scanning (Windows
only)</uicontrol> section, verify if <uicontrol>Enable client-based scanning</uicontrol> is
selected. If so, select <uicontrol>Low</uicontrol> from the <uicontrol>Sensitivity level</uicontrol>
list.</ph><p><ph id="mcafee_atp_settings_si">If McAfee Adaptive Threat Protection is enabled on the
server where an IBM Storage Insights data collector is installed, it might prevent the data
collector from starting or stopping. To help avoid this issue, open McAfee Adaptive Threat
Protection and go to settings. In the <uicontrol>Real Protect Scanning (Windows only)</uicontrol>
section, verify if <uicontrol>Enable client-based scanning</uicontrol> is selected. If so, select
<uicontrol>Low</uicontrol> from the <uicontrol>Sensitivity level</uicontrol> list.</ph></p><p><ph
id="mcafee_atp_settings_ul_intro">For more information about McAfee Adaptive Threat Protection, see
the following links:</ph></p><ul id="mcafee_atp_settings_ul">
<li><xref
href="https://docs.mcafee.com/bundle/endpoint-security-10.6.0-adaptive-threat-protection-client-interface-reference-guide-windows/page/GUID-D596AE24-4E70-4B59-A069-C835EADBFC9F.html"
format="html" scope="external">Adaptive Threat Protection — Options</xref></li>
<li><xref
href="https://docs.mcafee.com/bundle/endpoint-security-10.6.0-adaptive-threat-protection-product-guide-windows/page/GUID-7257A90C-32F1-48E8-836A-EBF5FF471273.html"
format="html" scope="external">Overview of Adaptive Threat Protection</xref></li>
</ul></section>
<section>
<title>ADD STORAGE SYSTEMS DEFINITIONS BY TYPE - Common to all storage systems</title>
<dl>
<dlentry id="common_host_name_ip_address">
<dt><ph>Host names or IP addresses</ph></dt>
<dd>The host names or IP addresses that are used to connect to the storage systems. <ph
otherprops="536_195667new" id="use_host_names">Use host names if your IP addresses change regularly.
</ph>Depending on what is supported in your environment, you can enter an Internet Protocol version
4 (IPv4) or IPv6 address. If you enter an IPv6 address, the preferred representation is written as
eight groups of four hexadecimal digits. Example: 2001:DB95:0000:1234:0000:0000:5678:ABCD.</dd>
</dlentry>
<dlentry>
<dt>User name and password guidelines</dt>
<!--The DD element is the resuable part of this dl entry. It is added to the user name and password field.-->
<dd id="usrname_pwd_guidelines">
<p>The user name and password cannot contain spaces and must have at least one character. Some newer
versions of virtualizer firmware require a minimum of 8 characters for a password. The maximum
length of a user name or password is 128 characters. To create new user names and passwords, use the
following characters and symbols:<ul>
<li>A - Z (uppercase characters)</li>
<li>a - z (lowercase characters)</li>
<li>0 - 9 (numeric characters)</li>
<li>Symbols: ! # % &amp; * + - / = ? ^ _ { } ( ) . ,</li>
</ul></p>
</dd>
</dlentry>
<dlentry id="common_usr_name_pwd">
<dt>User Name and Password</dt>
<dd>The user name and password for logging in to the storage system. <ph
conref="#tpch_r_entities.dita/RequiredRoles" audience="onpremonly"/><ph
conref="#tpch_r_entities.dita/SIRequiredRoles" audience="offpremonly"/></dd>
</dlentry>
<dlentry id="common_display_name">
<dt>Display Name</dt>
<dd audience="onpremonly">The name of the storage system that is displayed in the <keyword
conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/> interface. If you do not enter a value, the
default name is provided. This is the name that is provided with the storage system.</dd>
<dd audience="offpremonly">The name of the storage system that is displayed in the <keyword
conref="fqz0_entities.dita#fqz0_entities/tpc_saas_short"/> GUI. If you do not enter a value, the
default name is provided. This is the name that is provided with the storage system.</dd>
</dlentry>
<dlentry id="common_location">
<dt>Location</dt>
<dd>The location, such as the geographical location or the building, where the resource is
located.</dd>
</dlentry>
<dlentry id="common_data_collection">
<dt>Data Collection</dt>
<dd>Schedule the probe and performance monitor for the storage system. The probe collects status,
asset, and storage information about the storage system. The performance monitor collects metrics
that measure the performance of the storage system.</dd>
</dlentry>
<dlentry id="common_probe">
<dt>Probe</dt>
<dd>Enter the time and schedule for the storage system probe to run.</dd>
<dd audience="onpremonly">Enter the time in <varname>hh:mm</varname> format, where
<varname>hh</varname> equals the hour and <varname>mm</varname> equals the minute. The time zone
that is shown is determined by the location of the Data server for <keyword
conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/>.</dd>
<dd audience="offpremonly">Enter the time in <varname>hh:mm</varname> format, where
<varname>hh</varname> equals the hour and <varname>mm</varname> equals the minute.</dd>
</dlentry>
<dlentry id="common_peformance_monitor">
<dt>Performance Monitor</dt>
<dd>Enable or disable the performance monitor for the storage system. If you enable the performance
monitor, select the schedule for how often you want performance data to be collected.</dd>
</dlentry>
</dl>
</section>
<section>Other common content<p>
<note id="time_zone_for_data_collection" othertype="Time zone determination" type="other">The time
zone of the browser that you use to access <keyword
conref="fqz0_entities.dita#fqz0_entities/ksif_short"/> determines when the data is collected and the
date and time that is shown in the chart and table views.</note>
</p><p><ph id="RequiredRoles">The role or user group that is assigned to the user name must have the
appropriate privileges to monitor the data that is collected and, if required, to change the
frequency of the data collection schedules. <xref href="fqz0_r_planning_resources_roles.dita">Learn
more about the role requirements for the user name.</xref></ph></p><p><ph id="SIRequiredRoles">The
role or user group that is assigned to the user name must have the appropriate privileges to monitor
the data that is collected and, if required, to change the frequency of the data collection
schedules. <xref href="si_required_monitoring_roles_for_collecting_data.dita">Learn more about the
role requirements for the user name.</xref></ph></p></section>
<section>
<title><tm tmtype="reg" trademark="DS8000">DS8000</tm></title>
<dl>
<dlentry id="ds8_primary_hostname_ip_addr">
<dt>Primary HMC host name or IP address</dt>
<dd>The host name or IP address of the Hardware Management Console (HMC) that is used to manage the
storage system. </dd>
</dlentry>
<dlentry id="ds8_secondary_hostname_ip_addr">
<dt>Secondary HMC host name or IP address</dt>
<dd>The host name or IP address of the secondary HMC that is used to manage the storage system. Data
entry is optional.</dd>
</dlentry>
<dlentry id="ds8_usrname_pwd">
<dt>User Name and Password</dt>
<dd>The user name and password for logging in to the storage system. This user name is the same as
the user name for the enterprise storage server network interface (ESSNI). <ph
conref="#tpch_r_entities.dita/RequiredRoles"/></dd>
</dlentry>
<dlentry id="ds8_display_name">
<dt>Display name for HMC</dt>
<dd audience="onpremonly">The name of the HMC that is displayed on the <keyword
conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/> interface.</dd>
<dd audience="offpremonly">The name of the HMC that is displayed in the <keyword
conref="fqz0_entities.dita#fqz0_entities/tpc_saas_short"/> GUI.</dd>
</dlentry>
<dlentry id="ds8_hmc_description">
<dt>Description of HMC</dt>
<dd>A description of the HMC. This description can be any combination of alphanumeric
characters.</dd>
</dlentry>
<dlentry id="ds8_safeguarded">
<dt>Safeguarded</dt>
<dd>Shows whether the volume is protected by the Safeguarded Copy feature in <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>. </dd>
</dlentry>
<dlentry id="si_ds8_sv_safeguarded">
<!--Spectrum Virtualize and DS8000-->
<dt>Safeguarded</dt>
<dd>Shows whether the volume is protected by the Safeguarded Copy feature in <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, the volume can be a source. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>, the volume can be a source or
copy. If this value is blank, the volume is not in a Safeguarded Copy relationship.<p>
<dl>
<dlentry>
<dt>Source</dt>
<dd>The volume that is protected by the Safeguarded Copy feature to prevent data loss and
corruption.</dd>
</dlentry>
<dlentry>
<dt>Copy</dt>
<dd>The volume that is used to store a point-in-time version of the data on the Safeguarded source
volume.</dd>
</dlentry>
</dl>
</p></dd>
</dlentry>
<dlentry id="ds8_safeguarded_location">
<dt>Safeguarded Location</dt>
<dd>The pool that backup copies of a volume are written to by the Safeguarded Copy feature in
<keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>.</dd>
</dlentry>
<dlentry id="si_ds8_sv_safeguarded_location">
<!--Spectrum Virtualize and DS8000-->
<dt>Safeguarded Location</dt>
<dd>
<p>The name of the child pool where the Safeguarded Copies are kept in <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>. To view more information about the pool, click
its name. This value applies only to volumes that are the source in a Safeguarded Copy
relationship.</p>
</dd>
</dlentry>
<dlentry id="ds8_safeguarded_capacity">
<!--Volume level-->
<dt>Safeguarded Capacity (GiB)</dt>
<dd>The amount of capacity that is used to store volume backups that are created by the Safeguarded
Copy feature in <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>. </dd>
</dlentry>
<dlentry id="si_ds8_sv_safeguarded_capacity">
<!--Spectrum Virtualize and DS8000-->
<dt>Safeguarded Capacity (GiB)</dt>
<dd>
<p>The capacity that is consumed by all of the Safeguarded Copies for a source volume in <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>. This value applies only to volumes that are the
source in a Safeguarded Copy relationship.</p>
</dd>
</dlentry>
<dlentry id="ds8_total_safeguarded_capacity">
<!--Pool and Block level  Total-->
<dt>Safeguarded Capacity (GiB)</dt>
<dd>The total amount of capacity that is used to store volume backups that are created by the
Safeguarded Copy feature in <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>. </dd>
</dlentry>
</dl>
</section>
<section>
<title><tm tmtype="reg" trademark="XIV">XIV</tm></title>
<dl>
<dlentry id="xiv_host_name_ip_addr">
<dt>Host names or IP addresses</dt>
<dd>The host names or IP addresses that are used to connect to the storage systems. <ph
otherprops="536_195667new">Use host names if your IP addresses change regularly.</ph> Depending on
what is supported in your environment, you can enter an Internet Protocol version 4 (IPv4) or IPv6
address. If you enter an IPv6 address, the preferred representation is written as eight groups of
four hexadecimal digits. Example: 2001:DB95:0000:1234:0000:0000:5678:ABCD.</dd>
</dlentry>
<dlentry id="xiv_usrname_pwd">
<dt>User name and Password</dt>
<dd>The user name and password for logging in to the storage system. The password can't contain this
special character: <b>&amp;</b>. <ph conref="#tpch_r_entities.dita/RequiredRoles"/></dd>
</dlentry>
</dl>
</section>
<section>
<title>SAN Volume Controller and <tm tmtype="reg" trademark="Storwize">Storwize</tm> series</title>
<dl>
<dlentry id="sv_authentication">
<dt><ph>Authentication Type</ph></dt>
<dd>The user name and password or the private Secure Shell (SSH) key that is used to connect to the
storage system.</dd>
</dlentry>
</dl>
<dl id="ssh_field_defs">
<dlentry id="sv_ssh">
<dt>Secure Shell (SSH)</dt>
<dd>An existing SSH key or a new SSH key can be used to authenticate with the storage system.</dd>
</dlentry>
</dl>
<dl id="use_existing_ssh_key">
<dlentry>
<dt>Use an existing SSH key</dt>
<dd>The SSH key that was created to authenticate with the storage system.<dl>
<dlentry id="sv_ssh_key" audience="onpremonly">
<dt>SSH key</dt>
<dd>The directory where the SSH key is stored.</dd>
<dd audience="onpremonly">The default location is <filepath>${device.conf}\tpc_svc.pem</filepath>,
which represents the <keyword conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/> default key
file <filepath>tpc_svc.pem</filepath>. The <filepath>tpc_svc.pem</filepath> file is in the
<filepath>conf</filepath> directory where the Device server is installed. </dd>
<dd audience="onpremonly">You can enter another location or select <uicontrol>Browse</uicontrol> to
search for a key file. If you select <uicontrol>Browse</uicontrol>, the following fields are
displayed:<dl audience="onpremonly">
<dlentry>
<dt>Select file</dt>
<dd>The location of the SSH key file. You can click <uicontrol>Browse</uicontrol> to search for the
file.</dd>
</dlentry>
<dlentry id="sv_ssh_username_pwd">
<dt>User Name and Password</dt>
<dd>The name and password of the user that is assigned the role of Security Administrator for the
storage system or for the cluster that contains the storage system. </dd>
<dd audience="onpremonly"><keyword conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/> uses the
user name and password values to configure the SSH key for the user that is added as an associate
user. To do this, the user must have privileges to modify other user accounts, otherwise the SSH key
cannot be configured for the associate user.</dd>
</dlentry>
<dlentry id="sv_associate_user">
<dt>Associate user</dt>
<dd>The name of the user that is associated with the SSH key. If the user name does not exist, it is
created and the user is assigned the role of Administrator for the storage system. If an existing
user name is selected, the user must have the role of Administrator for the storage system.</dd>
<dd>You can click <uicontrol>Get Users</uicontrol> to retrieve all of the existing users from the
storage system. You must select a user that belongs to the storage system administrator role.</dd>
</dlentry>
<dlentry>
<dt>Passphrase</dt>
<dd>The passphrase that is associated with the SSH key pair. If a passphrase was not created for the
SSH key pair, leave the field blank.</dd>
</dlentry>
</dl></dd>
<dd audience="onpremonly">The SSH key file is transferred from the computer where the web browser is
located to the computer where the <keyword conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/>
server is located.</dd>
</dlentry>
<dlentry id="sv_ssh_key_saas_no_assoc" audience="offpremonly">
<dt>SSH key</dt>
<dd>The directory where the SSH key is stored.</dd>
<dd>You can enter another location or select <uicontrol>Browse</uicontrol> to search for a key file.
If you select <uicontrol>Browse</uicontrol>, the following fields are displayed:<dl>
<dlentry>
<dt>Select file</dt>
<dd>The location of the SSH key file. You can click <uicontrol>Browse</uicontrol> to search for the
file.</dd>
</dlentry>
<dlentry>
<dt>Passphrase</dt>
<dd>The passphrase that is associated with the SSH key pair. If a passphrase was not created for the
SSH key pair, leave the field blank.</dd>
</dlentry>
</dl></dd>
</dlentry>
</dl></dd>
</dlentry>
</dl>
<dl id="upload_new_ssh_key">
<dlentry>
<dt>Upload a new SSH key</dt>
<dd>The new SSH key that was generated to authenticate with the storage system.<note
audience="onpremonly" othertype="Learn more" type="other">When you upload a new SSH key, you must
also provide a user name, password, and an associate user. You can also provide a passphrase for the
SSH private key if the file that is uploaded is in OpenSSH format.</note><note
audience="offpremonly" othertype="Tip" type="other">When you upload a new SSH key, you must also
provide a user name, password, and an associate user. You can also provide a passphrase for the SSH
private key if the file that is uploaded is in OpenSSH format.</note><dl>
<dlentry>
<dt>SSH key</dt>
<dd>The directory where the SSH key is stored. The valid file formats for SSH keys are OpenSSH and
PuTTY. For SSH keys that use the PuTTY file format, you cannot use a passphrase to protect the
private key.</dd>
</dlentry>
</dl></dd>
</dlentry>
</dl>
</section>
<section>
<title>FlashSystem Family</title>
<dl>
<dlentry id="fs_authentication">
<dt><ph>Authentication Type</ph></dt>
<dd>The user name and password or the private Secure Shell (SSH) key that is used to connect to the
storage system.</dd>
<dd>
<note othertype="Availability" type="other">
<keyword conref="fqz0_entities.dita#fqz0_entities/kflash_family"/> devices that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_900_short"/> devices.</note>
</dd>
</dlentry>
</dl>
</section>
<section>
<title><tm tmtype="reg" trademark="Storwize">Storwize</tm> V7000 Unified</title>
<dl>
<dlentry id="storwizev7u_use_different_authenticate_credentials">
<dt>Use different authentication credentials for file storage</dt>
<dd>The credentials for logging in to <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/> block-level data and file-level data
might be different. If so, add the authentication credentials for file storage. <note type="tip">If
you use an SSH key to log in to the file module, the user that you associate with the key must exist
on the <keyword conref="fqz0_entities.dita#fqz0_entities/kstor_file_module"><tm tmtype="reg"
trademark="Storwize">Storwize</tm> V7000 File Module</keyword>.</note></dd>
</dlentry>
</dl>
</section>
<section>
<title>SONAS</title>
<!--IBM SONAS-->
<dl>
<dlentry id="sonas_hostname_ipaddr">
<dt>Host name or IP address</dt>
<dd>The host name or IP address for the storage system. The format that is used to enter the IP
address is IPv4, for example, 127.0.0.1.<note type="restriction"><keyword
conref="fqz0_entities.dita#fqz0_entities/ksonas_short"/> does not support the IPv6 address
protocol.</note></dd>
</dlentry>
<dlentry id="sonas_data_collection">
<dt>Data Collection</dt>
<dd>Schedule the probe for the storage system. The probe collects status, asset, and storage
information about the storage system.</dd>
</dlentry>
</dl>
<dl id="use_existing_ssh_key_sonas" audience="offpremonly">
<dlentry audience="offpremonly">
<dt>Use an existing SSH key</dt>
<dd>The SSH key that was created to authenticate with the storage system.<dl>
<dlentry id="sv_saas_ssh_key" audience="offpremonly">
<dt>SSH key</dt>
<dd>The directory where the SSH key is stored.</dd>
<dd>You can enter another location or select <uicontrol>Browse</uicontrol> to search for a key file.
If you select <uicontrol>Browse</uicontrol>, the following fields are displayed:<dl>
<dlentry>
<dt>Select file</dt>
<dd>The location of the SSH key file. You can click <uicontrol>Browse</uicontrol> to search for the
file.</dd>
</dlentry>
<dlentry>
<dt>Passphrase</dt>
<dd>The passphrase that is associated with the SSH key pair. If a passphrase was not created for the
SSH key pair, leave the field blank.</dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="sv_saas_ssh_username_pwd" audience="offpremonly">
<dt>User Name and Password</dt>
<dd>The name and password of the user that is assigned the role of Security Administrator for the
storage system.</dd>
<dd audience="offpremonly"><keyword conref="fqz0_entities.dita#fqz0_entities/tpc_saas_short"/> uses
the user name and password values to configure the SSH key for the user that is added as an
associate user. To do this, the user must have privileges to modify other user accounts, otherwise
the SSH key cannot be configured for the associate user.</dd>
</dlentry>
<dlentry id="sv_saas_associate_user" audience="offpremonly">
<dt>Associate user</dt>
<dd>The name of the user that is associated with the SSH key. If the user name does not exist, it is
created and the user is assigned the role of Administrator for the storage system. If an existing
user name is selected, the user must have the role of Administrator for the storage system.</dd>
<dd>You can click <uicontrol>Get Users</uicontrol> to retrieve all of the existing users from the
storage system. You must select a user that belongs to the storage system administrator role.</dd>
</dlentry>
</dl></dd>
</dlentry>
</dl>
</section>
<section>
<title>EMC STORAGE SYSTEMS</title>
<dl>
<dlentry id="emc_new_usr_name_pwd">
<dt>User Name and Password</dt>
<dd>The account credentials that are required to connect to the <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/> storage system. <ph
conref="#tpch_r_entities.dita/RequiredRoles" audience="onpremonly"/><ph
conref="#tpch_r_entities.dita/SIRequiredRoles" audience="offpremonly"/></dd>
</dlentry>
<dlentry id="emc_unity_usr_name_pwd">
<dt>User Name and Password</dt>
<dd>The account credentials that are required to connect to the <keyword
conref="fqz0_entities.dita#fqz0_entities/kunity"/> storage system. <ph
conref="#tpch_r_entities.dita/RequiredRoles" audience="onpremonly"/><ph
conref="#tpch_r_entities.dita/SIRequiredRoles" audience="offpremonly"/></dd>
</dlentry>
<dlentry id="emc_cim_smis_provider">
<dt>SMI-S Provider host name or IP address</dt>
<dd>The IP address or host name of the server where the SMI-S Provider or Solutions Enabler that
communicates with the storage systems is installed. Depending on what is supported in your
environment, you can enter an Internet Protocol version 4 (IPv4) or IPv6 address. If you enter an
IPv6 address, the preferred representation is written as eight groups of four hexadecimal digits.
Example: 2001:DB95:0000:1234:0000:0000:5678:ABCD.</dd>
</dlentry>
<dlentry id="emc_usr_name_pwd">
<dt>User Name and Password</dt>
<dd>The account credentials that are required to connect to SMI-S Provider or Solutions Enabler. <ph
conref="#tpch_r_entities.dita/RequiredRoles" audience="onpremonly"/><ph
conref="#tpch_r_entities.dita/SIRequiredRoles" audience="offpremonly"/></dd>
</dlentry>
<dlentry id="emc_advanced">
<dt>Advanced</dt>
<dd>Expand <uicontrol>Advanced</uicontrol> to set the optional protocol, port, and namespace for
SMI-S Provider or Solutions Enabler.<dl>
<dlentry>
<dt>Protocol</dt>
<dd>The version of the cim-xml protocol, http or https. </dd>
</dlentry>
<dlentry>
<dt>Port</dt>
<dd>The port on which SMI-S Provider or Solutions Enabler is listening. By default this port is 5989
for a secure connection and 5988 for an unsecured connection. For more information about the ports
that are available, see the documentation for SMI-S Provider or Solutions Enabler.</dd>
</dlentry>
<dlentry>
<dt>Namespace</dt>
<dd>This namespace within SMI-S Provider or Solutions Enabler allows for accessing the CIM Interop
Schema (including the class instances of the Server Profile) and determines how <keyword
conref="fqz0_entities.dita#fqz0_entities/ktpc_short" audience="onpremonly"/><keyword
conref="fqz0_entities.dita#fqz0_entities/tpc_saas_short" audience="offpremonly"/> interacts with
SMI-S Provider or Solutions Enabler when retrieving information. <ph audience="onpremonly">For
information about the namespace to use, see <image href="sout.gif" placement="inline"><alt>External
link icon</alt></image>
<xref keyref="sc_support_matrix_emc_storage"/>. </ph><p audience="onpremonly">To view a list of
default namespaces for SMI-S providers for storage systems that is included with <keyword
conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/>, open the following file in a text editor:
<codeph><i>install_directory</i>/data/config/namespace.config</codeph>, where
<codeph><i>install_directory</i></codeph> represents the directory where the product is
installed.</p><p>Check the documentation for SMI-S Provider or Solutions Enabler or contact the
storage system vendor to ensure that you use the most current namespaces.</p></dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="emc_si_advanced">
<dt>Advanced</dt>
<dd>Expand <uicontrol>Advanced</uicontrol> to set the optional protocol, port, and namespace for
SMI-S Provider or Solutions Enabler.<dl>
<dlentry>
<dt>Protocol</dt>
<dd>The version of the cim-xml protocol, http or https. </dd>
</dlentry>
<dlentry>
<dt>Port</dt>
<dd>The port on which SMI-S Provider or Solutions Enabler is listening. By default this port is 5989
for a secure connection and 5988 for an unsecured connection. For more information about the ports
that are available, see the documentation for SMI-S Provider or Solutions Enabler.</dd>
</dlentry>
<dlentry>
<dt>Namespace</dt>
<dd>This namespace within SMI-S Provider or Solutions Enabler allows for accessing the CIM Interop
Schema (including the class instances of the Server Profile) and determines how <keyword
conref="fqz0_entities.dita#fqz0_entities/ktpc_short" audience="onpremonly"/><keyword
conref="fqz0_entities.dita#fqz0_entities/tpc_saas_short" audience="offpremonly"/> interacts with
SMI-S Provider or Solutions Enabler when retrieving information. <p>Check the documentation for
SMI-S Provider or Solutions Enabler or contact the storage system vendor to ensure that you use the
most current namespaces.</p></dd>
</dlentry>
</dl></dd>
</dlentry>
</dl>
</section>
<section>
<title>HITACHI VSP SYSTEMS</title>
<dl>
<dlentry id="hvsp_cs_usr_name_pwd">
<dt>User Name and Password</dt>
<dd>The account credentials that are required to connect to <keyword
conref="fqz0_entities.dita#fqz0_entities/khitachi"/> Command Suite. <ph
conref="#tpch_r_entities.dita/RequiredRoles" audience="onpremonly"/><ph
conref="#tpch_r_entities.dita/SIRequiredRoles" audience="offpremonly"/></dd>
</dlentry>
<dlentry id="hvsp_dm_usr_name_pwd">
<dt>User Name and Password</dt>
<dd>The account credentials that are required to connect to <keyword
conref="fqz0_entities.dita#fqz0_entities/khitachi"/> Device Manager. <ph
conref="#tpch_r_entities.dita/RequiredRoles" audience="onpremonly"/><ph
conref="#tpch_r_entities.dita/SIRequiredRoles" audience="offpremonly"/></dd>
</dlentry>
</dl>
</section>
<section>
<title>STORAGE SYSTEMS CONNECTED TO WITH CIM ADDRESSES</title>
<dl>
<dlentry id="cim_agentname_ip_addr">
<dt>SMI-S provider host name or IP address</dt>
<dd>The host name or IP address for the computer on which the SMI-S provider that manages the
storage system is installed. Depending on what is supported in your environment, you can enter an
Internet Protocol version 4 (IPv4) or IPv6 address. If you enter an IPv6 address, the preferred
representation is written as eight groups of four hexadecimal digits. Example:
2001:DB95:0000:1234:0000:0000:5678:ABCD.</dd>
</dlentry>
<dlentry id="cim_usrname_pwd">
<dt>User Name and Password</dt>
<dd conref="#tpch_r_entities.dita/usrname_pwd_guidelines"/>
</dlentry>
</dl>
</section>
<section>
<title>NetApp FILERS MANAGED BY SRAs in RXA MODE</title>
<dl>
<dlentry id="filers_managed_by_rxa_sras">
<dt><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> filers that are managed by Storage
Resource agents in RXA mode</dt>
<dd>
<dl>
<dlentry>
<dt>Host name or IP address</dt>
<dd>The host name or IP address for the computer on which the Storage Resource agent is
installed.</dd>
</dlentry>
<dlentry>
<dt>User Name and Password</dt>
<dd>The user ID and password for logging in to the computer on which the Storage Resource agent is
installed.</dd>
</dlentry>
<dlentry>
<dt>Advanced</dt>
<dd>You can modify the certificate location and passphrase for the Storage Resource agent.<dl>
<dlentry>
<dt>Certificate location</dt>
<dd>The fully qualified path of the certificate file for the Storage Resource agent. For example,
<filepath>TPC_install_directory/data/sra/operating_system/certs/sra.pem</filepath>.This file is on
the computer where the <keyword conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/> Data server
is located.</dd>
</dlentry>
</dl><dl>
<dlentry>
<dt>Passphrase</dt>
<dd>The passphrase for the certificate file. The passphrase was created when the certificate was
generated.</dd>
</dlentry>
</dl></dd>
</dlentry>
</dl>
</dd>
</dlentry>
</dl>
</section>
<section>
<title><keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/></title>
<dl>
<dlentry id="elastic_storage_user_name">
<dt>User name</dt>
<dd>The user name that is used to log on to the cluster node. To run administration commands for
monitoring the storage system, the user must have the following privileges:<ul>
<li>The user must have root privileges or privileges to run a set of specified administration
commands using the <codeph>sudo</codeph> command on the cluster node.</li>
<li>The user must have permission to log on from the specified node without entering a password to
the other nodes in the cluster.</li>
</ul></dd>
<dd>
<note type="tip" id="ss_dedicated_user" audience="offpremonly">Why not create a dedicated user
account to manage the monitoring of your storage systems more efficiently? So, instead of using an
existing user account, create a new user account to connect to and collect metadata from your
storage resources.</note>
</dd>
</dlentry>
</dl>
</section>
<!--Reusable definitions for data collected by the probe for storage system resources.-->
<section>
<title>BLOCK STORAGE SYSTEMS</title>
<dl>
<dlentry id="ssysAllocatedSpaceGiB">
<dt>Used Capacity (GiB)</dt>
<dd id="ssys_allocated_space">(Previously known as Allocated Space) The amount of space that is used
by the standard- and thin-provisioned volumes in the pools. If the pool is a parent pool, the amount
of space that is used by the volumes in the child pools is also calculated.</dd>
<dd>The capacity that is used by for thin-provisioned volumes is less than their provisioned
capacity, which is shown in the <uicontrol>Provisioned Capacity (GiB)</uicontrol> column. If a pool
doesn't have thin-provisioned volumes, the value for used capacity is the same as the value for
provisioned capacity.</dd>
<!--
<dd>Allocated space is the same as used space on all storage systems with the following exceptions:<ul>
<li><keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/> storage systems that are thin
provisioned</li>
<li><keyword conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that
are configured with block storage and that are thin provisioned</li>
</ul></dd>
-->
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_assigned_volume_space">
<dt>Mapped Capacity (GiB)</dt>
<dd>(Previously known as Assigned Volume Space) The total volume space in the storage system that is
mapped or assigned to host systems, including child pool capacity.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_available_pool_space">
<dt>Available Capacity (GiB)</dt>
<dd>(Previously known as Available Pool Space) The total amount of the space in the pools that is
not used by the volumes in the pools. To calculate available capacity, the following formula is
used:</dd>
<dd>
<codeblock>(pool capacity - used capacity)</codeblock>
</dd>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>, pool capacity is the physical
capacity of the pools and does not include the provisioned capacity of the pools. </dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_compression_savings">
<dt>Compression Savings (%)</dt>
<dd>The estimated amount and percentage of capacity that is saved by using data compression, across
all pools on the storage system. The percentage is calculated across all compressed volumes in the
pools and does not include the capacity of non-compressed volumes.</dd>
<dd>For storage systems with drives that use inline data compression technology, the Compression
Savings does not include the capacity savings that are achieved at the drive level. </dd>
<dd>The following formula is used to calculate the amount of storage space that is saved:
<codeblock>written capacity − compressed size</codeblock></dd>
<dd>The following formula is used to calculate the percentage of capacity that is
saved:<codeblock>((written capacity − compressed size) ÷ written capacity) × 100</codeblock></dd>
<dd>For example, the written capacity, which is the amount of data that is written to the volumes
before compression, is 40 GiB. The compressed size, which reflects the size of compressed data that
is written to disk, is just 10 GiB. Therefore, the compression savings percentage across all
compressed volumes is 75%.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/> storage systems with firmware version 11.6 or
later, and resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.<p>For <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, all volumes in the pools are
compressed.</p></note>
</dd>
</dlentry>
<dlentry id="ssys_l2_drive_compression_savings">
<dt otherprops="536_183345new">Drive Compression Savings (%)</dt>
<dd>The amount and percentage of capacity that is saved with drives that use inline data compression
technology. The percentage is calculated across all compressed drives in the pools.</dd>
<dd>The amount of storage space that is saved is the sum of drive compression savings.</dd>
<dd>The following formula is used to calculate the percentage of capacity that is
saved:<codeblock>((used written capacity − compressed size) ÷ used written capacity) × 100</codeblock></dd>
<dd>
<note othertype="Availability" type="other">Storage systems that contain <tm
trademark="IBM FlashCore" tmtype="reg">IBM FlashCore</tm> Modules with hardware compression.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_custom_tags">
<dt>Custom Tag 1,2, and 3</dt>
<dd>Any user-defined text that is associated with a storage system. This text can be included as a
report column when you generate reports for the storage system.</dd>
</dlentry>
<dlentry id="sys_l2_deduplication_savings">
<dt>Deduplication Savings (%)</dt>
<dd>The estimated amount and percentage of capacity that is saved by using data deduplication,
across all data reduction pools on the storage system. The percentage is calculated across all
deduplicated volumes in the pools and does not include the capacity of volumes that are not
deduplicated.</dd>
<dd>The following formula is used to calculate the amount of storage space that is saved:
<codeblock>written capacity − deduplicated size</codeblock></dd>
<dd>The following formula is used to calculate the percentage of capacity that is
saved:<codeblock>((written capacity − deduplicated size) ÷ written capacity) × 100</codeblock></dd>
<dd>For example, the written capacity, which is the amount of data that is written to the volumes
before deduplication, is 40 GiB. The deduplicated size, which reflects the size of deduplicated data
that is written to disk, is just 10 GB. Therefore, data deduplication reduced the size of the data
that is written by 75%.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and resources that run
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> 8.1.3 or later.</note>
</dd>
</dlentry>
<dlentry id="ssys_effective_capacity">
<dt>Written Capacity Limit (GiB)</dt>
<dd>(Previously known as Effective Capacity) The maximum of amount of capacity that can be written
to a pool before inline-disk compression is applied. If a pool is not compressed, this value is the
same as Capacity.</dd>
</dlentry>
<dlentry id="ssys_effective_used_capacity">
<dt>Used Written Capacity (GiB)</dt>
<dd>(Previously known as Effective Used Capacity) The amount of capacity that is written to the
volumes in a pool before inline disk compression is applied. If a pool is not compressed, this value
is the same as Used Capacity.</dd>
</dlentry>
<dlentry id="ssys_effective_used_capacity_percentage">
<dt>Used Written Capacity (%)</dt>
<dd>(Previously known as Effective Used Capacity) For devices with inline hardware compression, the
effective used capacity percentage is the percentage of capacity that is provisioned to the
standard-provisioned volumes and the thin-provisioned volumes, given the drive compression
savings.</dd>
</dlentry>
<dlentry id="ssys_effective_available_capacity">
<dt>Available Written Capacity (GiB)</dt>
<dd>(Previously known as Effective Used Capacity) The amount of capacity that can be written to the
pools before inline compression is applied. If the pools are not compressed, this value is the same
as Available Capacity.<note type="other" othertype="Important">Because data compression is very
efficient, a pool can run out of Available Written Capacity while physical capacity is still
available. To stay aware of your capacity needs, monitor this value and Available
Capacity.</note></dd>
</dlentry>
<dlentry id="ssys_l2_firmware">
<dt>Firmware</dt>
<dd>
<p>The firmware version of the microcode on a storage system. For the DS-series of storage systems,
this value represents the SEA version of the firmware.</p>
<p>To view information about the code bundles for the firmware versions of the DS-series, go to the
<xref format="html" href="http://www.ibm.com/support/entry/portal/Overview" scope="external"><tm
tmtype="reg" trademark="IBM">IBM</tm> support site</xref> at
http://www.ibm.com/support/entry/portal/Overview and search for <varname>code bundle
information</varname>. An internet connection is required to access the support site.</p>
</dd>
</dlentry>
<dlentry id="ssys_l2_last_successful_probe">
<dt>Last Successful Probe</dt>
<dd>The most recent date and time when the probe successfully completed the collection of asset and
status data.</dd>
</dlentry>
<dlentry id="ssys_mirrored_volumes">
<dt>Mirrored Volumes</dt>
<dd>The number of volumes that are used for VDisk mirroring. The number includes target volumes or
secondary volumes of mirrored volumes in storage virtualizer pools. The mirrored volumes are in
pools in <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.<note othertype="Learn more" type="other">To view more information
about the volumes, click the number that is shown in the column.</note>
</dd>
</dlentry>
<dlentry id="sc_ssys_monthly_growth_percent" audience="onpremonly" otherprops="536_196547">
<dt>Recent Fill Rate (%)</dt>
<dd>
<p>The rate at which the capacity of the storage system is being consumed over the last 30 days. Use
this value to see how quickly your storage systems are filling up. </p>
<p><image placement="break" href="mgr_ssystem_monthy_growth_rate_percentage_illus.svg">
<alt>Fill rate for storage systems</alt>
</image>If 30 days of historical data is not available, it is the difference between today's fill
rate % and the oldest value for the fill rate % in the last 30 days.</p>
</dd>
<dd>If you want to add the recent growth rate in GiB, right-click any column heading and click
<uicontrol>Recent Growth (GiB)</uicontrol>. </dd>
<dd>
<note type="tip">To see the recent fill rate % for the pools in the storage system, double-click the
storage system, and click <uicontrol>View capacity by pool</uicontrol> on the
<wintitle>Overview</wintitle> page.</note>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry audience="offpremonly" otherprops="536_196547" id="si_ssys_monthly_growth_percent">
<dt>Recent Fill Rate (%)</dt>
<dd>
<p>The rate at which the capacity of the storage system is being consumed over the last 30 days. Use
this value to see how quickly your storage systems are filling up. </p>
<p><image placement="break" href="mgr_ssystem_monthy_growth_rate_percentage_illus.svg">
<alt>Fill rate for storage systems</alt>
</image>If 30 days of historical data is not available, it is the difference between today's fill
rate % and the oldest value for the fill rate % in the last 30 days.</p>
</dd>
<dd>If you want to add the growth rate in GiB, right-click any column heading and click
<uicontrol>Recent Growth (GiB)</uicontrol>. </dd>
<dd>
<p>To see the <uicontrol>Recent Fill Rate (%)</uicontrol> for the pools in <keyword
conref="fqz0_entities.dita#fqz0_entities/tpc_saas"/>, double-click the storage system, and click
<uicontrol>View capacity by pool</uicontrol> on the <wintitle>Overview</wintitle> page. </p>
<p>To see the <uicontrol>Recent Fill Rate (%)</uicontrol> for the pools in <keyword
conref="fqz0_entities.dita#fqz0_entities/ksif_short"/>, double-click the storage system, and click
<uicontrol>View capacity by pool</uicontrol> in the <wintitle>Capacity</wintitle> section of the
<uicontrol>Overview</uicontrol> tab.</p>
</dd>
<dd>
<note type="other" othertype="Did you know"><imagemap id="imagemap_r3g_kcf_3kb">
<image href="mgr_mthly_growth_rates_create_reports_illus.svg" alt="Create reports"
placement="inline"/>
<area>
<shape translate="no">rect</shape>
<coords translate="no">15, 90, 461, 279</coords>
<xref href="rpt_predefined_capacity_reports_steps.dita" format="dita"/>
</area>
<area>
<shape translate="no">rect</shape>
<coords translate="no">504, 88, 878, 282</coords>
<xref href="rpt_predefined_inventory_reports_steps.dita" format="dita"/>
</area>
</imagemap><keyword conref="fqz0_entities.dita#fqz0_entities/ksif_short"/> users can now also create
capacity reports for block storage systems and pools, and inventory reports for block storage
systems. Try it out! From the menu, click <menucascade><uicontrol>Reports</uicontrol></menucascade>
and pick a report.</note>
</dd>
</dlentry>
<dlentry id="sc_ssys_monthly_growth_GiB" audience="onpremonly" otherprops="536_196547">
<dt>Recent Growth (GiB)</dt>
<dd>
<p>The amount of used capacity that is consumed by the storage system over the last 30 days. Use
this value to identify the storage systems with the highest growth rates in used capacity.</p>
<p>Recent growth is the difference between today's used capacity of the storage system and the used
capacity of the storage system that was reported 30 days ago. If 30 days of historical data is not
available, it is the difference between today's used capacity for the storage system and the oldest
value for the used capacity of the storage system that was reported in the last 30 days. </p>
</dd>
<dd>
<note type="tip">To see the growth in used capacity for the pools in the storage system,
double-click the storage system, and click <uicontrol>View capacity by pool</uicontrol> on the
<wintitle>Overview</wintitle> page.</note>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry audience="offpremonly" otherprops="536_196547" id="si_ssys_monthly_growth_GiB">
<dt>Recent Growth (GiB)</dt>
<dd>
<p>The amount of used capacity that is consumed by the storage system over the last 30 days. Use
this value to identify the storage systems with the highest growth rates in used capacity.</p>
<p>Recent growth is the difference between today's used capacity of the storage system and the used
capacity of the storage system that was reported 30 days ago. If 30 days of historical data is not
available, it is the difference between today's used capacity for the storage system and the oldest
value for the used capacity of the storage system that was reported in the last 30 days.</p>
</dd>
<dd>
<p>To see the growth in used capacity for the pools in <keyword
conref="fqz0_entities.dita#fqz0_entities/tpc_saas"/>, double-click the storage system, and click
<uicontrol>View capacity by pool</uicontrol> on the <wintitle>Overview</wintitle> page. </p>
<p>To see the growth in used capacity for the pools in <keyword
conref="fqz0_entities.dita#fqz0_entities/ksif_short"/>, double-click the storage system, and click
<uicontrol>View capacity by pool</uicontrol> in the <wintitle>Capacity</wintitle>section of the
<uicontrol>Overview</uicontrol> tab.</p>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry id="ssys_pools_monthly_growth_percent" otherprops="536_196547">
<dt>Recent Fill Rate (%)</dt>
<dd>
<p>The rate at which the capacity of the pool is being consumed over the last 30 days. Use this
value to see how quickly your pools are filling up. </p>
<p><image placement="break" href="mgr_pool_monthy_growth_rate_percentage_illus.svg">
<alt>Fill rate for pools</alt>
</image>If 30 days of historical data is not available, it is the difference between today's fill
rate % and the oldest value for the fill rate % in the last 30 days.</p>
</dd>
<dd>If you want to add the growth in used capacity in GiB, right-click any column heading and click
<uicontrol>Recent Growth (GiB)</uicontrol>.</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry id="ssys_pools_monthly_growth_gib">
<dt>Recent Growth (GiB)</dt>
<dd>
<p>The amount of used capacity that is consumed by the storage system over the last 30 days. Use
this value to identify the pools with the highest growth rates in used capacity.</p>
<p>Recent growth is the difference between today's used capacity of the pool and the used capacity
of the pool that was reported 30 days ago. If 30 days of historical data is not available, it is the
difference between today's used capacity for the pool and the oldest value for the used capacity of
the pool that was reported in the last 30 days.</p>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry otherprops="537_198366" id="capacity_limit">
<dt>Capacity Limit (%) and Capacity Limit (GiB)</dt>
<dd>
<p>The limit that was set on the capacity that is used by your storage systems. For example, the
policy of your company is to keep 20% of the usable capacity of your storage systems in reserve. So,
you log into the GUI as Administrator and set the capacity limit to 80%.</p>
<p>
<imagemap id="imagemap_yv3_nk4_vkb">
<image href="mgr_capacity_limit_infographic.svg" placement="inline"><alt>Set the capacity
limit</alt></image>
<area>
<shape translate="no">rect</shape>
<coords translate="no">2, 1, 699, 193</coords>
<xref href="mgr_set_capacity_limit.dita" format="dita">Set the capacity limit</xref>
</area>
</imagemap>
</p>
<p><image href="idea_32.svg" placement="inline"><alt>Tip</alt></image> Click the illustration above
to find out how to set capacity limits.</p>
<p>The GiB value for the capacity limit for the storage system is calculated when you set the value
for the <uicontrol>Capacity Limit (%)</uicontrol>. </p>
<p>To add the <uicontrol>Capacity Limit (%)</uicontrol> and the <uicontrol>Capacity Limit
(GiB)</uicontrol> columns, right-click any column heading on the <wintitle>Block Storage
Systems</wintitle> page.</p>
<p>See these related values for more information <uicontrol>Adjusted Used Capacity (%)</uicontrol>
and <uicontrol>Capacity-to-Limit (GiB)</uicontrol>.</p>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry otherprops="537_198366" id="adjusted_used_capacity">
<dt>Adjusted Used Capacity (%)</dt>
<dd>
<p>The amount of capacity that can be used without exceeding the capacity limit.</p>
<p>
<image placement="break" href="mgr_capacity_adjusted_used_capacity.svg">
<alt>Adjusted Used Capacity</alt>
</image>
</p>
<p>The formula for calculating <uicontrol>Adjusted Used Capacity (%)</uicontrol> is
<equation-inline>(Used Capacity in GiB/Capacity Limit in GiB )*100</equation-inline>. For example,
if the capacity is 100 GiB, the used capacity is 40 GiB, and the capacity limit is 80% or 80 GiB,
then the value for <uicontrol>Adjusted Used Capacity (%)</uicontrol> is <equation-inline>(40 GiB/80
GiB )* 100</equation-inline> or 50%. So, in this example, you can use 30% or 40 GiB of the usable
capacity of the resource before you reach the capacity limit.</p>
<p>If the used capacity exceeds the capacity limit, the value for <uicontrol>Adjusted Used Capacity
(%)</uicontrol> is over 100%.</p>
<p>To add the <uicontrol>Adjusted Used Capacity (%)</uicontrol> column, right-click any column
heading on the <wintitle>Block Storage Systems</wintitle> page.</p>
<p>See these related values for more information <uicontrol>Capacity Limit (%)</uicontrol>, and
<uicontrol>Capacity-to-Limit (GiB)</uicontrol>.</p>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry otherprops="537_198366" id="capacity_to_limit">
<dt>Capacity-to-Limit (GiB)</dt>
<dd>
<p>The amount of capacity that is available before the capacity limit is reached. </p>
<p>
<image placement="break" href="mgr_capacity_to_limit_infographic.svg">
<alt>Capacity-to-limit</alt>
</image>
</p>
<p>The formula for calculating <uicontrol>Capacity-to-Limit (GiB)</uicontrol> is
<equation-inline>(Capacity Limit in GiB - Used Capacity in GiB)</equation-inline>. For example, if
the capacity limit is 80% or 80 GiB and the used capacity is 40 GiB, then the value for
<uicontrol>Capacity-to-Limit (GiB)</uicontrol> is <equation-inline>(80 GiB - 40 GiB or 80% -
50%)</equation-inline> which is 30% or 40 GiB.</p>
<p>See these related values for more information <uicontrol>Capacity Limit (%)</uicontrol> and
<uicontrol>Adjusted Used Capacity (%)</uicontrol>.</p>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>. </note>
</dd>
</dlentry>
<dlentry otherprops="537_198366" id="capacity_limit_pools">
<dt>Capacity Limit (%) and Capacity Limit (GiB)</dt>
<dd>
<p>The limit that was set on the capacity that is used by your pools. For example, the policy of
your company is to keep 20% of the usable capacity of your pools in reserve. So, you log into the
GUI as Administrator and set the capacity limit of your pools to 80%.</p>
<p>
<imagemap id="imagemap_bsr_mdz_vkb">
<image href="mgr_capacity_limit_infographic.svg" placement="inline">
<alt>Set the capacity limit</alt>
</image>
<area>
<shape translate="no">rect</shape>
<coords translate="no">2, 1, 699, 193</coords>
<xref href="mgr_set_capacity_limit.dita" format="dita">Set the capacity limit</xref>
</area>
</imagemap>
</p>
<p><image href="idea_32.svg" placement="inline"><alt>Tip</alt></image> Click the illustration above
to find out how to set capacity limits.</p>
<p>The GiB value for the capacity limit for the pool is calculated when you set the value for the
<uicontrol>Capacity Limit (%)</uicontrol>. </p>
<p>To add the <uicontrol>Capacity Limit (%)</uicontrol> and the <uicontrol>Capacity Limit
(GiB)</uicontrol> columns, right-click any column heading on the <wintitle>Pools</wintitle>
page.</p>
<p>See these related values for more information <uicontrol>Adjusted Used Capacity (%)</uicontrol>
and <uicontrol>Capacity-to-Limit (GiB)</uicontrol>.</p>
<p>
<note type="other" othertype="Zero capacity">When you set the capacity limit for pools, the values
shown for <uicontrol>Zero Capacity</uicontrol> are readjusted to take into account the capacity
limit of the pool. The date will represent when the capacity limit of the pool is reached. If the
pool has already reached the capacity limit, Depleted is shown. None is shown when a trend in
storage consumption can't be detected because the pool's storage isn't being consumed or because not
enough data was collected to predict storage consumption.</note>
</p>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry otherprops="537_198366" id="adjusted_used_capacity_pools">
<dt>Adjusted Used Capacity (%)</dt>
<dd>
<p>The amount of capacity that can be used without exceeding the capacity limit.</p>
<p>
<image placement="break" href="mgr_capacity_adjusted_used_capacity.svg">
<alt>Adjusted Used Capacity</alt>
</image>
</p>
<p>The formula for calculating <uicontrol>Adjusted Used Capacity (%)</uicontrol> is
<equation-inline>(Used Capacity in GiB/Capacity Limit in GiB )*100</equation-inline>. For example,
if the capacity is 100 GiB, the used capacity is 40 GiB, and the capacity limit is 80% or 80 GiB,
then the value for <uicontrol>Adjusted Used Capacity (%)</uicontrol> is <equation-inline>(40 GiB/80
GiB )* 100</equation-inline> or 50%. So, in this example, you can use 30% or 40 GiB of the usable
capacity of the resource before you reach the capacity limit.</p>
<p>If the used capacity exceeds the capacity limit, the value for <uicontrol>Adjusted Used Capacity
(%)</uicontrol> is over 100%.</p>
<p>To add the <uicontrol>Adjusted Used Capacity (%)</uicontrol> column, right-click any column
heading on the <wintitle>Pools</wintitle> page.</p>
<p>See these related values for more information <uicontrol>Capacity Limit (%)</uicontrol> and
<uicontrol>Capacity-to-Limit (GiB)</uicontrol>.</p>
</dd>
<dd>
<note othertype="Availability" type="other">This metric is not available for all storage systems,
such as <keyword conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_performance_monitor_status">
<dt>Performance Monitor Status</dt>
<dd>The status value indicates whether the performance monitor was successful, or unsuccessful, or
generated warning messages during the collection of performance data for the resource. One of the
following status values might be displayed: <dl>
<dlentry>
<dt>Starting</dt>
<dd>The performance monitor is starting.</dd>
</dlentry>
<dlentry>
<dt>Running</dt>
<dd>The performance monitor is running.</dd>
</dlentry>
<dlentry>
<dt>Running with problems</dt>
<dd>The performance monitor is running, but encountered warning conditions during processing. Check
the log to view the warning messages.</dd>
</dlentry>
<dlentry>
<dt>Stopping</dt>
<dd>The performance monitor is stopping.</dd>
</dlentry>
<dlentry>
<dt>Completed</dt>
<dd>The performance monitor completed data collection.</dd>
</dlentry>
<dlentry>
<dt>Completed with warnings</dt>
<dd>The performance monitor completed, but encountered warning conditions during processing. Check
the log to view the warning messages.</dd>
</dlentry>
<dlentry>
<dt>Failed</dt>
<dd>The performance monitor encountered error conditions during processing and is no longer running.
Check the log of a performance monitor to view its error messages.</dd>
</dlentry>
<dlentry>
<dt>Canceled</dt>
<dd>The performance monitor was stopped and is no longer collecting performance data.</dd>
</dlentry>
<dlentry>
<dt>Not running</dt>
<dd>The performance monitor is not running.</dd>
</dlentry>
<dlentry>
<dt>Disabled</dt>
<dd>The performance monitor was never run. To enable a performance monitor, go to the list page for
the resource and modify its data collection. <p>For example, for a performance monitor for a storage
system, go to the <wintitle>Storage Systems</wintitle> page, right-click the storage system, and
select <menucascade><uicontrol>Data Collection</uicontrol>
<uicontrol>Schedule</uicontrol></menucascade>. On the <wintitle>Data Collection Schedule</wintitle>
window, select <uicontrol>Enabled</uicontrol>. When enabled, the performance monitor runs according
to the defined interval.</p></dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="ssys_l2_physical_allocation">
<dt>Used Capacity (%)</dt>
<dd>
<p>(Previously known as Physical Allocation)</p>
<p>
<image href="artcomp-ssysphysalloc-001.svg" placement="break"
alt="Illustration of used capacity percentage"/>
</p>
<p>The percentage of physical capacity in the pools that is used by the standard-provisioned
volumes, the thin-provisioned volumes, and the volumes in child pools. Check the value for used
capacity percentage to see:<ul>
<li>Whether the physical capacity of the pools is fully allocated. That is, the value for used
capacity is 100%.</li>
<li>Whether you have sufficient capacity to provision new volumes with storage</li>
<li>Whether you have sufficient capacity to allocate to the compressed and thin-provisioned volumes
in the pools</li>
</ul></p>
<p otherprops="tvt"><i><b>TVT Instructions</b>: The source file for this graphic which contains text
for translation is <filepath>artcomp-ssysphysalloc-001.svg</filepath> .</i></p>
</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_saas_l2_physical_allocation">
<dt>Used Capacity (%)</dt>
<dd>
<p>(Previously known as Physical Allocation) </p>
<p>
<image href="artcomp-ssysphysalloc-001.svg" placement="break"
alt="Illustration of used capacity percentage"/>
</p>
<p>The percentage of physical capacity in the pools that is used by the standard-provisioned
volumes, the thin-provisioned volumes, and the volumes in child pools. Check the value for used
capacity percentage to see:<ul>
<li>Whether the physical capacity of the pools is fully allocated. That is, the value for used
capacity is 100%.</li>
<li>Whether you have sufficient capacity to provision new volumes with storage</li>
<li>Whether you have sufficient capacity to allocate to the compressed and thin-provisioned volumes
in the pools</li>
</ul></p>
<p otherprops="tvt"><i><b>TVT Instructions</b>: The source file for this graphic which contains text
for translation is <filepath>artcomp-ssysphysalloc-001.svg</filepath> .</i></p>
</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_pool_capacity">
<dt>Capacity (GiB)</dt>
<dd id="pool_capacity_ss">(Previously known as Pool Capacity) The total amount of storage space in
the pools. For <tm trademark="XIV" tmtype="reg">XIV</tm> systems and <tm trademark="IBM Spectrum"
tmtype="reg">IBM Spectrum</tm> Accelerate, capacity represents the physical ("hard") capacity of the
pool, not the provisioned ("soft") capacity. Pools that are allocated from other pools are not
included in the total pool space. </dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_pool_shortfall">
<dt><!--Pool -->Shortfall (%)</dt>
<dd>The percentage of space that is over committed to the pools with thin-provisioned volumes. For
example, you commit 100 GiB of space to a thin-provisioned volume in a pool with a capacity of 50
GiB. As the space is used by the thin-provisioned volume in increments of 10 GiB, the space
available for allocation decreases and the shortfall in capacity becomes more acute.</dd>
<dd>To calculate the shortfall, the following formula is
used:<codeblock>[(overprovisioned capacity ÷ committed but available capacity) × 100] </codeblock></dd>
<dd>A <!--pool -->shortfall occurs when you commit more space to the volumes in the pools than is
physically available to the pools. If the physical space available to the pools is less than the
committed provisioned capacity, then the pools do not have enough space to fulfill the commitment to
the provisioned capacity.</dd>
<dd>For example, the physical capacity of the pools is 70 GiB, but 150 GiB of provisioned capacity
was committed to the thin-provisioned volumes. If the volumes are using 50 GiB, 100 GiB is committed
to those volumes (150 GiB − 50 GiB) with only 20 GiB of available pool space (70 GiB − 50 GiB).
Because only 20 GiB of the pool space is available, 80 GiB of the committed space cannot be
allocated (100 GiB - 20 GiB). In this case, the percentage of committed space that is unavailable is
80% [(80 GiB ÷ 100 GiB × 100].</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kvnx"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kvnxe"/> storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_full"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage, <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>, and
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_saas_l2_pool_shortfall">
<dt><!--Pool -->Shortfall (%)</dt>
<dd>The percentage of space that is over committed to the pools with thin-provisioned volumes. For
example, you commit 100 GiB of space to a thin-provisioned volume in a pool with a capacity of 50
GiB. As the space is used by the thin-provisioned volume in increments of 10 GiB, the space
available for allocation decreases and the shortfall in capacity becomes more acute.</dd>
<dd>To calculate the shortfall, the following formula is
used:<codeblock>[(overprovisioned capacity ÷ committed but available capacity) × 100] </codeblock></dd>
<dd>A <!--pool -->shortfall occurs when you commit more space to the volumes in the pools than is
physically available to the pools. If the physical space available to the pools is less than the
committed provisioned capacity, then the pools do not have enough space to fulfill the commitment to
the provisioned capacity.</dd>
<dd>For example, the physical capacity of the pools is 70 GiB, but 150 GiB of provisioned capacity
was committed to the thin-provisioned volumes. If the volumes are using 50 GiB, then 100 GiB is
still committed to those volumes (150 GiB − 50 GiB) with only 20 GiB of available pool space (70 GiB
− 50 GiB). Because only 20 GiB of the pool space is available, 80 GiB of the committed space cannot
be allocated (100 GiB - 20 GiB). In this case, the percentage of committed space that is unavailable
is 80% [(80 GiB ÷ 100 GiB × 100].</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage, <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>,
<keyword conref="fqz0_entities.dita#fqz0_entities/kemc"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/kvmax"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kvnx"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kvnxe"/> storage systems, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_probe_status">
<dt>Probe Status</dt>
<dd>The status of the most recent run of a probe. Use this value to quickly identify a probe that
failed or generated warning messages during processing. One of the following status values might be displayed:<dl>
<dlentry>
<dt>Failed</dt>
<dd>The probe encountered error conditions during processing and did not complete the monitoring
action. Check the log to view the error messages.</dd>
</dlentry>
<dlentry>
<dt>Warning</dt>
<dd>The probe encountered warning conditions during processing, but still completed the monitoring
action. Check the log to view the warning messages.</dd>
</dlentry>
<dlentry>
<dt>Running</dt>
<dd>The probe is running.</dd>
</dlentry>
<dlentry>
<dt>Successful</dt>
<dd>The probe completed the monitoring action and encountered no warning or error conditions.</dd>
</dlentry>
<dlentry>
<dt>Never probed</dt>
<dd>The resource was never probed.</dd>
</dlentry>
</dl>If a probe is run while a resource is unreachable, the status of the probe is not shown in the
column. Instead, the status of the previous probe is shown.</dd>
</dlentry>
<dlentry id="ssys_l2_raw_disk_capacity">
<dt>Raw Capacity (GB)</dt>
<dd>(Previously known as Raw Disk Capacity) The total raw (unformatted) disk capacity of a storage
system. The capacity of managed disks and external disks for storage virtualizers is included in the
calculation. The capacity of spare disks that are identified on <keyword
conref="fqz0_entities.dita#fqz0_entities/kds_full"/> storage systems is not included in the
calculation. </dd>
</dlentry>
<dlentry id="ssys_l2_read_cache_memory">
<dt>Read Cache</dt>
<dd>The amount of reach cache memory that is internal to the storage system.</dd>
</dlentry>
<dlentry id="ssys_replicated_volumes">
<dt>Replicated Volumes</dt>
<dd>The number of volumes that are protected by either a point-in-time or <tm tmtype="reg"
trademark="FlashCopy">FlashCopy</tm> relationship or a remote copy relationship. The number includes
source volumes in <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> relationships or remote copy
relationships.<note othertype="Learn more" type="other">To view more information about the volumes,
click the number that is shown in the column.</note></dd>
</dlentry>
<dlentry id="ssys_target_volumes">
<dt>Target Volumes</dt>
<dd>The number of volumes that are used as a backup of the data. The number includes target volumes
and volumes that are both source and target volumes in <tm tmtype="reg" trademark="FlashCopy"
>FlashCopy</tm> relationships or remote copy relationships.<note othertype="Learn more" type="other"
>To view more information about the volumes, click the number that is shown in the
column.</note></dd>
</dlentry>
<dlentry id="ssys_l2_total_volume_capacity">
<dt>Provisioned Capacity (GiB)</dt>
<dd>(Previously known as Total Volume Capacity) <p>The total amount of provisioned capacity of
volumes within the pool. If the pool is a parent pool, it also includes the storage space that can
be made available to the volumes in the child pools.</p></dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_turbo_performance">
<dt>Turbo Performance</dt>
<dd>The turbo performance mode determines whether the performance of the <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> or <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/> storage system is enhanced. If turbo performance
mode is not available for the storage system, the field is left blank.</dd>
</dlentry>
<dlentry id="ssysUnallocatableVolumeSpaceGiB">
<dt>Overprovisioned Capacity (GiB)</dt>
<dd>(Previously known as Unallocatable Volume Space) The capacity that cannot be used by volumes
because the physical capacity of the pools cannot meet the demands for provisioned capacity. The
following formula is used to calculate this value:</dd>
<dd>
<codeblock>[Provisioned Capacity − Capacity]</codeblock>
</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssysUnallocatedVolumeSpaceGiB">
<dt>Available Volume Capacity (GiB)</dt>
<dd>(Previously known as Effective Unallocated Volume Space) The total amount of remaining space
that can be used by the volumes in the pools. The following formula is used to calculate this
value:</dd>
<dd><codeblock>[Provisioned Capacity − Used Capacity]</codeblock> The capacity that is used by
thin-provisioned volumes is typically less than their provisioned capacity. Therefore, the available
capacity represents the difference between the provisioned capacity and the used capacity for all
the volumes in the pools. <ph audience="onpremonly">For <keyword
conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"/> non-thin provisioned pool capacity,
the available capacity is always zero.</ph></dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_unassigned_volume_space">
<dt>Unmapped Capacity (GiB)</dt>
<dd>(Previously known as Unassigned Volume Space) The total amount of space in the volumes that are
not assigned to hosts. </dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_unreplicated_volumes">
<dt>Unreplicated Volumes</dt>
<dd>The number of volumes that are not protected by any copy relationship. The number includes
volumes that are not source volumes, or target volumes, or source and target volumes in <tm
tmtype="reg" trademark="FlashCopy">FlashCopy</tm> relationships or remote copy relationships. <note
othertype="Learn more" type="other">To view more information about the volumes, click the number
that is shown in the column.</note></dd>
</dlentry>
<dlentry id="ssys_l2_virtual_allocation">
<dt>Provisioned Capacity (%)</dt>
<dd>(Previously known as Virtual Allocation) The percentage of the physical capacity that is
committed to the provisioned capacity of the volumes in the pools. If the value exceeds 100%, the
physical capacity doesn't meet the demands for provisioned capacity. </dd>
<dd>To calculate provisioned capacity percentage, the following formula is used:</dd>
<dd>
<codeblock>[(provisioned capacity ÷ pool capacity) × 100]</codeblock>
</dd>
<dd>For example, if the provisioned capacity percentage is 200% for a storage pool with a physical
capacity of 15 GiB, then the provisioned capacity that is committed to the volumes in the pools is
30 GiB. Twice as much space is committed to the pools than is physically available to the pools. If
the provisioned capacity percentage is 100% and the physical capacity is 15 GiB, then the
provisioned capacity that is committed to the pools is 15 GiB. The total physical capacity that is
available to the pools is used by the volumes in the pools.</dd>
<dd>A provisioned capacity percentage that is higher than 100% is considered to be aggressive
because insufficient physical capacity is available to the pools to satisfy the allocation of the
committed space to the compressed and thin-provisioned volumes in the pools. In such cases, you can
check the <uicontrol><!--Pool -->Shortfall (%)</uicontrol> value to determine how critical the
shortage of space is for the storage system pools.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ssys_l2_write_cache_memory">
<dt>Write Cache</dt>
<dd>The amount of write cache memory that is internal to the storage system.</dd>
</dlentry>
</dl>
</section>
<section>
<title>STORAGE SYSTEMS FILE STORAGE</title>
<dl id="saas_l2_file_tab_defs">
<dlentry id="ssys_l2_avail_filesys_space">
<dt>Available File System Capacity (GiB)</dt>
<dd>The amount of unused storage space on all of the file systems on the storage system or
filer.</dd>
</dlentry>
<dlentry id="ssys_l2_clusters">
<dt>Cluster</dt>
<dd audience="onpremonly">The name of the <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster.</dd>
<dd audience="offpremonly">The name of the <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/> or <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster.</dd>
</dlentry>
<dlentry conref="#tpch_r_entities.dita/ssys_l2_custom_tags">
<dt/>
<dd/>
</dlentry>
<dlentry id="ssys_l2_filesys_capacity">
<dt>File System Capacity (%)</dt>
<dd>The percentage of the total file system capacity on the storage system or filer that is used by
the files and the directories. For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>,
the used capacity includes snapshot space.</dd>
</dlentry>
<dlentry>
<dt>IP Address</dt>
<dd>The IP address of the storage system. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, the IP address of the cluster node that is
used to communicate with the cluster is shown.</dd>
</dlentry>
<dlentry conref="#tpch_r_entities.dita/ssys_l2_probe_status">
<dt/>
<dd/>
</dlentry>
<dlentry id="ssys_l2_files_raw_disk_capacity">
<dt>Raw Capacity (GB)</dt>
<dd>(Previously known as Raw Disk Capacity) The total raw disk capacity of the storage system, which
includes the capacity that is used to format the RAID.</dd>
</dlentry>
<dlentry id="ssys_l2_snapshot_space">
<dt>Snapshot Capacity (GiB)</dt>
<dd audience="onpremonly">The amount of space that is used by all of the snapshots of the file
systems that are associated with the <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"
/> cluster.</dd>
<dd audience="offpremonly">The amount of space that is used by all of the snapshots of the file
systems that are associated with the <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/> or <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster.</dd>
</dlentry>
<dlentry id="ssys_l2_total_filesys_capacity">
<dt>File System Capacity (GiB)</dt>
<dd>(Previously known as Total File System Capacity) The total storage capacity on all of the file
systems on the storage system or filer.</dd>
</dlentry>
<dlentry id="ssys_l2_used_filesys_space">
<dt>Used File System Capacity (GiB)</dt>
<dd>The file system capacity on the storage system or filer that is used by files and directories.
<ph>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, the capacity that is used
by all of the snapshots of the file systems is included in the value for Used File System
Capacity.</ph></dd>
</dlentry>
<dlentry id="ssys_l2_total_data_reduction_savings">
<dt>Total Capacity Savings (%)</dt>
<dd otherprops="536_183345new">The estimated amount and percentage of capacity that is saved by
using data deduplication, pool compression, thin provisioning, and drive compression, across all
volumes in the pool.</dd>
<dd>The following formula is used to calculate the amount of storage space that is saved:
<codeblock>[Provisioned Capacity − Used Capacity]</codeblock>
</dd>
<dd>The following formula is used to calculate the percentage of capacity that is saved:
<codeblock>((Provisioned Capacity − Used Capacity) ÷ Provisioned Capacity) × 100</codeblock>
</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/> storage systems with firmware version 11.6 or
later, and resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>. </note>
</dd>
</dlentry>
</dl>
</section>
<section>
<title>BLOCK POOL DEFINITIONS</title>
<dl>
<dlentry id="pAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a pool as acknowledged. An acknowledged status
indicates that the status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used to determine the status of related, higher-level resources.</dd>
<dd>For example, if the status of a pool is Error, the status of the storage system that contains it
is also Error. If the Error status for the pool is acknowledged, then its status is not used to
determine the overall status of the associated storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also Normal.
</dd>
</dlentry>
<dlentry id="common_async_replica_vols">
<dt>Asynchronous Replica Volumes</dt>
<dd>The number of volume pairs that are in Global Mirror relationships.</dd>
</dlentry>
<dlentry id="activity_def">
<dt>Activity</dt>
<dd>
<p>Shows the activity level of pools. For pools on storage systems other than <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, the activity level is set to the
following value:</p>
<codeblock>
[Read I/O Rate × (1 − Read I/O Cache Hit %)] ÷ Total Pool Capacity 
</codeblock>
<!--<p><synph><sep>[</sep><kwd>Read I/O Rate</kwd>
<oper>&times;</oper>
<sep>(</sep><kwd>1</kwd><oper>&minus;</oper><kwd> Read I/O Cache Hit %</kwd><sep>)</sep><sep>]</sep>
<oper>&divide;</oper>
<kwd>Total Pool Capacity</kwd>
</synph></p>-->
<p>The activity level of pools on <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/> and
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> is set to the following
value:</p>
<codeblock>
(Total I/O Rate ÷ Total Capacity)
</codeblock>
<!--<p><synph><sep>(</sep><kwd>Total I/O Rate</kwd>
<oper>&divide;</oper>
<kwd>Total Capacity</kwd><sep>)</sep></synph></p>-->
</dd>
</dlentry>
<dlentry id="deviation_def">
<dt>Activity Deviation (%)</dt>
<dd>
<p><image href="tpc_balancing_pools_thresholds.svg" placement="break">
<alt>Activity deviation percentage of pools.</alt>
</image>The activity level of the pool in relation to the average activity level of all of the pools
on the same tier and in the same storage system. The activity deviation percentage is set to the
following value:</p>
<p><synph><sep>[</sep><sep>(</sep><kwd>Pool Activity Level</kwd>
<oper>−</oper>
<kwd>Average Activity Level</kwd><sep>)</sep>
<oper>÷</oper>
<kwd>Average Activity Level</kwd><sep>]</sep>
<oper>×</oper>
<kwd>100</kwd></synph></p>
<p> If the activity deviation percentage of the pool lies between −10% and +10%, the pool is
balanced. Pools with activity levels over +10% are candidates for balancing whereas pools with
activity levels under −10% are candidates for receiving volumes from pools with high activity
deviation percentages.</p>
<p>For example, a storage environment consists of a <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/> storage virtualizer with one
<keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage system. Three tier-1 pools have
the following activity levels:<table><title>Information about calculating activity deviation
percentages</title><desc>
<p outputclass="summary"/>
</desc><tgroup cols="2">
<colspec colname="col2"/>
<colspec colname="COLSPEC0"/>
<thead>
<row valign="bottom">
<entry colname="col2">Pool</entry>
<entry colname="COLSPEC0">Activity</entry>
</row>
</thead>
<tbody>
<row>
<entry colname="col2">pool_1</entry>
<entry colname="COLSPEC0">110</entry>
</row>
<row>
<entry colname="col2">pool_2</entry>
<entry colname="COLSPEC0">90</entry>
</row>
<row>
<entry colname="col2">pool_3</entry>
<entry colname="COLSPEC0">140</entry>
</row>
</tbody>
</tgroup></table></p>
<p>The average activity level of the pools on tier 1 is 113. The activity deviation percentage
values for the pools on tier 1 in the <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>
storage system are as follows:<ul>
<li>The activity deviation percentage for <systemoutput>pool_1</systemoutput> = −3%</li>
<li>The activity deviation percentage for <systemoutput>pool_2</systemoutput> = −20%</li>
<li>The activity deviation percentage for <systemoutput>pool_3</systemoutput> = +24%</li>
</ul></p>
<p>In the preceding example, <systemoutput>pool_2</systemoutput> and
<systemoutput>pool_3</systemoutput> are candidates for balancing.</p>
</dd>
</dlentry>
<dlentry id="saas_activity_deviation_def">
<dt>Activity Deviation (%)</dt>
<dd>
<p>Shows the activity level of the pool in relation to the average activity level of all of the
pools on the same tier and in the same storage system. The activity deviation percentage is set to
the following value:</p>
<p><synph><sep>[</sep><sep>(</sep><kwd>Pool Activity Level</kwd>
<oper>−</oper>
<kwd>Average Activity Level</kwd><sep>)</sep>
<oper>÷</oper>
<kwd>Average Activity Level</kwd><sep>]</sep>
<oper>×</oper>
<kwd>100</kwd></synph></p>
<p> A positive value indicates that the relative activity level of the pool is above average. A
negative value indicates that the relative activity level of the pool is below average.</p>
<p>For example, a storage environment consists of a <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/> storage virtualizer with one
<keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage system. Three tier-1 pools have
the following activity levels:<table><title>Information about calculating activity deviation
percentages</title><desc>
<p outputclass="summary"/>
</desc><tgroup cols="2">
<colspec colname="col2"/>
<colspec colname="COLSPEC0"/>
<thead>
<row valign="bottom">
<entry colname="col2">Pool</entry>
<entry colname="COLSPEC0">Activity</entry>
</row>
</thead>
<tbody>
<row>
<entry colname="col2">pool_1</entry>
<entry colname="COLSPEC0">110</entry>
</row>
<row>
<entry colname="col2">pool_2</entry>
<entry colname="COLSPEC0">90</entry>
</row>
<row>
<entry colname="col2">pool_3</entry>
<entry colname="COLSPEC0">140</entry>
</row>
</tbody>
</tgroup></table></p>
<p>The average activity level of the pools on tier 1 is 113. The activity deviation percentage
values for the pools on tier 1 in the <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>
storage system are as follows:<ul>
<li>The activity deviation percentage for pool_1 = −3%</li>
<li>The activity deviation percentage for pool_2 = −20%</li>
<li>The activity deviation percentage for pool_3 = +24%</li>
</ul></p>
<p>If the activity deviation percentage of the pool exceeds the deviation threshold of 10%, the pool
is a candidate for balancing. </p>
<p>In the preceding example, the pool_3 storage pool is a candidate for balancing because the
activity deviation value for the pool is 14% higher than the threshold of 10%.</p>
</dd>
</dlentry>
<dlentry id="dl_pool_allocated_space">
<dt>Used Capacity (GiB)</dt>
<dd id="pool_allocated_space">(Previously known as Allocated Space) The amount of physical capacity
that is used by the volumes in the pool. If the pool is a parent pool, the amount of space that is
used by the volumes in the child pools is also included. </dd>
<dd>The capacity that is used by thin-provisioned volumes is less than their provisioned capacity,
which is shown in the Provisioned Capacity column. If a pool does not contain thin-provisioned
volumes, this value is the same as Provisioned Capacity .</dd>
<!--<dd>Allocated space is the same as used space on all storage systems, except for resources that run
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>. These resources might
have more allocated space than used space if the storage administrator preallocated some space for
thin-provisioned volumes when the volumes were created.</dd>-->
<dd>
<note othertype="Availability" type="other">All storage systems, except <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>. </note>
</dd>
</dlentry>
<dlentry>
<dt>Mapped Capacity (%)</dt>
<dd id="pAssignedVolumeSpaceGiB">(Previously known as Assigned Volume Space) The total amount of
space in the volumes that is assigned to hosts. For <keyword
conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"/> non-thin provisioning pool space, this
value is the sum of assigned regular host-accessible volumes. Volumes that are used for
thin-provisioning (pool volumes) are not included.</dd>
<dd id="pSaaS_AssignedVolumeSpaceGiB">The space on all of the volumes in a pool that are mapped or
assigned to host systems. For a thin-provisioning pool, this value includes the provisioned capacity
of thin-provisioned volumes, which might exceed the total space in the pool.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="dl_pool_available_space">
<dt>Available Capacity (GiB)</dt>
<dd>(Previously known as Available Pool Space) The amount of physical space that is available in the
pool. If the pool is a parent pool, the amount of space that is used by the volumes in the child
pools is also included.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, this value represents
provisioned capacity rather than physical space.</note>
</dd>
</dlentry>
<dlentry id="pAvailableRepositorySpaceGiB">
<dt>Available Repository Capacity (GiB)</dt>
<dd>The available, unallocated storage space in the repository for Track Space-Efficient (TSE)
thin-provisioning.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> thin-provisioned pools.</note>
</dd>
</dlentry>
<dlentry id="pAvaillableSoftSpaceGiB">
<dt>Available Soft Capacity (GiB)</dt>
<dd>The amount of virtual storage space that is available to allocate to volumes in a storage
pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="pBackEndStorageDisks">
<dt>Back-End Storage Disks</dt>
<dd>The number of physical disks that contribute to the volumes on the back-end storage system.
Available only for pools on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
<dd>If the back-end storage system was not probed, then the value in this column is blank. To
manually define the number of disks to help calculate the approximate read I/O capability of the
pool, right-click a pool in the list and select <uicontrol>View Properties</uicontrol>. On the
<uicontrol>Back-end Storage</uicontrol> tab in properties notebook, click
<uicontrol>Edit</uicontrol>. </dd>
</dlentry>
<dlentry id="pBackEndStorageDiskType">
<dt>Back-End Storage Disk Type</dt>
<dd>The class and speed of the physical disks that contribute to the volumes on the back-end storage
system. Available only for pools on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
<dd>If the back-end storage system was not probed, then the value in this column is blank. To
manually define a disk type to help calculate the approximate read I/O capability of the pool,
right-click a pool in the list and select <uicontrol>View Properties</uicontrol>. On the
<uicontrol>Back-end Storage</uicontrol> tab in properties notebook, click
<uicontrol>Edit</uicontrol>. </dd>
</dlentry>
<dlentry id="pBackEndStorageRAIDLevel">
<dt>Back-End Storage RAID Level</dt>
<dd>The RAID level of the volumes on the back-end storage system that are providing storage space to
a pool. Available only for pools on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
<dd>If the back-end storage system was not probed, then the value in this column is blank. To
manually define a RAID level to help calculate the approximate read I/O capability of the pool,
right-click a pool in the list and select <uicontrol>View Properties</uicontrol>. On the
<uicontrol>Back-end Storage</uicontrol> tab in properties notebook, click
<uicontrol>Edit</uicontrol>. </dd>
</dlentry>
<dlentry id="pBackEndStorageSystemType">
<dt>Back-End Storage System Type</dt>
<dd>The type of storage system that is providing storage space to a pool. Available only for pools
on storage systems that run <keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"
/>.</dd>
<dd>If the back-end storage system was not probed, then the value in this column is blank. To
manually select a type of storage system to help calculate the approximate read I/O capability of
the pool, right-click a pool in the list and select <uicontrol>View Properties</uicontrol>. On the
<uicontrol>Back-end Storage</uicontrol> tab in properties notebook, click
<uicontrol>Edit</uicontrol>. </dd>
</dlentry>
<dlentry id="dl_pool_capacity">
<dt>Capacity (GiB)</dt>
<dd id="pool_capacity">The total amount of storage space in the pool. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, capacity represents the physical
or ("hard") capacity of the pool, not the provisioned ("soft") capacity.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="pCapacityPool">
<dt>Capacity Pool</dt>
<dd>The name of the capacity pool that the storage pool is assigned to.</dd>
</dlentry>
<dlentry id="pCompressionSavingsPercentage">
<dt>Compression Savings (%)</dt>
<dd>The estimated amount and percentage of capacity that is saved by using data compression. The
percentage is calculated across all compressed volumes in the pool and does not include the capacity
of non-compressed volumes.</dd>
<dd>For storage systems with drives that use inline data compression technology, the Compression
Savings does not include the capacity savings that are achieved at the drive level. </dd>
<dd>The following formula is used to calculate the amount of storage space that is saved:
<codeblock>written capacity − compressed size</codeblock></dd>
<dd>The following formula is used to calculate the percentage of capacity that is
saved:<codeblock>((written capacity − compressed size) ÷ written capacity) × 100</codeblock></dd>
<dd>For example, the written capacity, which is the amount of data that is written to the volumes
before compression, is 40 GiB. The compressed size, which reflects the size of compressed data that
is written to disk, is just 10 GiB. Therefore, the compression savings percentage across all
compressed volumes is 75%.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/> storage systems with firmware version 11.6 or
later, and resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry>
<dt>Custom tag 1, 2, and 3</dt>
<dd id="pCustomTags">Any user-defined text that is associated with a pool. This text can be included
as a report column when you generate reports for the pool. <p>You can tag a pool to satisfy custom
requirements of a service class. A service class can specify up to three tags. To provide a service
class, storage resources must have all the same tags that are specified by the service class. If a
pool is not tagged, any tags on the containing storage system also apply to the pool for determining
whether it satisfies the custom requirements of a service class.</p>To edit the custom tags,
right-click it and select <uicontrol>View Properties</uicontrol>. On the properties notebook, click
<uicontrol>Edit</uicontrol>.</dd>
<dd id="pSaaSCustomTags">Any user-defined text that is associated with the pool. This text can be
included in reports that you create for the pool.</dd>
</dlentry>
<dlentry id="pDeduplicationSavings">
<dt>Deduplication Savings (%)</dt>
<dd>The estimated amount and percentage of capacity that is saved by using data deduplication. The
percentage is calculated across all deduplicated volumes in the pool and does not include the
capacity of volumes that are not deduplicated.</dd>
<dd>The following formula is used to calculate the amount of storage space that is saved:
<codeblock>written capacity − deduplicated size</codeblock></dd>
<dd>The following formula is used to calculate the percentage of capacity that is
saved:<codeblock>((written capacity − deduplicated size) ÷ written capacity) × 100</codeblock></dd>
<dd>For example, the written capacity, which is the amount of data that is written to the volumes
before deduplication, is 40 GiB. The deduplicated size, which reflects the size of deduplicated data
that is written to disk, is just 10 GB. Therefore, data deduplication reduced the size of the data
that is written by 75%.</dd>
<dd>
<note othertype="Availability" type="other">Storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> 8.1.3 or later.</note>
</dd>
</dlentry>
<dlentry id="pEasyTier">
<dt><tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm></dt>
<dd>The <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> value determines whether <tm
tmtype="reg" trademark="Easy Tier">Easy Tier</tm> is enabled and the <tm trademark="Easy Tier"
tmtype="reg">Easy Tier</tm> Status determines how tiering is managed. For example, <tm tmtype="reg"
trademark="Easy Tier">Easy Tier</tm> can be configured to tier all pools, single-tier pools (pools
with one class drive), or multitier pools (pools with multiple class drives).</dd>
<dd>You can configure <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
<dd>
<p>The following table shows the possible <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> and
related <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> Status values: </p>
<simpletable frame="all" relcolwidth="1* 1* 1.5*">
<sthead>
<stentry>Number of Tiers</stentry>
<stentry><tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm></stentry>
<stentry><tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> Status</stentry>
</sthead>
<strow>
<stentry>One</stentry>
<stentry>Off</stentry>
<stentry>Inactive</stentry>
</strow>
<strow>
<stentry>Two or more</stentry>
<stentry>Off</stentry>
<stentry>Inactive</stentry>
</strow>
<strow>
<stentry>One</stentry>
<stentry>Measure</stentry>
<stentry>Measured</stentry>
</strow>
<strow>
<stentry>Two or more</stentry>
<stentry>Measure</stentry>
<stentry>Measured</stentry>
</strow>
<strow>
<stentry>One</stentry>
<stentry>Auto</stentry>
<stentry>Balanced</stentry>
</strow>
<strow>
<stentry>Two or more</stentry>
<stentry>Auto</stentry>
<stentry>Active</stentry>
</strow>
<strow>
<stentry>One</stentry>
<stentry>On</stentry>
<stentry>Balanced</stentry>
</strow>
<strow>
<stentry>Two or More</stentry>
<stentry>On</stentry>
<stentry>Active</stentry>
</strow>
</simpletable>
</dd>
</dlentry>
<!--29 July 2015 PL 127613 Easy TierShows how the Easy Tier function is enabled on a pool.For  6.1,  family storage systems that are configured with block storage, and  pools, possible values are Enabled/Inactive, Enabled/Active, Auto/Active, Auto/Inactive, and Disabled.For , the value can be Tiered Pools/Yes, Tiered Pools/No, All Pools/Yes, and Disabled.-->
<dlentry id="pEncryption">
<dt>Encryption</dt>
<dd>Shows whether a pool is encrypted. This column is blank if encryption is not supported for a
pool.</dd>
</dlentry>
<dlentry id="pEncryptionGroup">
<dt>Encryption Group</dt>
<dd>The identifier of the encryption group that was specified when the pool was created. Available
only for <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> pools.</dd>
</dlentry>
<dlentry id="pTierAvailableSpaceEnterpriseGiB">
<dt>Enterprise HDD Available Capacity (GiB)</dt>
<dd>The amount of storage space that is available on the Enterprise hard disk drives that can be
used by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for re-tiering the volume extents in
the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="pTierCapacityEnterprise">
<dt>Enterprise HDD Capacity (GiB)</dt>
<dd>The total amount of storage space on the Enterprise hard disk drives that can be used by <tm
tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for re-tiering the volume extents in the
pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="pExtentSizeMiB">
<dt>Extent Size (MiB)</dt>
<dd>The extent granularity that was specified when a pool was created. Smaller extent sizes limit
the maximum size of the volumes that can be created in a pool, but minimize the amount of
potentially wasted space per volume. Available only for pools on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="common_flashcopy_vols">
<dt><tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> Volumes</dt>
<dd>The number of volumes that are in <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> pair
relationships.</dd>
</dlentry>
<dlentry>
<dt>Format</dt>
<dd id="pFormat">The formats of the volumes that are allocated from a pool, such as FB (fixed block)
or CKD (count key data). Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> pools.</dd>
<dd id="pSaaSFormat">The formats of the volumes that are allocated from a pool, such as FB (fixed
block) or CKD (count key data). Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> pools.</dd>
</dlentry>
<dlentry id="pLastDataCollection">
<dt>Last Data Collection</dt>
<dd>The most recent date and time when data was collected about the storage system that contains a
storage pool.</dd>
</dlentry>
<dlentry id="pLSSorLCU" audience="onpremonly">
<dt>LSS or LCU</dt>
<dd>The logical subsystem (LSS) for fixed block volumes, or the logical control unit (LCU) for count
key data volumes. Available only for <keyword conref="fqz0_entities.dita#fqz0_entities/kess_short"/>
pools.</dd>
</dlentry>
<dlentry id="pManagedDisks">
<dt>Managed Disks</dt>
<dd>The number of managed disks that are assigned to a pool. Available only for pools in <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</dd>
</dlentry>
<dlentry id="pName">
<dt>Name</dt>
<dd>The name of a pool that uniquely identifies it within a storage system. <ph>The icon that is
displayed next to the name of the pool indicates the type of pool:</ph><simpletable>
<sthead>
<stentry>Icon</stentry>
<stentry>Type of pool</stentry>
<stentry>Description</stentry>
</sthead>
<strow>
<stentry><image href="mgr_pool_type_internal.png" placement="inline">
<alt>Internal pool</alt>
</image></stentry>
<stentry>Internal</stentry>
<stentry>A pool that is internal to a storage system.</stentry>
</strow>
<strow>
<stentry><image href="mgr_pool_type_external.png" placement="inline">
<alt>External pool</alt>
</image></stentry>
<stentry>External</stentry>
<stentry>A pool that is external to a storage system but its provider cannot be
determined.</stentry>
</strow>
<strow>
<stentry><image href="mgr_pool_type_spectrumarchive.png" placement="inline">
<alt><keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_archive"/> pool</alt>
</image></stentry>
<stentry><keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_archive"/></stentry>
<stentry>A pool that is external to an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster and is provided by <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_archive"/>. Applies only to <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>.</stentry>
</strow>
<strow>
<stentry><image href="mgr_pool_type_spectrumprotect.png" placement="inline">
<alt><keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_protect"/> pool</alt>
</image></stentry>
<stentry><keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_protect"/></stentry>
<stentry>A pool that is external to an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster and is provided by <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_protect"/>. Applies only to <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>.</stentry>
</strow>
<strow>
<stentry><image href="mgr_pool_type_objectcloudstorage.png" placement="inline">
<alt><keyword conref="fqz0_entities.dita#fqz0_entities/kcloudobject"/> pool</alt>
</image></stentry>
<stentry><keyword conref="fqz0_entities.dita#fqz0_entities/kcloudobject"/></stentry>
<stentry>A pool that is external to an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster and is provided by <keyword
conref="fqz0_entities.dita#fqz0_entities/kcloudobject"/>. Applies only to <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>.</stentry>
</strow>
<strow>
<stentry><image href="mgr_pool_type_amazons3.png" placement="inline">
<alt>Amazon S3 pool</alt>
</image></stentry>
<stentry>Amazon Simple Storage Service (S3)</stentry>
<stentry>A pool that is external to an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster and represents cloud storage from
Amazon S3. Applies only to <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>.
</stentry>
</strow>
<strow>
<stentry><image href="mgr_pool_type_openstack.png" placement="inline">
<alt>OpenStack Swift pool</alt>
</image></stentry>
<stentry>OpenStack Swift</stentry>
<stentry>A pool that is external to an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster and represents cloud storage from
OpenStack Swift. Applies only to <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"
/>.</stentry>
</strow>
<strow>
<stentry><image href="mgr_pool_type_othercloud.png" placement="inline">
<alt>Other Cloud pool</alt>
</image></stentry>
<stentry>Other cloud</stentry>
<stentry>A pool that is external to an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster and represents cloud storage from a
provider that cannot be determined. </stentry>
</strow>
</simpletable></dd>
</dlentry>
<dlentry id="pTierAvailableSpaceNearlineGiB">
<dt>Nearline HDD Available Capacity (GiB)</dt>
<dd>The amount of storage space that is available on the Nearline hard disk drives that can be used
by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for re-tiering the volume extents in the
pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="pTierCapacityNearline">
<dt>Nearline HDD Capacity (GiB)</dt>
<dd>The total amount of storage space on the Nearline hard disk drives that can be used by <tm
tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for re-tiering the volume extents in the
pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry>
<dt>Node</dt>
<dd id="pController">The name of the node to which volumes from a pool are assigned. Available only
for volumes in <keyword conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>.</dd>
<dd id="pSaaSController">The name of the node to which volumes from a pool are assigned. Available
only for <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> volumes.</dd>
</dlentry>
<dlentry id="pOwnerChildPool">
<dt>Owner Name</dt>
<dd>The name of the user who owns the child pool.</dd>
</dlentry>
<dlentry id="pParentPoolName">
<dt>Parent Name</dt>
<dd>The name of the parent pool.</dd>
</dlentry>
<dlentry id="pPhysicalAllocationPercentage">
<dt>Used Capacity (%)</dt>
<dd>
<p>(Previously known as Physical Allocation) </p>
<p>
<image href="artcomp-poolphysalloc-001.svg" placement="break"
alt="Illustration of used capacity percentage"/>
</p>
<p>The percentage of physical capacity that is used by the volumes in the pool, including the
volumes in child pools. This value is always less than or equal to 100% because you cannot allocate
more physical space than is available in a pool. Check the value for used capacity to see:<ul>
<li>Whether the physical capacity of the pool is fully allocated. That is, the value for used
capacity is 100%.</li>
<li>Whether you have sufficient capacity to provision new volumes with storage</li>
<li>Whether you have sufficient capacity to allocate to the compressed and thin-provisioned volumes
in the pool</li>
</ul></p>
<p otherprops="tvt"><i><b>TVT Instructions</b>: The source file for this graphic which contains text
for translation is <filepath>artcomp-poolphysalloc-001.svg</filepath> .</i></p>
</dd>
<dd>
<note othertype="Availability" type="other">All storage systems, except <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>.</note>
</dd>
</dlentry>
<dlentry id="pPoolAttributes">
<dt>Pool Attributes</dt>
<dd><p>Shows whether data reduction, encryption, thin provisioned, or all the three are configured
for the pool. If neither feature is configured, the column is blank. </p><p>Data reduction is
available for pools on <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short">
and </keyword>, <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"> and
</keyword>, and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> 8.1.1 or later.</p>Encryption is
available for all pools.<note type="other" othertype="Examples">
<ul id="ul_ar2_twq_fdb">
<li>If only data reduction is configured, the value Data Reduction is shown.</li>
<li>If only encryption is configured, the value Encryption is shown.</li>
<li>If both features are configured, the value Data Reduction, Encryption is shown.</li>
</ul>
</note></dd>
</dlentry>
<dlentry id="pRAIDLevel">
<dt>RAID Level</dt>
<dd>The RAID level of the pool, such as RAID 5 and RAID 10. The RAID level affects the performance
and fault tolerance of the volumes that are allocated from the pool. In some cases, there might be a
mix of RAID levels in a pool. The RAID levels in a mixed pool are shown in a comma-separated
list.</dd>
</dlentry>
<dlentry>
<dt>Rank Group</dt>
<dd id="pRankGroup">The rank group to which a pool is assigned. Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> pools.</dd>
<dd id="pSaaSRankGroup">The rank group to which a pool is assigned. Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> pools.</dd>
</dlentry>
<dlentry id="pReadIOCapability">
<dt>Read I/O Capability (ops/s)</dt>
<dd audience="offpremonly">The projected maximum number of I/O operations per second for a pool.
This value is calculated based on the values in the <uicontrol>Back-End Storage System
Type</uicontrol>, <uicontrol>Back-End Storage RAID Level</uicontrol>, <uicontrol>Back-End Storage
Disk Type</uicontrol>, and <uicontrol>Back-End Storage Disks</uicontrol> fields. Available only for
pools in <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/> pools.</dd>
<dd audience="onpremonly">The projected maximum number of I/O operations per second for a pool. This
value is calculated based on the values in the <uicontrol>Back-End Storage System Type</uicontrol>,
<uicontrol>Back-End Storage RAID Level</uicontrol>, <uicontrol>Back-End Storage Disk
Type</uicontrol>, and <uicontrol>Back-End Storage Disks</uicontrol> fields. Available only for
<keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_V9000_short"/> pools.</dd>
<dd>If the back-end storage system was not probed, then the value in this column is blank. To help
calculate an approximate read I/O capability for the pool, you must manually define values for the
columns that are related to back-end storage.</dd>
</dlentry>
<dlentry id="pRepsoitoryCapacityGiB">
<dt>Repository Capacity (GiB)</dt>
<dd>The total storage capacity of the repository for Track Space-Efficient (TSE)
thin-provisioning.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> thin-provisioned pools.</note>
</dd>
</dlentry>
<dlentry id="pReservedPoolSpace">
<dt>Reserved Capacity (GiB)</dt>
<dd>(Previously known as Reserved Pool Space) The amount of unused capacity in a pool that is
reserved by provisioning tasks. Pool space is reserved when a provisioning task is created, and
allocated when a provisioning task is run. If the provisioning task is not run and is deleted, the
reserved space is freed. If space is reserved, you can click this value to display information about
the tasks for which the space is reserved.<p audience="offpremonly">This measurement includes the
capacity of planned standard-provisioned volumes. This measurement does not include the capacity of
planned thin-provisioned volumes, unless all of the following conditions are true: <ul>
<li>The provisioning task allocates the volumes from a <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/> or <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_short"/> storage system. </li>
<li>The provisioning task was created by using a service class that specifies the used capacity
property. The used capacity property specifies the percentage of the provisioned capacity that is
allocated when the volume is created. The used capacity percentage represents real capacity, and is
included in the reserved pool space measurement.</li>
</ul></p><p audience="onpremonly">This measurement includes the capacity of planned
standard-provisioned volumes. This measurement does not include the capacity of planned
thin-provisioned volumes, unless all of the following conditions are true: <ul>
<li>The provisioning task allocates the volumes from a storage system that runs <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</li>
<li>The provisioning task was created by using a service class that specifies the used capacity
property. The used capacity property specifies the percentage of the provisioned capacity that is
allocated when the volume is created. The used capacity percentage represents real capacity, and is
included in the reserved pool space measurement.</li>
</ul></p></dd>
</dlentry>
<dlentry id="pSCMAvailCapacityGiB">
<dt>SCM Available Capacity (GiB)</dt>
<dd>The available capacity on Storage Class Memory (SCM) drives in the pool. <tm
trademark="Easy Tier" tmtype="reg">Easy Tier</tm> can use these drives to retier the volume extents
in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> systems, such as <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_9100_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_7200_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="pSCMCapacityGiB">
<dt>SCM Capacity (GiB)</dt>
<dd>The total capacity on Storage Class Memory (SCM) drives in the pool. <tm trademark="Easy Tier"
tmtype="reg">Easy Tier</tm> can use these drives to retier the volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> systems, such as <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_9100_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_7200_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="vSCMCapacityGiB">
<dt>SCM Capacity (GiB)</dt>
<dd>The amount of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Storage Class Memory (SCM) drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> systems, such as <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_9100_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_7200_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="vSCMCapacityPercent">
<dt>SCM Capacity (%)</dt>
<dd>The percentage of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Storage Class Memory (SCM) drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> systems, such as <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_9100_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_7200_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="pShortfallPercentage">
<dt>Shortfall (%)</dt>
<dd>The difference between the remaining unused volume capacity and the available capacity of the
associated pool, expressed as a percentage of the remaining unused volume capacity. The shortfall
represents the relative risk of running out of space for overallocated thin-provisioned volumes. If
the pool has sufficient available capacity to satisfy the remaining unused volume capacity, no
shortfall exists. As the remaining unused volume capacity grows, or as the available pool capacity
decreases, the shortfall increases and the risk of running out of space becomes higher. If the
available capacity of the pool is exhausted, the shortfall is 100% and any volumes that are not yet
fully allocated have run out of space.</dd>
<dd>If the pool isn't thin-provisioned, the shortfall percentage equals zero. If shortfall
percentage isn't calculated for the storage system, the field is left blank.</dd>
<dd id="pDdShortfallPercentage3">The following formula is used to calculate this value:
<codeblock>Overprovisioned Capacity ÷ Committed but Unused Capacity</codeblock>
</dd>
<dd id="pDdShortfallPercentage4">You can use this percentage to determine when the amount of
over-committed space in a pool is at a critically high level. Specifically, if the physical space in
a pool is less than the committed provisioned capacity, then the pool does not have enough space to
fulfill the commitment to provisioned capacity. This value represents the percentage of the
committed provisioned capacity that is not available in a pool. As more space is used over time by
volumes while the pool capacity remains the same, this percentage increases. </dd>
<dd otherprops="infocenteronly"><object
data="https://cdnapisec.kaltura.com/p/1773841/sp/177384100/embedIframeJs/uiconf_id/39954662/partner_id/1773841?iframeembed=true&amp;playerId=kplayer&amp;entry_id=1_9zoy6pd0&amp;wid=1_l2f00zvj&amp;flashvars[streamerType]=auto"
height="360" width="640" outputclass="iframe">
<desc>Simplifying storage, deploying new applications, and controlling costs with <tm
trademark="IBM Spectrum" tmtype="reg">IBM Spectrum</tm> Storage<image id="image_cj2_j14_3fb"
placement="inline">
<alt>Simplifying storage, deploying new applications, and controlling costs with IBM Spectrum
Storage</alt>
</image></desc>
</object></dd>
<dd otherprops="helpsonly"><b><xref keyref="vid_shortfall"/></b></dd>
<dd otherprops="usersguideonly"><b><xref keyref="vid_shortfall"/></b></dd>
<dd id="pDdShortfallPercentage5">Example: The remaining physical capacity of a pool is 70 GiB, but
150 GiB of provisioned capacity was committed to thin-provisioned volumes. If the volumes are using
50 GiB, then 100 GiB is still committed to the volumes (150 GiB − 50 GiB) with a shortfall of 30 GiB
(70 GiB remaining pool space − 100 GiB remaining commitment of volume space to the volumes). </dd>
<dd id="pDdShortfallPercentage6">Because the volumes are overcommitted by 30 GiB based on the
available capacity in the pool, the shortfall is 30% when the following calculation is used:
<codeblock scale="80">[(100 GiB unused volume capacity − 70 GiB remaining pool capacity)
 ÷ 100 GiB unused volume capacity] × 100</codeblock></dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_full"/>, and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>. <p>For <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, this value is not
available.</p>
</note>
</dd>
</dlentry>
<dlentry id="pSaasShortfallPercentagePre535">
<dt>Shortfall (%)</dt>
<dd>The percentage of space that is over committed to pools with thin-provisioned volumes. For
example, you commit 100 GiB of space to a thin-provisioned volume in a pool with a capacity of 50
GiB. As the space is used by the thin-provisioned volume in increments of 10 GiB, the space
available for allocation decreases and the shortfall in capacity becomes more acute.</dd>
<dd>If the pool is not thin-provisioned, the shortfall percentage equals zero. If shortfall
percentage isn't calculated for the storage system, the field is left blank.</dd>
<dd>To calculate shortfall, the following formula is used:
<codeblock>Overprovisioned Capacity ÷ Committed but Unused Capacity</codeblock>
</dd>
<dd>You can use this percentage to determine when the amount of over-committed space in a pool is at
a critically high level. Specifically, if the physical space in a pool is less than the committed
provisioned capacity, then the pool does not have enough space to fulfill the commitment to
provisioned capacity. This value represents the percentage of the committed provisioned capacity
that is not available in a pool. As more space is used over time by volumes while the pool capacity
remains the same, this percentage increases. </dd>
<dd><object
data="https://cdnapisec.kaltura.com/p/1773841/sp/177384100/embedIframeJs/uiconf_id/39954662/partner_id/1773841?iframeembed=true&amp;playerId=kplayer&amp;entry_id=1_9zoy6pd0&amp;wid=1_l2f00zvj&amp;flashvars[streamerType]=auto"
height="360" width="640" outputclass="iframe">
<desc>Simplifying storage, deploying new applications, and controlling costs with <tm
trademark="IBM Spectrum" tmtype="reg">IBM Spectrum</tm> Storage<image placement="inline">
<alt>Simplifying storage, deploying new applications, and controlling costs with IBM Spectrum
Storage</alt>
</image></desc>
</object></dd>
<!--<dd><image href="prd_anm_cap_shortfall.gif" placement="break" alt="Shortfall (%) for pools" scalefit="yes"/></dd>-->
<dd>Example: The remaining physical capacity of a pool is 70 GiB, but 150 GiB of provisioned
capacity was committed to thin-provisioned volumes. If the volumes are using 50 GiB, then 100 GiB is
still committed to the volumes (150 GiB − 50 GiB) with a shortfall of 30 GiB (70 GiB remaining pool
space − 100 GiB remaining commitment of volume space to the volumes). </dd>
<dd>Because the volumes are overcommitted by 30 GiB based on the available space in the pool, the
shortfall is 30% when the following calculation is used:
<codeblock scale="80">[(100 GiB unallocated volume space − 70 GiB remaining pool space)
 ÷ 100 GiB unallocated volume space] × 100</codeblock></dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage, <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>, and
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="pSaasShortfallPercentage">
<dt>Shortfall (%)</dt>
<dd>The percentage of space that is over committed to pools with thin-provisioned volumes. For
example, you commit 100 GiB of space to a thin-provisioned volume in a pool with a capacity of 50
GiB. As the space is used by the thin-provisioned volume in increments of 10 GiB, the space
available for allocation decreases and the shortfall in capacity becomes more acute.</dd>
<dd>If the pool is not thin-provisioned, the shortfall percentage equals zero. If shortfall
percentage isn't calculated for the storage system, the field is left blank.</dd>
<dd>You can use this percentage to determine when the amount of over-committed space in a pool is at
a critically high level. Specifically, if the physical space in a pool is less than the committed
provisioned capacity, then the pool does not have enough space to fulfill the commitment to
provisioned capacity. This value represents the percentage of the committed provisioned capacity
that is not available in a pool. As more space is used over time by volumes while the pool capacity
remains the same, this percentage increases. </dd>
<dd><object
data="https://cdnapisec.kaltura.com/p/1773841/sp/177384100/embedIframeJs/uiconf_id/39954662/partner_id/1773841?iframeembed=true&amp;playerId=kplayer&amp;entry_id=1_9zoy6pd0&amp;wid=1_l2f00zvj&amp;flashvars[streamerType]=auto"
height="360" width="640" outputclass="iframe">
<desc>Simplifying storage, deploying new applications, and controlling costs with <tm
trademark="IBM Spectrum" tmtype="reg">IBM Spectrum</tm> Storage<image placement="inline">
<alt>Simplifying storage, deploying new applications, and controlling costs with IBM Spectrum
Storage</alt>
</image></desc>
</object></dd>
<!--<dd><image href="prd_anm_cap_shortfall.gif" placement="break" alt="Shortfall (%) for pools" scalefit="yes"/></dd>-->
<dd>Example: The remaining physical capacity of a pool is 70 GiB, but 150 GiB of provisioned
capacity was committed to thin-provisioned volumes. If the volumes are using 50 GiB, then 100 GiB is
still committed to the volumes (150 GiB − 50 GiB) with a shortfall of 30 GiB (70 GiB remaining pool
space − 100 GiB remaining commitment of volume space to the volumes). </dd>
<dd>Because the volumes are overcommitted by 30 GiB based on the available capacity in the pool, the
shortfall is 30%.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage, <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>, and
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="pSaasPoolShortfallPercentage">
<dt><!--Pool -->Shortfall (%)</dt>
<dd>The percentage of the remaining unused volume capacity in a pool that is not available to be
used. The higher the percentage, the more critical the shortfall of pool capacity.</dd>
<dd>This value is available when the pool is thin-provisioned on the following storage systems:<ul>
<li><keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/></li>
<li><keyword conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that
are configured with block storage</li>
<li><keyword conref="fqz0_entities.dita#fqz0_entities/kds_short"/></li>
</ul>For the preceding storage systems, the value is zero when the pool is not thin-provisioned. For
all other storage systems, a blank space is displayed. </dd>
<dd>The following formula is used to calculate this value:
<codeblock>Overprovisioned Capacity ÷ Committed but Unused Capacity</codeblock>
</dd>
<dd>You can use this percentage to determine when the amount of over-committed space in a pool is at
a critically high level. Specifically, if the physical space in a pool is less than the committed
provisioned capacity, then the pool does not have enough space to fulfill the commitment to
provisioned capacity. This value represents the percentage of the committed provisioned capacity
that is not available in a pool. As more space is used over time by volumes while the pool capacity
remains the same, this percentage increases. </dd>
<dd>Example: The remaining physical capacity of a pool is 70 GiB, but 150 GiB of provisioned
capacity was committed to thin-provisioned volumes. If the volumes are using 50 GiB, then 100 GiB is
still committed to the volumes (150 GiB − 50 GiB) with a shortfall of 30 GiB (70 GiB remaining pool
space − 100 GiB remaining commitment of volume capacity to the volumes). </dd>
<dd>Because the volumes are overcommitted by 30 GiB based on the available capacity in the pool, the
shortfall is 30% when the following calculation is used:
<codeblock scale="80">[(100 GiB unused volume capacity − 70 GiB remaining pool capacity)
 ÷ 100 GiB unused volume capacity] × 100</codeblock></dd>
<dd>The first section of the bar uses the color blue and a percent (%) sign to represent the
shortfall percentage. The second section of the bar uses the color gray to represent the unused
volume capacity. Hover the mouse pointer over the percentage bar to view the following values:<dl>
<dlentry>
<dt>Unused Volume Capacity (GiB)</dt>
<dd>(Previously known as Unallocated Volume Space) The amount of the Provisioned Capacity in the
pool that is not used.</dd>
</dlentry>
<dlentry>
<dt>Available Capacity (GiB)</dt>
<dd>(Previously known as Available Pool Space) The amount of capacity in a pool that is not used by
volumes. </dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="common_protected_vols">
<dt>Protected Volumes</dt>
<dd>The number of source volumes that have replicas, or that have VDisk mirrors, or are in a <tm
tmtype="reg" trademark="FlashCopy">FlashCopy</tm> pair relationship.</dd>
</dlentry>
<dlentry id="pSoftSpaceGiB">
<dt>Soft Capacity (GiB)</dt>
<dd>The amount of virtual storage space that is configured for the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="pSolidState">
<dt><tm tmclass="IGNORE" tmtype="reg" trademark="Solid">Solid</tm> State</dt>
<dd> Shows whether a pool contains solid-state disk drives. If a pool contains solid-state disks and
other disks, the value Mixed is shown.</dd>
</dlentry>
<dlentry id="pTierAvailableSpaceSSDGiB">
<dt>SSD Available Space (GiB)</dt>
<dd>The amount of storage space that is available on the solid-state drives that can be used by <tm
tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>.</note>
</dd>
</dlentry>
<dlentry id="pTierCapacitySSDGiB">
<dt>SSD Capacity (GiB)</dt>
<dd>The total amount of storage space on the solid-state drives that can be used by <tm tmtype="reg"
trademark="Easy Tier">Easy Tier</tm> for retiering the volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="pStatus">
<dt>Status</dt>
<dd>The status of a pool. Statuses include Normal, Warning, Error, and Unknown. Use the status to
determine the condition of a pool, and if any actions must be taken. For example, if a pool has an
Error status, take immediate action to correct the problem.</dd>
</dlentry>
<dlentry>
<dt>Storage System</dt>
<dd id="pStorageSystem">The name of the storage system that contains a pool. This name was defined
when the storage system was added to <keyword conref="fqz0_entities.dita#fqz0_entities/ktpc_short"
/>. If a name was not defined, the ID of the storage system is displayed.</dd>
<dd id="pSaaSStorageSystem">The name of the storage system that contains a pool. This name was
defined when the storage system was added to <keyword
conref="fqz0_entities.dita#fqz0_entities/tpc_saas_short"/>. If a name was not defined, the ID of the
storage system is displayed.</dd>
</dlentry>
<dlentry id="common_sync_replica_vols">
<dt>Synchronous Replica Volumes</dt>
<dd>The number of volume pairs that are in Metro Mirror relationships.</dd>
</dlentry>
</dl>
<dl id="descriptions_pools">
<dlentry>
<dt>Thin Provisioning</dt>
<dd id="pThinProvisioning">Storage pools that are used as thin-provisioning pools or as non
thin-provisioning pools. The value is Yes if the storage pool is a thin-provisioned pool. The value
is No if the storage pool is a non thin-provisioned pool. This value applies only to Hitachi
<keyword conref="fqz0_entities.dita#fqz0_entities/kvspsys_pl"/>.</dd>
<dd id="pSaaSThinProvisioning">Storage pools that are used as thin-provisioning pools or as non
thin-provisioning pools. The value is Yes if the storage pool is a thin-provisioned pool. The value
is No if the storage pool is a non thin-provisioned pool.</dd>
</dlentry>
<dlentry id="pTier">
<dt>Tier</dt>
<dd>The tier level of pools on storage virtualizers. If the pool is not assigned a tier level, the
cell is blank. To set or change the tier level, select one or more pools. Right-click, and then
select <menucascade>
<uicontrol>Set Tier</uicontrol>
</menucascade>. Note that tier changes to a parent or child pool affect all pool family
members.</dd>
</dlentry>
<dlentry id="pTier0AvailableSpaceGiB">
<dt>Tier 0 Flash Available Capacity (GiB)</dt>
<dd>The amount of storage space that is available on the Tier 0 flash solid-state drives that can be
used by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the volume extents in
the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vTier0CapacityGiB">
<dt>Tier 0 Flash Capacity (GiB)</dt>
<dd>The amount of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Tier 0 flash drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vTier0CapacityPercent">
<dt>Tier 0 Flash Capacity (%)</dt>
<dd>The percentage of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Tier 0 flash drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vTier1CapacityGiB">
<dt>Tier 1 Flash Capacity (GiB)</dt>
<dd>The amount of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Tier 1 flash, read-intensive drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vTier1CapacityPercent">
<dt>Tier 1 Flash Capacity (%)</dt>
<dd>The percentage of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Tier 1 flash, read-intensive drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vTier2CapacityGiB">
<dt>Tier 2 Flash Capacity (GiB)</dt>
<dd>The amount of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Tier 2 flash, high-capacity drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="vTier2CapacityPercent">
<dt>Tier 2 Flash Capacity (%)</dt>
<dd>The percentage of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Tier 2 flash, high-capacity drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="pTier1AvailableSpaceGiB">
<dt>Tier 1 Flash Available Capacity (GiB)</dt>
<dd>The amount of storage space that is available on the Tier 1 flash, read-intensive solid-state
drives that can be used by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the
volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="pTier0CapacityGiB">
<dt>Tier 0 Flash Capacity (GiB)</dt>
<dd>The total amount of storage space on the Tier 0 flash solid-state drives that can be used by <tm
tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="pTier1CapacityGiB">
<dt>Tier 1 Flash Capacity (GiB)</dt>
<dd>The total amount of storage space on the Tier 1 flash, read-intensive solid-state drives that
can be used by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the volume
extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="pTier2FlashCapacity">
<dt>Tier 2 Flash Capacity (GiB)</dt>
<dd>The total capacity on Tier 2 flash, high-capacity drives in the pool. <tm trademark="Easy Tier"
tmtype="reg">Easy Tier</tm> can use these drives to retier the volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="pTier2FlashAvailableCapacity">
<dt>Tier 2 Flash Available Capacity (GiB)</dt>
<dd>The available capacity on Tier 2 flash, high-capacity drives in the pool. <tm
trademark="Easy Tier" tmtype="reg">Easy Tier</tm> can use these drives to retier the volume extents
in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="pTierAvailableSpaceHDDGiB">
<dt>Tier Available Space HDD (GiB)</dt>
<dd>The unused storage space on the hard disk drives in a pool. </dd>
<dd id="tier_unused_sapce_hdd">For <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, the
unused storage space can include the unused space on enterprise and nearline hard disk drives.</dd>
</dlentry>
<dlentry id="pEasyTierDistribution">
<dt>Tier Distribution (%)</dt>
<dd>The distribution of the volume extents across each <tm tmtype="reg" trademark="Easy Tier">Easy
Tier</tm> drive class or tier in the pool, such as the percentage of volume extents on Tier 0 and
Tier 1 flash drives, on Enterprise hard disk drives, and on Nearline hard disk drives.</dd>
</dlentry>
<dlentry id="pEasyTierDistribution_536">
<dt>Tier Distribution (%)</dt>
<dd>The distribution of volume extents across the <tm trademark="Easy Tier" tmtype="reg">Easy
Tier</tm> drive classes in a pool, such as the percentage of volume extents on SCM drives, Tier 0
and Tier 1 flash drives, Enterprise hard disk drives, and Nearline hard disk drives.</dd>
</dlentry>
<dlentry id="pEasyTierDistribution_537">
<dt>Tier Distribution (%)</dt>
<dd>The distribution of volume extents across the <tm trademark="Easy Tier" tmtype="reg">Easy
Tier</tm> drive classes in a pool, such as the percentage of volume extents on SCM drives, Tier 0,
Tier 1, and Tier 2 flash drives, Enterprise hard disk drives, and Nearline hard disk drives.</dd>
</dlentry>
<dlentry>
<dt>Provisioned Capacity (GiB)</dt>
<dd id="pTotalVolumeCapacityGIB">(Previously known as Total Volume Capacity) <p>The total amount of
provisioned capacity of volumes within the pool. If the pool is a parent pool, it also includes the
storage capacity that can be made available to the volumes in the child pools.<ph
audience="onpremonly">For <keyword conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"
/></ph></p><ph audience="onpremonly"> non-thin provisioned pool space, this value is the sum of the
capacity of regular host-accessible volumes. Volumes that are used for thin-provisioning (pool
volumes) are not included.</ph></dd>
<dd id="pSaaSTotalVolumeCapacityGIB" audience="offpremonly">The total amount of provisioned capacity
of volumes within the pool. If the pool is a parent pool, it also includes the storage capacity that
can be made available to the volumes in the child pools.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="pTotalDataReductionSavings">
<dt>Total Capacity Savings (%)</dt>
<dd otherprops="536_183345org">The estimated amount and percentage of capacity that is saved by
using data deduplication, data compression, and thin provisioning, across all volumes in the
pool.</dd>
<dd otherprops="536_183345new">The estimated amount and percentage of capacity that is saved by
using data deduplication, pool compression, thin provisioning, and drive compression, across all
volumes in the pool.</dd>
<dd>The following formula is used to calculate the amount of storage space that is saved:
<codeblock>Provisioned Capacity − Used Capacity</codeblock>
</dd>
<dd>The following formula is used to calculate the percentage of capacity that is saved:
<codeblock>((Provisioned Capacity − Used Capacity) ÷ Provisioned Capacity) × 100 </codeblock><note
othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/> storage systems with firmware version 11.6 or
later, and resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>. </note></dd>
</dlentry>
<dlentry id="p_total_vol_capacity">
<dt>Provisioned Capacity (GiB)</dt>
<dd>(Previously known as Total Volume Capacity) The total amount of storage capacity that can be
made available to the standard- and thin-provisioned volumes in the pool. If the pool is a parent
pool, it also includes the storage capacity that can be made available to the volumes in the child
pools. For <keyword conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"/> non-thin
provisioned pool capacity, this value is the sum of the capacity of regular host-accessible volumes.
Volumes that are used for thin-provisioning (pool volumes) are not included.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="p_offprem_total_vol_capacity">
<dt>Provisioned Capacity (GiB)</dt>
<dd>(Previously known as Total Volume Capacity) The total amount of storage capacity that can be
made available to the standard- and thin-provisioned volumes in the pool. If the pool is a parent
pool, it also includes the storage capacity that can be made available to the volumes in the child
pools.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="pUnallocatableVolumeSpaceGiB">
<dt>Overprovisioned Capacity (GiB)</dt>
<dd>(Previously known as Unallocatable Volume Space) The capacity that cannot be used by volumes
because the physical capacity of the pool cannot meet the demands for provisioned capacity. The
following formula is used to calculate this value:</dd>
<dd><codeblock>[Provisioned Capacity − Capacity]</codeblock> In thin-provisioned environments, it is
possible to over commit (over provision) storage in a pool by creating volumes with more provisioned
capacity than can be physically allocated in the pool. This value represents the amount of volume
capacity that cannot be allocated based on the current capacity of the pool. <ph>For <keyword
conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"/> non-thin provisioned pool capacity,
this value is always zero.</ph></dd>
<dd>
<note othertype="Availability" type="other">All storage systems, except <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>. </note>
</dd>
</dlentry>
<dlentry id="pSaaSUnallocatableVolumeSpaceGiB">
<dt>Overprovisioned Capacity (GiB)</dt>
<dd>(Previously known as Unallocatable Volume Space) The amount of space that cannot be used by
volumes because the physical capacity of the pool cannot meet the demands for provisioned capacity.
The following formula is used to calculate this value:</dd>
<dd><codeblock>[Provisioned Capacity − Capacity]</codeblock> In thin-provisioned environments, it is
possible to over commit (over provision) storage in a pool by creating volumes with more provisioned
capacity than can be physically allocated in the pool. This value represents the amount of volume
space that cannot be allocated based on the current capacity of the pool.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="pUnallocatedVolumeSpaceGiB">
<dt>Available Volume Capacity (GiB)</dt>
<dd>(Previously known as Effective Unallocated Volume Space) The total amount of remaining capacity
that can be used by the existing volumes in the pools. The following formula is used to calculate
this value:</dd>
<dd>
<codeblock>Provisioned Capacity − Used Capacity</codeblock> The capacity that is used by
thin-provisioned volumes is typically less than their provisioned capacity. Therefore, the available
capacity represents the difference between the provisioned capacity and the used capacity for all
the volumes in the pool. For <keyword conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"/>
non-thin provisioned pool capacity, the unused volume capacity is always zero.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems, except <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>. </note>
</dd>
</dlentry>
<dlentry id="pSaaSUnallocatedVolumeSpaceGiB">
<dt>Available Volume Capacity (GiB)</dt>
<dd>(Previously known as Effective Unallocated Volume Space) The total amount of remaining space
that can be used by the volumes in the pools. The following formula is used to calculate this
value:</dd>
<dd>
<codeblock>[Provisioned Capacity − Used Capacity]</codeblock> The capacity that is used by
thin-provisioned volumes is typically less than their provisioned capacity. Therefore, the available
capacity represents the difference between the provisioned capacity and the used capacity for all
the volumes in the pool.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry>
<dt>Unmapped Capacity (GiB)</dt>
<dd id="pUnassignedVolumeSpaceGiB" audience="onpremonly">(Previously known as Unassigned Volume
Space) The total amount of space in the volumes that are not assigned to hosts. For <keyword
conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"/> non-thin provisioning pool space, this
value is the sum of unassigned regular host-accessible volumes. Volumes that are used for
thin-provisioning (pool volumes) are not included. </dd>
<dd id="pSaaSUnassignedVolumeSpaceGiB" audience="offpremonly"> The total amount of space in the
volumes that are not assigned to hosts. For a thin-provisioning pool, this value includes the
provisioned capacity of thin-provisioned volumes, which might exceed the total space in the
pool.</dd>
<dd id="NoteAllStorageSystems">
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="common_unprotected_volumes">
<dt>Unprotected Volumes</dt>
<dd>The number of volumes that don't have copies and that are not the source volume or the target
volume of a copy services relationship.</dd>
</dlentry>
<dlentry id="pUnreservedPoolSpace">
<dt>Unreserved Capacity (GiB)</dt>
<dd>(Previously known as Unreserved Pool Space) The capacity in a pool that is not used by volumes,
and is not reserved by pending or scheduled provisioning tasks.</dd>
</dlentry>
<dlentry id="pUnusedSpaceGiB">
<dt>Reserved Volume Capacity</dt>
<dd>(Previously known as Unused Space) The amount of pool capacity that is reserved but has not been
used yet to store data on the thin-provisioned volume.</dd>
<dd>
<note othertype="Availability" type="other">Resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="pVirtualAllocationPercentage">
<dt>Provisioned Capacity (%)</dt>
<dd>(Previously known as Virtual Allocation) The percentage of the physical capacity that is
committed to the provisioned capacity of the volumes in the pool. If the value exceeds 100%, the
physical capacity doesn't meet the demands for provisioned capacity. The following formula is used
to calculate this value:</dd>
<dd>
<codeblock>[(Provisioned Capacity ÷ Capacity) × 100]</codeblock>
<ph>This value is available for all pools.</ph></dd>
<dd audience="onpremonly">For <keyword conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_short"/>
non-thin provisioned pool space, the following formula is used to calculate this value:
<codeblock>[(Used Capacity ÷ Capacity) × 100]</codeblock>
</dd>
<dd>Example: If the provisioned capacity percentage is 200% for a total storage pool size of 15 GiB,
then the provisioned capacity that is committed to the volumes in the pool is 30 GiB. This
configuration means that twice as much capacity is committed than is physically contained in the
pool. If the provisioned capacity percentage is 100% for the same pool, then the provisioned
capacity that is committed to the pool is 15 GiB. This configuration means that all the physical
capacity of the pool is already used by volumes.</dd>
<dd>A provisioned capacity percentage that is higher than 100% is considered aggressive because
insufficient physical capacity is available in the pool to satisfy the maximum allocation for all
the thin-provisioned volumes in the pool.  In such cases, you can use the value for Shortfall (%) to
estimate how critical the shortage of capacity is for a pool. </dd>
<dd>You can hover the mouse pointer over the percentage bar to view values for the provisioned
capacity and capacity.
<!--Total volume capacity total is the storage space on all of the volumes in the pool. For thin-provisioned volumes, this value includes provisioned capacity. Capacity is the total amount of storage in the pool.--></dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="pSaaSVirtualAllocationPercentage">
<dt>Provisioned Capacity (%)</dt>
<dd>(Previously known as Virtual Allocation) The percentage of the physical capacity that is
committed to the provisioned capacity of the volumes in the pool. If the value exceeds 100%, the
physical capacity doesn't meet the demands for provisioned capacity. The following formula is used
to calculate this value:</dd>
<dd>
<codeblock>[(Provisioned Capacity ÷ Capacity) × 100]</codeblock>
</dd>
<dd>Example: If the provisioned capacity percentage is 200% for a total storage pool size of 15 GiB,
then the provisioned capacity that is committed to the volumes in the pool is 30 GiB. This
configuration means that twice as much space is committed than is physically contained in the pool.
If the provisioned capacity percentage is 100% for the same pool, then the provisioned capacity that
is committed to the pool is 15 GiB. This configuration means that all the physical capacity of the
pool is already used by volumes.</dd>
<dd>A provisioned capacity percentage that is higher than 100% is considered aggressive because
insufficient physical capacity is available in the pool to satisfy the maximum allocation for all
the thin-provisioned volumes in the pool.  In such cases, you can use the value for Shortfall (%) to
estimate how critical the shortage of space is for a pool. </dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="pVolumes">
<dt>Volumes</dt>
<dd>The number of volumes that are allocated from a pool. </dd>
</dlentry>
<dlentry id="common_pools_zero_capacity">
<dt>Zero Capacity</dt>
<dd>
<ph>The capacity information that is collected over 180 days is analyzed to determine, based on
historical storage consumption, when the pools will run out of capacity. The pools that have already
run out of capacity are marked as depleted. For the other pools, a date is provided so that you know
when the pools are projected to run out of capacity. None is shown when a trend in storage
consumption can't be detected because the pool's storage isn't being consumed or because not enough
data was collected to predict storage consumption.</ph>
<ph otherprops="537_198366">When you set the capacity limit for pools, the values shown for
<uicontrol>Zero Capacity</uicontrol> are readjusted to take into account the capacity limit of the
pool. The date will represent when the capacity limit of the pool is reached. If the pool has
already reached the capacity limit, depleted is shown.</ph></dd>
<dd>For a demonstration of the Zero Capacity value, watch the video at <xref format="html"
href="https://youtu.be/hIhpG_TymS8" scope="external"/>.</dd>
</dlentry>
</dl>
</section>
<section>
<title>BLOCK VOLUMES DEFINITIONS</title>
<p>
<dl>
<dlentry id="vAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a volume as acknowledged. An acknowledged status
indicates that the status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a volume is Error, then the status of the related storage system
is also Error. If the Error status of the volume is acknowledged, then its status is not used to
determine the overall status of the storage system. In this case, if the other internal resources of
the storage system are Normal, then the status of the storage system is also Normal. </dd>
</dlentry>
<dlentry id="vAggregate" otherprops="5215_161230new">
<dt>Aggregate</dt>
<dd>The aggregate that contains the volume. An aggregate is a collection of RAID groups that consist
of physical disks. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="vAllocatedSpaceGiB">
<dt>Used Capacity (GiB)</dt>
<dd>(Previously known as Allocated Space) The amount of space that is used by the volume. For
thin-provisioned volumes, the space that is used by the volume might be less than the provisioned
capacity of the volume. <ph otherprops="537_199508new">In <keyword
conref="fqz0_entities.dita#fqz0_entities/k3parss"/>, this value represents the total used space for
a volume, that is the total of user, admin, and snap used space.</ph></dd>
<!--<dd>Allocated space is the same as used space on all storage systems, except for resources that run
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>. These resources might
have more allocated space than used space if the storage administrator preallocated some space for
thin-provisioned volumes when the volumes were created.</dd>-->
<dd>
<note type="other" othertype="Remark">If the volume is stored at self-compressing drives, the used
capacity does not reflect the inline disk compression savings.</note>
<note othertype="Availability" type="other">All storage systems, and Volumes from SpecV Data
Reduction Pools, except <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"
/> and <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>.</note>
</dd>
</dlentry>
<dlentry id="vAutoExpand">
<dt>Auto Expand</dt>
<dd>Shows whether a thin-provisioned volume automatically expands its allocated capacity as more of
its space is used. The value is shown only for volumes on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vCapacityGiB">
<dt>Capacity (GiB)</dt>
<dd>The total amount of storage space that is committed to a volume. For thin-provisioned volumes,
this value represents the provisioned capacity of the volume. <ph otherprops="537_199508new">In an
<keyword conref="fqz0_entities.dita#fqz0_entities/kxss"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, this value represents the physical
("hard") capacity of the volume, not the provisioned ("soft") capacity.</ph>
<ph otherprops="537_199508new">In <keyword conref="fqz0_entities.dita#fqz0_entities/k3parss"/>, this
value represents the total reserved space for a volume, that is the total of user, admin, and snap
reserved space.</ph></dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="vCompressed">
<dt>Compressed</dt>
<dd>Shows whether a storage volume is compressed. <note othertype="Availability" type="other"
><keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/> storage systems with firmware version 11.6 or
later, and resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note></dd>
</dlentry>
<dlentry id="vSaaSCompressed">
<dt>Compressed</dt>
<dd>Shows whether a storage volume is compressed. Available only for volumes in <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/> storage systems and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</dd>
</dlentry>
<dlentry id="vCompressionSavings">
<dt>Compression Savings (%)</dt>
<dd>The estimated amount and percentage of capacity that is saved by using data compression.</dd>
<dd>The following formula is used to calculate the amount of storage space that is saved:
<codeblock>written capacity − compressed size</codeblock></dd>
<dd>The following formula is used to calculate the percentage of capacity that is
saved:<codeblock>((written capacity  − compressed size) ÷ written capacity) × 100</codeblock></dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/> storage systems with firmware version 11.6 or
later, and resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
<dd>
<note type="other" othertype="Exception">For compressed volumes that are also deduplicated, on
storage systems that run <keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>,
this column is blank.</note>
</dd>
</dlentry>
<dlentry id="vSaaSCompressionSavingsPercentage">
<dt>Compression Savings (%)</dt>
<dd>The percentage of space that is being saved by the compressed volume.</dd>
<dd>
<note othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"
/> and <keyword conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems
that are configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="vCopies">
<dt>VDisk Mirror Copies</dt>
<dd>The number of secondary copies (virtual disk copies) for a volume. The primary copy of a virtual
disk is not counted as a mirror. This information is shown only for volumes on storage systems that
run <keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vSaaSCopies">
<dt>Copies</dt>
<dd>The number of secondary copies (virtual disk copies) for a volume. The primary copy of a virtual
disk is not counted as a mirror. This information is shown only for volumes on storage systems that
run <keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vCopyID">
<dt>Copy ID</dt>
<dd>The identifier for the volume copy. Each <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> volume has a copy ID of
<codeph>0</codeph> or <codeph>1</codeph>, even if the volume is not in a mirrored volume
relationship.<p>For mirrored volumes, the copy ID distinguishes between the primary and secondary
volume copies.</p>
</dd>
<dd>
<note othertype="Availability" type="other">Volumes on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vDeduplicated">
<dt>Deduplicated</dt>
<dd>Shows whether a storage volume is deduplicated. <note othertype="Availability" type="other"
><keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and resources that run
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> 8.1.3 or
later.</note></dd>
</dlentry>
<dlentry id="vEasyTier">
<dt><tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm></dt>
<dd>The operating mode determines whether <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> is
enabled and how tiering is managed when it is enabled. For example, <tm tmtype="reg"
trademark="Easy Tier">Easy Tier</tm> can be configured to tier all pools, single-tier pools (pools
with one class drive), or multitier pools (pools with multiple class drives).</dd>
<dd>You can configure <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/> 6.1, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage, and <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"
/>.</dd>
</dlentry>
<dlentry id="vEncryption">
<dt>Encryption</dt>
<dd>Shows whether a volume is encrypted. The value <systemoutput>N/A</systemoutput> indicates that
encryption is not supported for this volume.</dd>
</dlentry>
<dlentry id="vFastWriteState">
<dt>Fast Write State</dt>
<dd>Shows the cache state for a volume, such as empty, not empty, corrupted, and repairing. The
corrupted state indicates that you must recover the volume by using one of the
<cmdname>recovervdisk</cmdname> commands for the storage system. The repairing state indicates that
repairs initiated by a <cmdname>recovervdisk</cmdname> command are in progress. Available only for
volumes in <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/> storage systems and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems.</dd>
</dlentry>
<!--<dlentry id="vFileSystemPool"><dt>File System Pool</dt><dd>TBD</dd></dlentry>-->
<dlentry id="vSaaSFileSystemPool" audience="offpremonly">
<dt>File System Pool</dt>
<dd>The name of the storage pool in which a volume is a member.</dd>
</dlentry>
<dlentry>
<dt>Format</dt>
<dd id="vFormat">The format of the volumes that are allocated from a pool, such as FB (fixed block)
or CKD (count key data). Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> volumes.</dd>
<dd id="vSaaSFormat">The format of the volumes that are allocated from a pool, such as FB (fixed
block) or CKD (count key data). Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> volumes.</dd>
</dlentry>
<dlentry id="vFormatted">
<dt>Formatted</dt>
<dd>Shows whether a volume is formatted. This information is available only for volumes on storage
systems that run <keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vSaaSFormatted">
<dt>Formatted</dt>
<dd>Shows whether a volume is formatted. Available only for volumes in storage systems that run
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vGrainSizeKiB">
<dt>Grain Size (KiB)</dt>
<dd>The grain size with which a thin-provisioned volume was created. This value is typically 32, 64,
128, or 256 KiB. Larger grain sizes maximize performance, whereas smaller grain sizes maximize space
efficiency. Grain sizes also limit the maximum provisioned capacity of the volume. This information
is available only for volumes on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vSaaSGrainSizeKiB">
<dt>Grain Size (KiB)</dt>
<dd>The grain size with which a thin-provisioned volume was created. This value is typically 32, 64,
128, or 256 KiB. Larger grain sizes maximize performance, whereas smaller grain sizes maximize space
efficiency. Grain sizes also limit the maximum provisioned capacity of the volume. Available only
for volumes in storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry>
<dt>Hosts</dt>
<dd id="vHosts">The name of the host to which a volume is assigned. This name is the host name as
defined on the storage system, and is not the name of the server that was discovered by a <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra"/>. If more than one host is assigned, the number of
hosts is displayed. For storage systems that are managed by a CIM agent, the host name in this
column might not match the configured host name on the storage system. Instead, the host name might
be replaced by the WWPN of the host port or text that is generated by the CIM agent. </dd>
<dd id="vSaaSHosts">The name of the host to which a volume is assigned. This name is the host name
as defined on the storage system. If more than one host is assigned, the number of hosts is
displayed. For storage systems that are managed by a CIM agent, the host name in this column might
not match the configured host name on the storage system. Instead, the host name might be replaced
by the WWPN of the host port or text that is generated by the CIM agent. </dd>
</dlentry>
<dlentry id="vID">
<dt>ID</dt>
<dd>The identifier for a volume, such as a serial number or internal ID.</dd>
</dlentry>
<dlentry id="vIOgroup">
<dt>I/O Group</dt>
<dd>The name of the I/O Group to which a volume is assigned. This information is available only for
volumes on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vSaaSIOgroup">
<dt>I/O Group</dt>
<dd>The name of the I/O Group to which a volume is assigned. Available only for volumes in storage
systems that run <keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vLastDataCollection">
<dt>Last Data Collection</dt>
<dd>The most recent date and time when data was collected about the storage system that contains a
volume.</dd>
</dlentry>
<dlentry>
<dt>LSS or LCU</dt>
<dd id="vLSSorLSU"> The logical subsystem (LSS) for fixed block volumes, or the logical control unit
(LCU) for count key data volumes. Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> volumes.</dd>
<dd id="vSaaSLSSorLSU"> The logical subsystem (LSS) for fixed block volumes, or the logical control
unit (LCU) for count key data volumes. Available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> volumes.</dd>
</dlentry>
<dlentry id="vName">
<dt>Name</dt>
<dd>The name that was assigned to a volume when it was created. If the volume is a count-key-data
(CKD) device and the volume serial number (VOLSER) is defined, the VOLSER is displayed in the Name
column.</dd>
</dlentry>
<dlentry>
<dt>Node</dt>
<dd id="vController">For <keyword conref="fqz0_entities.dita#fqz0_entities/kds4000"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds5000"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, this value represents the name of the disk
controller that is associated with the volume. </dd>
<dd id="vSaaSController">For <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, this
value represents the name of the node to which a volume is associated.</dd>
<dd id="vSaaSController2">For <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/> storage
systems and <keyword conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage
systems that are configured with block storage, this value represents the name of the preferred node
within the I/O Group to which a volume is assigned. </dd>
</dlentry>
<dlentry id="vOriginalCapacityPool">
<dt>Original Capacity Pool</dt>
<dd>The name of the capacity pool from which a volume was provisioned. The storage resource on which
the volume was created might no longer be a member of the capacity pool.</dd>
</dlentry>
<dlentry id="vPhysicalAllocationPercentage">
<dt>Used Capacity (%)</dt>
<dd>(Previously known as Physical Allocation) The percentage of the capacity of the volume that is
used. The space that is used by a thin-provisioned volume might be less than the capacity of the
volume.</dd>
<dd>This value is determined by the formula, <varname>Used Capacity ÷ Capacity × 100</varname>. For
example, if the capacity that is used by volumes is 50 GiB for a volume size of 200 GiB, used
capacity is 25%. </dd>
<dd>
<note type="other" othertype="Remark">If the volume is stored at self-compressing drives, the used
capacity does not reflect the inline disk compression savings.</note>
</dd>
<dd>
<note othertype="Availability" type="other">All storage systems, and Volumes from SpecV Data
Reduction Pools, except <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"
/> and <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>.</note>
</dd>
</dlentry>
<dlentry id="vPool">
<dt>Pool</dt>
<dd>The name of the storage pool in which a volume is a member.</dd>
</dlentry>
<dlentry id="vRAIDlevel">
<dt>RAID Level</dt>
<dd>The RAID level of a volume, such as RAID 5 and RAID 10. The RAID level affects the performance
and fault tolerance of the volume. The value <codeph>None</codeph> indicates that the volume is on a
single disk and performance or fault tolerance is not improved.</dd>
</dlentry>
<dlentry id="vServiceClass">
<dt>Service Class</dt>
<dd>The name of the block storage service class that is associated with the volume. A block storage
service class typically represents a particular quality of service.<note othertype="Learn more"
type="other">Go to <xref href="tpch_r_l2_serviceclass.dita"/>.</note></dd>
</dlentry>
<dlentry id="vShortfallPercentage">
<dt>Shortfall (%)</dt>
<dd>The difference between the remaining unused volume capacity and the available capacity of the
pool that the volume is in, expressed as a percentage of the remaining unused volume capacity. The
shortfall represents the relative risk of running out of space for an overallocated thin-provisioned
volume. If the pool has sufficient available capacity to satisfy the remaining unused volume
capacity, no shortfall exists. As the remaining unused volume capacity grows, or as the available
pool capacity decreases, the shortfall increases and the risk of running out of space becomes
higher. If the available capacity of the pool is exhausted, the shortfall is 100% and any volumes
that are not yet fully allocated have run out of space.</dd>
<dd>The following formula is used to calculate the shortfall percentage: <varname>Overprovisioned
Capacity ÷ Unused Volume Capacity × 100</varname>. When the shortfall exceeds 100%, a warning icon
is shown.</dd>
<dd>
<note othertype="Availability" type="other">Volumes on <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, <tm tmtype="tm" trademark="FlashSystem"
tmclass="IGNORE">FlashSystem</tm> storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/hitachi_vsp_full"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems.</note>
</dd>
</dlentry>
<dlentry id="vSaaS_ShortfallPercentage">
<dt>Shortfall (%)</dt>
<dd>The difference between the remaining unused volume capacity and the available capacity of the
pool that the volume is in, expressed as a percentage of the remaining unused volume capacity. The
shortfall represents the relative risk of running out of space for an overallocated thin-provisioned
volume. If the pool has sufficient available capacity to satisfy the remaining unused volume
capacity, no shortfall exists. As the remaining unused volume capacity grows, or as the available
pool capacity decreases, the shortfall increases and the risk of running out of space becomes
higher. If the available capacity of the pool is exhausted, the shortfall is 100% and any volumes
that are not yet fully allocated have run out of space.</dd>
<dd>The following formula is used to calculate the shortfall percentage: <varname>Overprovisioned
Capacity ÷ Unused Volume Capacity × 100</varname>. </dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, <tm tmtype="tm" trademark="FlashSystem"
tmclass="IGNORE">FlashSystem</tm> storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage, <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>, and
<keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="vStatus">
<dt>Status</dt>
<dd>The status of a volume. Statuses include Normal, Warning, Error, Unknown, Online, Offline,
Syncing, Degraded, Excluded, and Unreachable. Use the status to determine the condition of the
volume, and if any actions must be taken. For example, if a volume has an Error status, take
immediate action to correct the problem. Syncing status indicates that a <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/> or <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_short"/> volume is part of a mirrored pair and is in
the process of synchronizing with the primary volume. Syncing status will change when a subsequent
probe detects a status change in the volume.</dd>
</dlentry>
<dlentry id="vStorageSystem">
<dt>Storage System</dt>
<dd>The name of the storage system that contains a volume. This name was defined when the storage
system was added. If a name was not defined, the ID of the storage system is displayed.</dd>
</dlentry>
<dlentry id="vSVM">
<dt>Storage Virtual Machine</dt>
<dd>The storage virtual machine (SVM) to which the volume belongs. An SVM is a logical entity that
is used to serve data to clients and hosts. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/knetapp_ontap9"/> storage systems.</note></dd>
</dlentry>
<dlentry id="vCopyRelationship">
<dt>Copy Service Role</dt>
<dd>Shows whether a volume is in a replication relationship that creates a snapshot or point-in-time
copy of the volume on a specified target volume. A volume can either be a source, target, or both a
target for one copy pair and a source for a different copy pair. In storage systems, this
relationship might be referred to as a <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm>,
snapshot, or point-in-time copy relationship. A volume can have one of the following properties:<dl>
<dlentry>
<dt>Source</dt>
<dd>The volume is the source of the relationship.</dd>
</dlentry>
<dlentry>
<dt>Target</dt>
<dd>The volume is the target of the relationship.</dd>
</dlentry>
<dlentry>
<dt>Source and Target</dt>
<dd>The volume is a target for one copy pair and a source for a different copy pair.</dd>
</dlentry>
<dlentry>
<dt>blank</dt>
<dd>The volume is not part of any copy relationship.</dd>
</dlentry>
<dlentry>
<dt>Unavailable</dt>
<dd>Information about a copy relationship on this volume is not available.</dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="vStorageVirtualizer">
<dt>Storage Virtualizer</dt>
<dd>The name of the storage virtualizer that is managing a volume. A storage virtualizer is a
storage system that virtualizes storage space from internal storage or from another storage system.
Examples of storage virtualizers include <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/>
and <keyword conref="fqz0_entities.dita#fqz0_entities/kstor_short"/>. A value is shown in this
column only if the volume is managed by a storage virtualizer and data is collected for the storage
virtualizer. </dd>
</dlentry>
<dlentry id="vTarget">
<dt>VDisk Mirror Role</dt>
<dd>For mirrored volumes, the mirror role identifies the primary and secondary volume copies. If the
volume is not a mirrored volume, the column is blank.<note othertype="Availability" type="other"
>Volumes on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note></dd>
</dlentry>
<dlentry id="vTicket">
<dt>Ticket</dt>
<dd>A ticket identifier that was associated with the volume for tracking purposes. The ticket
identifier was specified when the volume was created.</dd>
</dlentry>
<dlentry id="vTierCapacityHDDGiB">
<dt>Tier Capacity HDD (GiB)</dt>
<dd>The total storage space on the hard disk drives that are participating in, or are eligible to
participate in, the <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> optimization for a
volume.</dd>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, the total storage space can
include the space on enterprise and nearline hard disk drives.</dd>
</dlentry>
<dlentry id="vTierCapacitySSDGiB">
<dt>Tier Capacity SSD (GiB)</dt>
<dd>The total storage space on the solid-state drives that are participating in, or are eligible to
participate in, the <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> optimization for a
volume.</dd>
</dlentry>
<dlentry id="vEasyTierVolumeDistribution">
<dt>Tier Distribution</dt>
<dd>The distribution of the volume's extents across each <tm tmtype="reg" trademark="Easy Tier">Easy
Tier</tm> drive class or tier, such as the percentage of the volume's extents on Tier 0 and Tier 1
flash drives, on Enterprise hard disk drives, and on Nearline hard disk drives.</dd>
</dlentry>
<dlentry id="vEasyTierVolumeDistribution_536">
<dt>Tier Distribution</dt>
<dd>The distribution of volume extents across the <tm trademark="Easy Tier" tmtype="reg">Easy
Tier</tm> drive classes, such as the percentage of the volume's extents on SCM drives, Tier 0 and
Tier 1 flash drives, Enterprise hard disk drives, and Nearline hard disk drives. </dd>
</dlentry>
<dlentry id="vEasyTierVolumeDistribution_537">
<dt>Tier Distribution</dt>
<dd>The distribution of volume extents across the <tm trademark="Easy Tier" tmtype="reg">Easy
Tier</tm> drive classes, such as the percentage of the volume's extents on SCM drives, Tier 0, Tier
1, and Tier 2 flash drives, Enterprise hard disk drives, and Nearline hard disk drives. </dd>
</dlentry>
<dlentry id="vThinProvisioned">
<dt>Thin Provisioned</dt>
<dd>Shows whether a volume is a thin-provisioned volume, and the type of thin-provisioning that is
used for the volume. A thin-provisioned volume is a volume with a provisioned capacity that is
different from its real capacity. Not all the storage capacity of the volume is allocated when the
volume is created, but is allocated over time as needed. Thin-provisioned volumes on a <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage system can be defined as Extent
Space-Efficient (ESE) or as Track Space-Efficient (TSE).</dd>
</dlentry>
<dlentry id="vUnallocatedSpaceGiB">
<dt>Available Capacity (GiB)</dt>
<dd id="availablespace">(Previously known as Unallocated Space) The total amount of remaining space
that can be used by the volume. That is, the capacity that is not used by thin-provisioned volumes.
This value is determined by the formula, <varname>Capacity - Used Capacity</varname>. <p>For
<keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, and Volumes from SpecV Data
Reduction Pools, this value is not available.</p></dd>
</dlentry>
<dlentry id="vBlockSize">
<dt>Block Size</dt>
<dd>The size of the data blocks that are written to disk for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>. The size depends on the format of the volume,
which is either fixed block (FB) or count key data (CKD). For FB volumes, the block size is 512
bytes. For CKD volumes, the block size depends on the device emulation mode and model, and is equal
to the number of bytes per cylinder. For example, for disk model 3390, the block size for a CKD
volume is 849960 bytes.</dd>
</dlentry>
<dlentry id="block_v_unused_space_gib">
<dt>Reserved Volume Capacity</dt>
<dd>(Previously known as Unused Space) The amount of pool capacity that is reserved but has not been
used yet to store data on the thin-provisioned volume.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="vUnusedSpaceGiB">
<dt>Reserved Volume Capacity</dt>
<dd>(Previously known as Unused Space) The amount of pool capacity that is reserved but has not been
used yet to store data on the thin-provisioned volume. </dd>
<dd>Available only for resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> and are configured with block
storage.</dd>
</dlentry>
<dlentry id="vUniqueID">
<dt>Unique ID</dt>
<dd>The ID that is used to uniquely identify a volume across multiple storage systems.</dd>
</dlentry>
<dlentry id="vVirtualAllocationPercentage">
<dt>Provisioned Capacity (%)</dt>
<dd>(Previously known as Virtual Allocation) The percentage of the volume capacity that is written
by the assigned host. The used capacity and provisioned capacity percentages are different when data
reduction reduces the physical capacity that is required to store the written data. </dd>
<dd>The following formula is used to determine the provisioned capacity:
<codeblock>(written capacity ÷ total capacity) × 100</codeblock></dd>
<dd>For example, if the space that is written to the volume is 50 GiB for a volume size of 200 GiB,
the provisioned capacity is 25%. </dd>
<dd>
<note othertype="Available for" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/> storage systems, and resources that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vVirtualDiskType">
<dt>Virtual Disk Type</dt>
<dd>The type of virtual disk with which a volume was created, such as sequential, striped, image,
and many. Available only for volumes in <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/>
storage systems and <keyword conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family
storage systems that are configured with block storage.</dd>
</dlentry>
<dlentry id="vVirtualizerDisk">
<dt>Virtualizer Disk</dt>
<dd>The managed disk for the virtualizer that corresponds to a volume.</dd>
</dlentry>
<dlentry>
<dt>Volume Number</dt>
<dd id="vVolumeNumber"> The volume number of the volume within the LSS or LCU. Available only for
<keyword conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> volumes.</dd>
<dd id="vSaaSVolumeNumber"> The volume number of the volume within the LSS or LCU. Available only
for <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> volumes.</dd>
</dlentry>
<dlentry id="vWarningLevel">
<dt>Warning Level (%)</dt>
<dd>The warning level that was defined when a thin-provisioned volume was created. This value is
measured either in MiB (10^20 bytes) or a percentage of the total, provisioned capacity of the
volume. The storage system generates a warning if the used capacity of a volume grows enough to
exceed the specified threshold. This information is available only for volumes on storage systems
that run <keyword conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vSaaSWarningLevel">
<dt>Warning Level (%)</dt>
<dd>The warning level that was defined when a thin-provisioned volume was created. This value is
measured either in MiB (10^20 bytes) or a percentage of the total, provisioned capacity of the
volume. The storage system generates a warning if the used capacity of a volume grows enough to
exceed the specified threshold. Available only for volumes in storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</dd>
</dlentry>
<dlentry id="vWrittenSpace">
<dt>Written Capacity (GiB)</dt>
<dd>(Previously known as Written Space) The amount of data that is written from the assigned hosts
to the volume before compression or data deduplication are used to reduce the size of the data. For
example, the written capacity for a volume is 40 GiB. After compression, the volume used space,
which reflects the size of compressed data that is written to disk, is just 10 GiB. <ph
otherprops="537_199508new">This value is not available for <keyword
conref="fqz0_entities.dita#fqz0_entities/k3parss"/>.</ph></dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>BLOCK VOLUME DEFINITIONS FOR COMPRESSED, THIN-PROVISIONED, and EASY TIER</title>
<p>
<dl>
<dlentry id="sevs_allocated_space_gib">
<dt>Used Capacity (GiB)</dt>
<dd>(Previously known as Allocated Space) The amount of space that is used by the compressed,
thin-provisioned, or the <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> volume. Typically,
the space that is used by the compressed or thin-provisioned volume is less than the capacity of the
volume. For <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> volumes, used capacity is the
capacity that is used by the volume's extents on the Enterprise HDD, Nearline HDD, or SSD drives. </dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="sevs_capacity_gib">
<dt>Capacity (GiB)</dt>
<dd>The capacity of the compressed or the thin-provisioned volume, which comprises the sum of the
used and available capacity. For thin-provisioned volumes in <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/> pools or <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_accelerate"/> pools, capacity is the physical
("hard") capacity of the volume.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="sevs_compression_savings_percent">
<dt>Compression Savings (%)</dt>
<dd>The percentage of space that is being saved by the compressed volume in the pools.</dd>
<dd>
<note othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"
/>, <keyword conref="fqz0_entities.dita#fqz0_entities/kez_storage"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_V9000_short"/>.</note>
</dd>
</dlentry>
<dlentry id="sevs_enterprise_hdd_capacity_gib">
<dt>Enterprise HDD Capacity (GiB)</dt>
<dd>The total amount of storage space on the Enterprise hard disk drive that the <tm
trademark="Easy Tier" tmtype="reg">Easy Tier</tm> volume uses for re-tiering the volume
extents.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="sevs_nearline_hdd_capacity_gib">
<dt>Nearline HDD Capacity (GiB)</dt>
<dd>The total amount of storage space on the Nearline hard disk drive that the <tm
trademark="Easy Tier" tmtype="reg">Easy Tier</tm> volume uses for re-tiering the volume extents. </dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="vNearlineHDDCapacityGiB">
<dt>Nearline HDD Capacity (GiB)</dt>
<dd>The amount of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Nearline hard disk drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vNearlineHDDCapacityPercent">
<dt>Nearline HDD Capacity (%)</dt>
<dd>The percentage of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm>
placed on Nearline hard disk drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vEnterpriseHDDCapacityGiB">
<dt>Enterprise HDD Capacity (GiB)</dt>
<dd>The amount of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Enterprise hard disk drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="vEnterpriseHDDCapacityPercent">
<dt>Enterprise HDD Capacity (%)</dt>
<dd>The percentage of volume capacity that <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> has
placed on Enterprise hard disk drives.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> and storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>.</note>
</dd>
</dlentry>
<dlentry id="sevs_physical_allocation_percent">
<dt>Physical Allocation (%)</dt>
<dd>The percentage of physical capacity in the pool that is used by the compressed, or the
thin-provisioned, or the <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> volume. The
percentage of physical capacity for standard-provisioned volumes that use <tm trademark="Easy Tier"
tmtype="reg">Easy Tier</tm> is always set to 100%. The formula for calculating used capacity is as
follows: <lines>
<codeph>Used Capacity/Capacity*100</codeph>
</lines> For example, if the provisioned capacity of the thin-provisioned volume is 100 GiB and 20
GiB is used by the volume, the used capacity is 20/100 *100, which is 20%.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="sevs_shortfall_percent">
<dt>Shortfall (%)</dt>
<dd>The percentage of space that is over committed to the compressed or the thin-provisioned volume.
For example, you commit 100 GiB of space to a thin-provisioned volume in a pool with a capacity of
50 GiB. As the space is used by the thin-provisioned volume in increments of 10 GiB, the capacity
available for use decreases and the shortfall in capacity becomes more acute.</dd>
<dd>The formula for calculating shortfall is as follows: <lines>
<codeph>Overprovisioned Capacity/Unused Volume Capacity * 100</codeph>
</lines> The shortfall is not calculated if the volume is not compressed, or thin-provisioned, or if
it cannot be determined whether the volume is compressed or thin-provisioned.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="sevs_ssd_capacity_gib">
<dt>SSD Capacity (GiB)</dt>
<dd>The total amount of storage space on the solid-state drive that the <tm trademark="Easy Tier"
tmtype="reg">Easy Tier</tm> volume uses for retiering the volume extents. </dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/>, FlashSystem storage systems, <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="sevs_tier0_available_space_gib">
<dt>Tier 0 Flash Available Capacity (GiB)</dt>
<dd>The amount of storage space that is available on the Tier 0 flash solid-state drives that can be
used by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the volume extents in
the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="sevs_tier1_available_space_gib">
<dt>Tier 1 Flash Available Capacity (GiB)</dt>
<dd>The amount of storage space that is available on the Tier 1 flash, read-intensive solid-state
drives that can be used by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the
volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="sevs_tier0_capacity_gib">
<dt>Tier 0 Flash Capacity (GiB)</dt>
<dd>The total amount of storage space on the Tier 0 flash solid-state drives that can be used by <tm
tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the volume extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="sevs_tier1_capacity_gib">
<dt>Tier 1 Flash Capacity (GiB)</dt>
<dd>The total amount of storage space on the Tier 1 flash, read-intensive solid-state drives that
can be used by <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> for retiering the volume
extents in the pool.</dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</note>
</dd>
</dlentry>
<dlentry id="sevs_unallocated_space_gib">
<dt>Unused Capacity (GiB)</dt>
<dd>(Previously known as Unallocated Space) The total amount of remaining space that can be used by
the compressed, thin-provisioned, or <tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm> volume. </dd>
<dd>The formula for calculating unused capacity is as
follows:<lines><codeph>Capacity - Used Capacity</codeph></lines></dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>RAID ARRAYS</title>
<p>
<dl>
<dlentry id="rAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a RAID array as acknowledged. An acknowledged status
indicates that the status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a RAID array is Error, then the status of the related storage
system is also Error. If the Error status of the RAID array is acknowledged, then its status is not
used to determine the overall status of the storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also Normal.
</dd>
</dlentry>
<dlentry id="rDDMCapacity">
<dt>DDM Capacity (GB), DDM Capacity (GiB)</dt>
<dd>The storage capacity of each disk drive module (DDM) in a RAID array, which is measured in GiB
(gibibyte) and GB (gigabyte). 1 GiB is equal to approximately 1.074 GB. For example, if the capacity
of a DDM is 135 GiBs, its value in GB is 146.</dd>
</dlentry>
<dlentry id="rDDMClass">
<dt>DDM Class</dt>
<dd otherprops="536_196892org">The disk class, which is either enterprise or near-line. Enterprise
is a high-end disk drive and has the highest reliability. Near-line is a low-end disk drive and has
less reliability than enterprise disk drives. For example, FC (Fibre Channel) is shown for some
enterprise disk drives and SATA is shown for some near-line disk drives. Other classes might include
SSD (<tm tmclass="IGNORE" tmtype="reg" trademark="Solid">Solid</tm> State), NVMe SSD, and SAS.</dd>
<dd otherprops="536_196892new">The technology type of the disk drives in the array, such as
Solid-State Drive, NVMe SSD, Storage Class Memory, Flash, Fibre Channel (FC), SATA, and other
types.</dd>
</dlentry>
<dlentry id="rDDMSpeed">
<dt>DDM Speed (RPM)</dt>
<dd>The revolutions per minute (RPM) of the DDMs in a RAID array. Examples: 5600, 7200, 10000, or
15000.</dd>
</dlentry>
<dlentry>
<dt>Device Adapter Pair</dt>
<dd id="rDeviceAdapterPair">The device adapter (DA) pair that is associated with a RAID array.
Available for <keyword conref="fqz0_entities.dita#fqz0_entities/kds6000"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
<dd id="rSaaSDeviceAdapterPair">The device adapter (DA) pair that is associated with a RAID array.
Available for <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
</dlentry>
<dlentry id="rEncryptionGroup">
<dt>Encryption Group</dt>
<dd>The encryption group of a rank.</dd>
</dlentry>
<dlentry>
<dt>Format</dt>
<dd id="rFormat">The format of a RAID array. Examples: FB (fixed block), CKD (count key data).
Available for <keyword conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
<dd id="rSaaSFormat">The format of a RAID array. Examples: FB (fixed block), CKD (count key data).
Available for <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
</dlentry>
<dlentry id="rName">
<dt>Name</dt>
<dd>The name of a RAID array. Examples: A5, A23.</dd>
</dlentry>
<dlentry id="rController">
<dt>Node</dt>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kds4000"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds5000"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/> storage systems, the name of the disk
controller that is associated with the RAID array. For all other storage systems, the node to which
a RAID array is associated.</dd>
</dlentry>
<dlentry id="rPool">
<dt>Pool</dt>
<dd>The name of the pool in which a RAID array is a member. </dd>
</dlentry>
<dlentry id="rRAIDLevel">
<dt>RAID Level</dt>
<dd>The RAID level of an array, such as RAID 5, or RAID 10, or for distributed arrays, the RAID
level is prefixed with “D”, such as DRAID 5.</dd>
</dlentry>
<dlentry id="rRAIDStatus">
<dt>RAID Status</dt>
<dd>The status of a RAID array. Statuses include Online, Offline, Degraded, Synchronizing,
Initializing, No Spare, and Unknown. Use the status to determine the condition of the array, and if
any actions must be taken. For example, a RAID array might have one of the following statuses:<dl>
<dlentry>
<dt>Degraded</dt>
<dd>A drive failed in the array and no spare is available. This status is a critical situation
because if a second drive fails before the first one is replaced and rebuilt, all data on the RAID
array is lost.</dd>
</dlentry>
<dlentry>
<dt>No Spare</dt>
<dd>A drive failed and was replaced automatically by the spare drive. Now, no spares are available
for the array. Replace the failed drive as soon as possible, but the RAID array can sustain another
drive failure without data loss.</dd>
</dlentry>
<dlentry>
<dt>Synchronizing</dt>
<dd>A drive failed in the array and is being replaced by the spare drive. The data that was on the
failed drive is being rebuilt and is being written to the new drive, which used to be the spare
drive. This status is a temporary critical situation because if a second drive fails before the data
is rebuilt, data on the RAID array is lost. </dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="rRAIDState">
<dt>RAID State</dt>
<dd>The state of a RAID array, based on the CLI value for the array. States include Online, Offline,
Degraded, Expanding, Synchronizing, Initializing, No Spare, and Unknown. Use the state to determine
the condition of the array, and if any actions must be taken. For example, a RAID array might have
one of the following states:<dl>
<dlentry>
<dt>Degraded</dt>
<dd>A drive failed in the array and no spare is available. This state is a critical situation
because if a second drive fails before the first one is replaced and rebuilt, all data on the RAID
array is lost.</dd>
</dlentry>
<dlentry>
<dt>Expanding</dt>
<dd>A new drive is being added to the RAID array. When the expansion is complete, the capacity
columns show the updated capacity of the array.</dd>
</dlentry>
<dlentry>
<dt>No Spare</dt>
<dd>A drive failed and was replaced automatically by the spare drive. Now, no spares are available
for the array. Replace the failed drive as soon as possible, but the RAID array can sustain another
drive failure without data loss.</dd>
</dlentry>
<dlentry>
<dt>Synchronizing</dt>
<dd>A drive failed in the array and is being replaced by the spare drive. The data that was on the
failed drive is being rebuilt and is being written to the new drive, which used to be the spare
drive. This state is a temporary critical situation because if a second drive fails before the data
is rebuilt, data on the RAID array is lost. </dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry>
<dt>Rank</dt>
<dd id="rRank">The rank of which a RAID array is a member. A rank is a logically contiguous storage
space. Typically, the relationship between arrays and ranks is a one-to-one relationship , except
for <keyword conref="fqz0_entities.dita#fqz0_entities/kds6000"/> where it is possible to define a
rank that uses two arrays. Available for <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
<dd id="rSaaSRank">The rank of which a RAID array is a member. A rank is a logically contiguous
storage space. Typically, the relationship between arrays and ranks is a one-to-one relationship.
Available for <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
</dlentry>
<dlentry id="rRankGroup">
<dt>Rank Group</dt>
<dd>The rank group that is associated with a RAID array.</dd>
</dlentry>
<dlentry>
<dt>Site</dt>
<dd id="rSite">The identifier of the array site for a RAID array. Available for <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
<dd id="rSaaSSite">The identifier of the array site for a RAID array. Available for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
</dlentry>
<dlentry id="rStatus">
<dt>Status</dt>
<dd>The status of a RAID array. Statuses include Normal, Warning, Error, and Unknown. Use the status
to determine the condition of an array, and if any actions must be taken. For example, if an array
has an Error status, take immediate action to correct the problem.</dd>
</dlentry>
<dlentry id="rTier">
<dt>Tier</dt>
<dd otherprops="537_198728org">In <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm>, the
performance capability of a tier is determined by the type of disks that the pool uses. Tier values
might include Enterprise Tier, Nearline Tier, Tier 1 Flash, or Tier 0 Flash.</dd>
<dd otherprops="537_198728new">In <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm>, the
performance capability of a tier is determined by the type of disks that the pool uses. Tier values
might include Enterprise Tier, Nearline Tier, Tier 0 Flash, Tier 1 Flash, or Tier 2 Flash.</dd>
</dlentry>
<dlentry id="rTotalCapacity">
<dt>Capacity (GiB)</dt>
<dd>(Previously known as Total Capacity) The total size of the storage capacity in the RAID
array.</dd>
</dlentry>
<dlentry>
<dt>Width</dt>
<dd id="rWidth">The rank width of a RAID array. Available for <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
<dd id="rSaaSWidth">The rank width of a RAID array. Available for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> arrays only.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>PORTS</title>
<p>
<dl>
<dlentry id="prtAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a port as acknowledged. An acknowledged status
indicates that the status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a port is Error, then the status of the related storage system is
also Error. If the Error status of the port is acknowledged, then its status is not used to
determine the overall status of the storage system. In this case, if the other internal resources of
the storage system are Normal, then the status of the storage system is also Normal. </dd>
</dlentry>
<dlentry id="prtConnectedDevice">
<dt>Connected Device</dt>
<dd>The name of a storage resource that is connected to a port.</dd>
</dlentry>
<dlentry id="prtConnectedNPIVPorts">
<dt>Connected NPIV Ports</dt>
<dd>The number of ports that use NPIV and that are logically connected to the port. Ports must be
online to be connected. <note type="tip">Click the number of ports to view the <uicontrol>Connected
NPIV Ports</uicontrol> tab in the switch port properties.</note><note type="restriction">The
<uicontrol>Connected NPIV Ports</uicontrol> tab is not shown in the following circumstances:<ul>
<li>When the logically connected ports cannot be identified</li>
<li>When there are no logical connections to the port</li>
</ul><p>If the physical and logical connections cannot be differentiated, the physically connected
port might be displayed on the <uicontrol>Connected NPIV Ports</uicontrol> tab. In other words, the
physically connected port is displayed with the logically connected ports, instead of being
displayed separately on the <uicontrol>Connectivity</uicontrol> tab.</p></note></dd>
</dlentry>
<dlentry id="prtConnectedPort">
<dt>Connected Port</dt>
<dd>The display name or WWPN of the remote port that is connected to a port on a storage system.
Both ports must be online to be connected.</dd>
</dlentry>
<dlentry id="prtConnectedWWPN">
<dt>Connected WWPN</dt>
<dd>The WWPN of the remote port that is connected to a port on a storage system.</dd>
</dlentry>
<dlentry id="prtFabric">
<dt>Fabric</dt>
<dd>The name of the fabric where the port is located.</dd>
</dlentry>
<dlentry id="prtFCPortID">
<dt>FC Port ID</dt>
<dd>A 24-bit ID that uniquely identifies a fabric endpoint, such as an F port for a storage system
or an N port for a server.</dd>
</dlentry>
<dlentry id="prtIQN">
<dt>IQN</dt>
<dd>The iSCSI qualified name of the port.</dd>
</dlentry>
<dlentry id="prtLocation">
<dt>Location</dt>
<dd>A code that represents the physical location of a port within the hardware of a storage
system.</dd>
</dlentry>
<dlentry id="prt_mac_address" otherprops="5215_161230new">
<dt>MAC Address</dt>
<dd>The unique hardware address of the port on a storage system. <note othertype="Availability"
type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage
systems.</note></dd>
</dlentry>
<dlentry id="prtName">
<dt>Name</dt>
<dd>The name of a Fibre Channel port on a storage system.</dd>
</dlentry>
<dlentry id="prt_node" otherprops="5215_161230new">
<dt>Node</dt>
<dd>The name of the node that contains a port. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="prtSpeedGbps">
<dt>Speed (Gbps)</dt>
<dd>The negotiated speed of a port.</dd>
</dlentry>
<dlentry id="prtStatus">
<dt>Status</dt>
<dd>The status of a Fibre Channel port. Statuses include Normal, Warning, Error, and Unknown. Use
the status to determine the condition of a port, and if any actions must be taken. For example, if a
port has an Error status, take immediate action to correct the problem. </dd>
</dlentry>
<dlentry id="prtWWPN">
<dt>WWPN</dt>
<dd>The World Wide Port Name (WWPN) of a port. A WWPN is the unique 64-bit identifier for a port in
a Fibre Channel fabric.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>DISKS</title>
<dl>
<dlentry id="dskAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a disk or drive as acknowledged. An acknowledged
status indicates that the status was reviewed and is either resolved or can be ignored. An
acknowledged status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a disk or drive is Error, then the status of the related storage
system is also Error. If the Error status of the disk or drive is acknowledged, then its status is
not used to determine the overall status of the storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also Normal.
</dd>
</dlentry>
<dlentry id="dskAggregate" otherprops="5215_161230new">
<dt>Aggregate</dt>
<dd>The aggregate that contains the disk. An aggregate is a collection of RAID groups that consist
of physical disks. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry>
<dt>Array Site</dt>
<dd id="dskArraySite">The array site to which this disk belongs. Usually, each array site has four
or eight disks. Applies to <keyword conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> disks only.</dd>
<dd id="dskSaaSArraySite">The array site to which this disk belongs. Usually, each array site has
four or eight disks. Applies to <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> disks
only.</dd>
</dlentry>
<dlentry id="dskCapacityGiB">
<dt>Capacity (GB)</dt>
<dd>The total amount of storage space that is available (unformatted) on a disk or drive.</dd>
</dlentry>
<dlentry id="dskClass">
<dt>Class</dt>
<dd>The technology type of the disk or drive, such as Solid-State Drive, NVMe SSD,<ph
otherprops="536_196892new"> Storage Class Memory,</ph> Flash, Fibre Channel (FC), SATA, and other
types.</dd>
</dlentry>
<dlentry id="dskEnclosure">
<dt>Enclosure</dt>
<dd>
<dl>
<dlentry>
<dt>For <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_a9000r_short"/></dt>
<dd>The number of the flash enclosure in which a drive is installed. </dd>
</dlentry>
<dlentry otherprops="5217_170692org">
<dt>For <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_900_short"/></dt>
<dd>The enclosure where the drive resides. A drive can be associated with a single enclosure
only.</dd>
</dlentry>
<dlentry otherprops="5217_170692new">
<dt>For <keyword conref="fqz0_entities.dita#fqz0_entities/kflashsystem_9100_short"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_900_short"/></dt>
<dd>The enclosure where the drive resides. A drive can be associated with a single enclosure
only.</dd>
</dlentry>
<dlentry>
<dt>For <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, the <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_family"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_V9000_short"/></dt>
<dd>The enclosure or node where the disk resides. A disk can be associated with a single enclosure
or node only.</dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry id="dskEncryption">
<dt>Encryption</dt>
<dd>Shows whether a disk is encrypted. This column is blank for local disks and for storage system
disks that do not support encryption. Available for <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> only.</dd>
</dlentry>
<dlentry id="dskFirmware">
<dt>Firmware</dt>
<dd>The firmware version of the microcode on a disk.</dd>
</dlentry>
<dlentry>
<dt>Location</dt>
<dd id="dskLocation">A code that represents the physical location of a disk within the hardware of a
storage system. Applies to <keyword conref="fqz0_entities.dita#fqz0_entities/kds6000"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> disks only.</dd>
<dd id="dskSaaSLocation">A code that represents the physical location of a disk within the hardware
of a storage system. Applies to <keyword conref="fqz0_entities.dita#fqz0_entities/kds6000"/> and
<keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> disks only.</dd>
</dlentry>
<dlentry id="dskManagedDisk">
<dt>Managed Disk</dt>
<dd>The managed disk to which a disk belongs. A managed disk can include storage from one or more
local disks. Applies to disks in storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> only.</dd>
</dlentry>
<dlentry>
<dt>Name</dt>
<dd id="dskName">The internal identifier of a disk or drive. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems, this name uniquely identifies a
disk within a single array site.</dd>
<dd id="dskSaaSName">The internal identifier of a disk or drive. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems, this name uniquely identifies a
disk within a single array site.</dd>
</dlentry>
<dlentry>
<dt>Rank</dt>
<dd id="dskRank">The rank to which a disk belongs. This rank is associated with the array site of
the disk. If no rank is defined, the value None is shown. Applies to <keyword
conref="fqz0_entities.dita#fqz0_entities/kess_short"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kds6000"/>, and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> disks only.</dd>
<dd id="dskSaaSRank">The rank to which a disk belongs. This rank is associated with the array site
of the disk. If no rank is defined, the value None is shown. Applies to <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> disks only.</dd>
</dlentry>
<dlentry id="dskSlot">
<dt>Slot</dt>
<dd>The slot number of the disk or drive within the associated enclosure or node.</dd>
</dlentry>
<dlentry id="dskSpare">
<dt>Spare</dt>
<dd>Shows whether a disk or drive is a spare disk or drive. A spare disk or drive is not allocated
for storage and is predesignated for use as a replacement for a failed disk or drive.</dd>
</dlentry>
<dlentry id="dskSpeedRRM">
<dt>Speed (RPM)</dt>
<dd>The number of revolutions per minute of a disk.</dd>
</dlentry>
<dlentry id="dskStatus">
<dt>Status</dt>
<dd>The status of a disk or drive. Use the status to determine the condition of the disk or drive,
and if any actions must be taken. For example, if a disk has an Error status, take immediate action
to correct the problem. If the disk has an Operational status, then it is operating normally and no
further action is required.</dd>
</dlentry>
</dl>
</section>
<section>
<title>ENCLOSURES</title>
<dl>
<dlentry audience="onpremonly" id="encAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of an enclosure as acknowledged. An acknowledged status
indicates that its status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used when the status of the associated storage system is determined. </dd>
<dd>For example, if the status of an enclosure is Error, then the status of the related storage
system is also Error. If the Error status of the enclosure is acknowledged, then its status is not
used to determine the overall status of the storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also Normal.
</dd>
</dlentry>
<dlentry id="encCanisterStatus">
<dt>Canister Status</dt>
<dd>The status of the canisters in an enclosure. <ph audience="onpremonly">The status of the
canisters is included as part of the overall status of the enclosure.</ph> Possible values are listed:<dl>
<dlentry>
<dt>Normal (Online)</dt>
<dd>The canisters in the enclosure are online and working normally.</dd>
</dlentry>
<dlentry>
<dt>Warning (Running with problems)</dt>
<dd>A canister in the enclosure is present but not working normally.</dd>
</dlentry>
<dlentry>
<dt>Error (Offline)</dt>
<dd>The canisters in the enclosure are offline and cannot be detected. For troubleshooting
information, see the <tm trademark="IBM" tmtype="reg">IBM</tm> Docs for the related storage
system.</dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="encPowerSupplies">
<dt><tm trademark="Power" tmtype="reg">Power</tm> Supplies</dt>
<dd>The number of power supply units (PSUs) in an enclosure. Each enclosure has at least 2 PSUs for
redundancy.</dd>
</dlentry>
<dlentry id="encPsuStatus">
<dt><tm trademark="Power" tmtype="reg">Power</tm> Supply Status</dt>
<dd>The status of the power supply units (PSUs) in an enclosure. <ph audience="onpremonly">The
status of the PSUs is included as part of the overall status of the enclosure.</ph> Possible values
are listed:<dl>
<dlentry>
<dt>Normal (Online)</dt>
<dd>The PSUs in the enclosure are online and working normally.</dd>
</dlentry>
<dlentry>
<dt>Warning (Running with problems)</dt>
<dd>A PSU in the enclosure is present but not working normally.</dd>
</dlentry>
<dlentry>
<dt>Error (Offline)</dt>
<dd>The PSUs in the enclosure are offline and cannot be detected. For troubleshooting information,
see the <tm trademark="IBM" tmtype="reg">IBM</tm> Docs for the related storage system.</dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry audience="onpremonly" id="encStatus">
<dt>Status</dt>
<dd>The status of an enclosure is determined by the most critical status of its native operation,
power supplies, and canisters. For example, if an enclosure is operating normally, but one of its
PSUs is offline, the overall status of the enclosure is Warning. </dd>
<dd>Use the status to determine the condition of the enclosure, and if any actions must be taken.
Possible values are listed:<dl>
<dlentry>
<dt>Normal (Online)</dt>
<dd>An enclosure, its PSUs, and its canisters are operating normally.</dd>
</dlentry>
<dlentry>
<dt>Warning</dt>
<dd>The enclosure is visible to the SAS network but not down both strands, or one of its canisters
or PSUs is offline.</dd>
</dlentry>
<dlentry>
<dt>Error (Offline)</dt>
<dd>A managed enclosure is not visible to the SAS network, or its PSUs or canisters are offline. For
troubleshooting information, see the <tm trademark="IBM" tmtype="reg">IBM</tm> Docs for the related
storage system.</dd>
</dlentry>
</dl></dd>
</dlentry>
</dl>
</section>
<section>
<title>EXTERNAL DISKS</title>
<dl>
<dlentry id="exdskAvailableSpaceGiB">
<dt>Available Capacity (GiB)</dt>
<dd>The storage capacity that is available (not allocated) on an external disk.</dd>
</dlentry>
<dlentry id="exdskAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of an external disk as acknowledged. An acknowledged
status indicates that the status was reviewed and is either resolved or can be ignored. An
acknowledged status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of an external disk is Error, then the status of the related storage
system is also Error. If the Error status of the external disk is acknowledged, then its status is
not used to determine the overall status of the storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also Normal.
</dd>
</dlentry>
<dlentry id="exdskName">
<dt>Name</dt>
<dd>The internal identifier of an external disk. </dd>
</dlentry>
<dlentry id="exdskPool">
<dt>Pool</dt>
<dd>The name of the storage pool to which an external disk belongs. If an external disk is
associated with a primordial pool, or is not yet assigned to a pool, then it is unmanaged. The value
None is displayed. </dd>
</dlentry>
<dlentry id="exdskStatus">
<dt>Status</dt>
<dd>The status of an external disk. Use the status to determine the condition of the external disk,
and if any actions must be taken. For example, if an external disk has an Error status, take
immediate action to correct the problem. If the external disk has an Operational status, then it is
operating normally and no further action is required.</dd>
</dlentry>
<dlentry id="exdskStorageSystem">
<dt>Storage System</dt>
<dd>The name of the storage system that uses an external disk. </dd>
</dlentry>
<dlentry id="exdskTotalCapacityGiB">
<dt>Capacity (GiB)</dt>
<dd>(Previously known as Total Capacity) The capacity on an external disk.</dd>
</dlentry>
<dlentry id="exdskVolumes">
<dt>Volumes</dt>
<dd>The number of volumes that are partially or completely on an external disk.</dd>
</dlentry>
</dl>
</section>
<section>
<title>MANAGED DISKS</title>
<dl>
<dlentry id="mandskAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a managed disk as acknowledged. An acknowledged status
indicates that the status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a managed disk is Error, then the status of the related storage
system is also Error. If the Error status of the managed disk is acknowledged, then its status is
not used to determine the overall status of the storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also Normal.
</dd>
</dlentry>
<dlentry id="mandskAvailableSpaceGiB">
<dt>Available Capacity (GiB)</dt>
<dd>The storage capacity that is available (not allocated) on a managed disk.</dd>
</dlentry>
<dlentry id="mandskBackEndStorageSystem">
<dt>Back-end Storage System</dt>
<dd>The name of the storage system that is providing storage to a managed disk.</dd>
</dlentry>
<dlentry id="mandskClass">
<dt>Class</dt>
<dd otherprops="536_196892new">The technology type of the managed disk, such as Tier 0 and Tier 1
flash solid-state drives (SSDs), NVMe SSDs, Storage Class Memory, Enterprise and Nearline hard disk
drives (HDDs), and other types. Managed disks that consist of internal, local disks are classified
automatically by the cluster, but you must manually classify managed disks that consist of external
disks. </dd>
<dd otherprops="536_196892org">The technology type of the managed disk, such as Tier 0 and Tier 1
flash solid-state drives (SSDs), NVMe SSDs, Enterprise and Nearline hard disk drives (HDDs), and
other types. Managed disks that consist of internal, local disks are classified automatically by the
cluster, but you must manually classify managed disks that consist of external disks. </dd>
</dlentry>
<dlentry id="mandskTier">
<dt><tm trademark="Easy Tier" tmtype="reg">Easy Tier</tm></dt>
<dd>The tier that a managed disk is assigned to by the internal cluster auto-detection for internal
disks or by the user for external disks. This classification is used by the storage virtualizer to
determine when to activate the <tm tmtype="reg" trademark="Easy Tier">Easy Tier</tm> function for
certain pools.</dd>
</dlentry>
<dlentry id="mandskMode">
<dt>Mode</dt>
<dd>
<p>The allocation mode of the managed disk. The following allocation modes might be shown: <dl>
<dlentry>
<dt>Unmanaged</dt>
<dd>The external volume is virtualized as a managed disk, but is not being used.</dd>
</dlentry>
<dlentry>
<dt>Managed </dt>
<dd>The external volume is virtualized as a managed disk and is assigned to a pool or a managed disk
group.</dd>
</dlentry>
<dlentry>
<dt>Image </dt>
<dd>The external volume is not virtualized, which means that the associated volume provides a direct
block-for-block translation of the external volume.</dd>
</dlentry>
<dlentry>
<dt>Array </dt>
<dd>One or more internal or local disks are used to form the managed disk.</dd>
</dlentry>
</dl></p>
</dd>
</dlentry>
<dlentry id="mandskName">
<dt>Name</dt>
<dd>The name of a managed disk.</dd>
</dlentry>
<dlentry id="mandskPool">
<dt>Pool</dt>
<dd>The name of the storage pool or managed disk group to which a managed disk belongs. If a managed
disk is associated with a primordial pool, or is not yet assigned to a pool, then it is unmanaged.
The value None is displayed. </dd>
</dlentry>
<dlentry id="mandskRAIDLevel">
<dt>RAID Level</dt>
<dd>The RAID level of a managed disk, such as RAID 5 and RAID 10, or if the managed disk is a
distributed RAID array, the RAID level is prefixed with “D”, such as DRAID 5. The RAID level affects
the performance and fault tolerance of the volumes that are used by a managed disk. This value is
shown only when the managed disk is a RAID array that was built from internal, local disks in the
storage virtualizer. The value None indicates that a managed disk is a single, local disk and
performance or fault tolerance is not improved. </dd>
</dlentry>
<dlentry id="mandskStatus">
<dt>Status</dt>
<dd>The status of a managed disk. Use the status to determine the condition of a managed disk, and
if any actions must be taken. For example, if a managed disk has an Error status, take immediate
action to correct the problem.</dd>
</dlentry>
<dlentry id="mandskStorageSystem">
<dt>Storage System</dt>
<dd>The name of the storage virtualizer that contains a managed disk. A storage virtualizer is a
storage system that virtualizes storage space from internal storage or from another storage system,
such as a <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/> or <keyword
conref="fqz0_entities.dita#fqz0_entities/kez_storage"/>. </dd>
</dlentry>
<dlentry id="mandskTotalCapacityGiB">
<dt>Capacity (GiB)</dt>
<dd>(Previously known as Total Capacity) Total amount of storage space on a managed disk.</dd>
</dlentry>
<dlentry id="mandskVolumes">
<dt>Volumes</dt>
<dd>The number of volumes that are partially or completely on a managed disk.</dd>
</dlentry>
<dlentry id="mandskQuorum">
<dt>Quorum disk</dt>
<dd>A quorum disk is an MDisk that is used exclusively for system management. A storage system might
have only one active quorum disk.</dd>
</dlentry>
</dl>
</section>
<section>
<title>IO GROUPS</title>
<dl>
<dlentry id="iogrpCompression">
<dt>Compression</dt>
<dd>The compression level of the I/O group. Possible values are listed. </dd>
<dd>
<dl>
<dlentry>
<dt>Unsupported</dt>
<dd>Indicates that the I/O group does not support compressed volumes or its nodes are down-level for
supporting compressed volumes. </dd>
</dlentry>
<dlentry>
<dt>Supported</dt>
<dd>Indicates that the I/O group supports compressed volumes.</dd>
</dlentry>
<dlentry>
<dt>Active</dt>
<dd>Indicates that the I/O group contains at least one compressed volume. </dd>
</dlentry>
</dl>
<note type="other" othertype="Availability">Nodes on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>. These nodes support Random Access
Compression Engine (RACE) compression for volumes in standard pools.</note>
</dd>
</dlentry>
<dlentry id="iogrpEnclosure">
<dt>Enclosure</dt>
<dd>The enclosure that is associated exclusively with the I/O Group. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kflashsystem_V9000_short"/>, the storage enclosure is
shared by all I/O groups.</dd>
</dlentry>
<dlentry id="iogrpNodes">
<dt>Nodes</dt>
<dd>The nodes that form the I/O group. Each node is separated by a comma. A single node in this list
might indicate a node failure.</dd>
</dlentry>
<dlentry id="iogrpTotalFlashMemoryMiB">
<dt>Total <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> Memory (MiB)</dt>
<dd>The total amount of I/O group memory that is configured for standard <tm tmtype="reg"
trademark="FlashCopy">FlashCopy</tm> and incremental <tm tmtype="reg" trademark="FlashCopy"
>FlashCopy</tm>. The amount of memory that is configured determines the amount of volume space that
can be associated with <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> targets. The column is
blank if no memory is available.</dd>
</dlentry>
<dlentry id="iogrpTotalMirroringMemoryMiB">
<dt>Total Mirroring Memory (MiB)</dt>
<dd>The total amount of I/O Group memory that is configured for volume mirroring. This value
determines the amount of volume space that can be mirrored. The column is blank if no memory is
configured for volume mirroring.</dd>
</dlentry>
<dlentry id="iogrpTotalRemoteCopyMemoryMiB">
<dt>Total Remote Copy Memory (MiB)</dt>
<dd>The total amount of I/O group memory that is configured for Metro Mirror and Global Mirror. The
amount of memory that is configured determines the amount of volume space that can be associated
with Metro Mirror and Global Mirror targets. The column is blank if no memory is available.</dd>
</dlentry>
<dlentry id="iogrpUsedFlashCopyMemoryMiB">
<dt>Used <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> Memory (MiB)</dt>
<dd>The amount of I/O group memory that is used for <tm tmtype="reg" trademark="FlashCopy"
>FlashCopy</tm>. This column is blank if no memory is available.</dd>
</dlentry>
<dlentry id="iogrpUsedMirroringMemoryMiB">
<dt>Used Mirroring Memory (MiB)</dt>
<dd>The amount of I/O Group memory that is used for volume mirroring. This column is blank if no
memory is available.</dd>
</dlentry>
<dlentry id="iogrpUsedRemoteCopyMemoryMiB">
<dt>Used Remote Copy Memory (MiB)</dt>
<dd>The amount of I/O group memory that is used for Metro Mirror and Global Mirror. This column is
blank if no memory is available.</dd>
</dlentry>
<dlentry id="iogrpVolumes">
<dt>Volumes</dt>
<dd>The number of volumes (virtual disks) that are assigned to the I/O Group.</dd>
</dlentry>
</dl>
</section>
<section>
<title>NODES and MODULES</title>
<dl>
<dlentry id="cnmAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a node or module as acknowledged. An acknowledged
status indicates that the status was reviewed and is either resolved or can be ignored. An
acknowledged status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a node is Error, then the status of the related storage system is
also Error. If the Error status of the node is acknowledged, then its status is not used to
determine the overall status of the storage system. In this case, if the other internal resources of
the storage system are Normal, then the status of the storage system is also Normal. </dd>
</dlentry>
<dlentry id="cnmCompression">
<dt>Compression</dt>
<dd>The compression level of a node is inherited from the associated I/O group. Possible values are
listed. </dd>
<dd>
<dl>
<dlentry>
<dt>Unsupported</dt>
<dd>Indicates that the associated I/O group does not support compressed volumes or nodes are
down-level for supporting compressed volumes.</dd>
</dlentry>
<dlentry>
<dt>Supported</dt>
<dd>Indicates that the associated I/O group supports compressed volumes.</dd>
</dlentry>
<dlentry>
<dt>Active</dt>
<dd>Indicates that the associated I/O group is compression active, and contains at least one
compressed volume.</dd>
</dlentry>
</dl>
<note type="other" othertype="Availability">Nodes on storage systems that run <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/>. These nodes support Random Access
Compression Engine (RACE) compression for volumes in standard pools.</note>
</dd>
</dlentry>
<dlentry id="cnmConfigurationNode">
<dt>Configuration Node</dt>
<dd>Indicates whether a node is a configuration node. This value is set to
<uicontrol>Yes</uicontrol> if the node is the only node in the cluster.</dd>
</dlentry>
<dlentry id="cnmEnclosure">
<dt>Enclosure</dt>
<dd>The enclosure that is associated with the canister node. A node can be associated with a single
enclosure only.</dd>
</dlentry>
<dlentry id="cnmFCPorts">
<dt>FC Ports</dt>
<dd>The number of Fibre Channel ports that are on the node or module.</dd>
</dlentry>
<dlentry id="cnm_ha_paired_node" otherprops="5215_161230new">
<dt>HA Paired Node</dt>
<dd>The name of the node with which a node is paired. For fault tolerance, each node is partnered
with another node in the cluster. The two nodes form a high-availability (HA) pair.<note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="cnmIOGroup">
<dt>I/O Group</dt>
<dd>The display name of the I/O group in which the node is a member. </dd>
</dlentry>
<dlentry id="cnmIPPorts">
<dt>IP Ports</dt>
<dd>The number of Internet Protocol ports that are on a node. </dd>
</dlentry>
<dlentry id="cnmModel">
<dt>Model</dt>
<dd>The hardware model number of the node. </dd>
</dlentry>
<dlentry id="cnm_network_interfaces" otherprops="5215_161230new">
<dt>Network Interfaces</dt>
<dd>The number of network logical interfaces that are configured on the node. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="cnmReadCacheGiB">
<dt>Read Cache (GiB)</dt>
<dd>The amount of read cache memory that is available on the node.</dd>
</dlentry>
<dlentry id="cnmSpareNodes">
<dt>Spare Nodes</dt>
<dd>The nodes that are available as spares for a node. <note othertype="Availability" type="other"
>Storage systems that are running <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_virtualize"/> 8.1.0 or later.</note></dd>
</dlentry>
<dlentry id="cnmStatus">
<dt>Status</dt>
<dd>The status of the node or module. For nodes, this status includes the spare node status. The
Online Spare status indicates that the node is a spare node and it is being used as a spare for
another node in the I/O group.</dd>
</dlentry>
<dlentry id="cnmStatus_pre5216">
<dt>Status</dt>
<dd>The status of the node or module. Use the status to determine the condition of the storage
resource, and if any actions must be taken. For example, if a node has an Error status, take
immediate action to correct the problem.</dd>
</dlentry>
<dlentry id="cnmWriteCacheGiB">
<dt>Write Cache (GiB)</dt>
<dd>The amount of write cache memory that is available on the node.</dd>
</dlentry>
<dlentry id="cnmWWN">
<dt>WWN</dt>
<dd>The worldwide network name of the node. </dd>
</dlentry>
<dlentry id="cnmSites">
<dt>Site</dt>
<dd>The physical location of the node in a stretched cluster. The site column is blank if no site is
configured.</dd>
</dlentry>
</dl>
</section>
<section>
<title>HOST CONNECTIONS</title>
<!--This information is provided on the host connections tab.-->
<dl>
<dlentry>
<dt>Associated Resource</dt>
<dd id="hcsAssociatedResource">The name of the server, hypervisor, virtualizer, or <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> storage system that is associated with the
host definition. For a resource name to be known, the resource must be discovered and probed. For a
server name to be known, it must have a <keyword conref="fqz0_entities.dita#fqz0_entities/ksra"/>
deployed, and be probed, or it must be added as an agentless server. <p>If a server name is not
known, but the server is a member of an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster, the name of the cluster is shown as
the associated resource.</p></dd>
</dlentry>
<dlentry>
<dt>Connected Resource</dt>
<dd id="hcsConnectedResource">The name of the server, hypervisor, or virtualizer that is associated
with the host definition. For the server name to be known, it must have a <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra"/> deployed, and be probed, or it must have been added
as an agentless server. For a hypervisor or virtualizer name to be known, the hypervisor or
virtualizer must be discovered and probed.</dd>
<dd id="hcsSaaSConnectedResource">The name of the server or virtualizer that is associated with the
host definition. <ph>For the server name to be known, it must be probed or added automatically as an
agentless server. </ph>For a virtualizer name to be known, the virtualizer must be discovered and
probed.</dd>
</dlentry>
<dlentry id="hcsCluster">
<dt>Cluster</dt>
<dd>The name of the cluster, if any, as defined on the storage system. If no cluster is defined for
the particular host connection, this field is blank.</dd>
</dlentry>
<dlentry id="hcsHost">
<dt>Host</dt>
<dd>The name of the host, as defined on the storage system.</dd>
</dlentry>
<dlentry id="hcsHostType">
<dt>Host Type</dt>
<dd>The type of host connection as defined on the storage system. The type of host connection
determines the low-level SCSI parameters that are used for communication between the storage system
and the host.<note type="tip">The host connection type can be different from the operating system of
the host.</note></dd>
</dlentry>
<dlentry id="hcsPorts">
<dt>Ports</dt>
<dd>The number of host ports that are associated with the host, as defined on the storage system.
</dd>
</dlentry>
<dlentry id="hcsVolumes">
<dt>Volumes</dt>
<dd>The number of volumes that are assigned to a specific cluster, host, or port.</dd>
</dlentry>
<dlentry id="hcsVolumeGroup">
<dt>Volume Group</dt>
<dd>The name of the volume group that is associated with the host connection. <ph
audience="onpremonly">Applies only to <keyword conref="fqz0_entities.dita#fqz0_entities/kds6000"/>
and <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems.</ph><ph
audience="offpremonly">Applies only to <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/>
storage systems.</ph></dd>
</dlentry>
<dlentry id="hcsWWPNs">
<dt>WWPNs</dt>
<dd>The World Wide Port Names (WWPN) that are associated with a host, as defined on the storage
system. A WWPN is the unique 64-bit identifier for a port in a Fibre Channel fabric.</dd>
</dlentry>
<!--This information is provided on the volume mappings tab.-->
</dl>
<dl>
<dlentry id="hcvmapCluster">
<dt>Cluster</dt>
<dd>The name of the cluster, if any, as defined on the storage system. If no cluster is defined for
this assignment, this column is blank.</dd>
</dlentry>
<dlentry id="hcvmapHost">
<dt>Host</dt>
<dd>The name of the host, as defined on the storage system.</dd>
</dlentry>
<dlentry id="hcvmapHostType">
<dt>Host Type</dt>
<dd>The type of host connection, as defined on the storage system. This type determines the
low-level SCSI parameters that are used for communication between the storage system and the
host.<note type="tip">The host connection type can be different from the operating system of the
host.</note></dd>
</dlentry>
<dlentry id="hcvmapOriginalCapacityPool">
<dt>Original Capacity Pool</dt>
<dd>The name of the capacity pool from which the volume was provisioned. The storage resource on
which the volume was created might no longer be a member of the capacity pool.</dd>
</dlentry>
<dlentry id="hcvmapPool">
<dt>Pool</dt>
<dd>The name of the pool in which this volume is a member.</dd>
</dlentry>
<dlentry id="hcvmapPorts">
<dt>Ports</dt>
<dd>The number of host-ports that are associated with the host, as defined on the storage
system.</dd>
</dlentry>
<dlentry id="hcvmapServiceClass">
<dt>Service Class</dt>
<dd>The name of the block-storage service class that is associated with the volume.<note
othertype="Learn more" type="other">Go to <xref href="tpch_r_l2_serviceclass.dita"/>.</note></dd>
</dlentry>
<dlentry id="hcvmapThinProvisioned">
<dt>Thin Provisioned</dt>
<dd>Indicates whether this volume is thin-provisioned, and if applicable, the type of
thin-provisioning that is used. Thin-provisioned volumes on a <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage system can be defined as extent
space-efficient (ESE) or as track space-efficient (TSE).</dd>
</dlentry>
<dlentry id="vSaaSEncrypted">
<dt>Encrypted</dt>
<dd>Shows whether a volume is encrypted. The value <systemoutput>N/A</systemoutput> indicates that
encryption is not supported for this volume.</dd>
</dlentry>
<dlentry id="hcvmapUsedSpaceGiB">
<dt>Used Capacity (GiB)</dt>
<dd>The storage space of a thin-provisioned volume that is used, measured in GiB, as of the time
when asset and health data was last collected for the storage system. The capacity that is used by a
thin-provisioned volume is typically less than its provisioned capacity, which is shown in the
column <uicontrol>Volume Space</uicontrol>. </dd>
</dlentry>
<dlentry id="hcvmapVisible">
<dt>Visible</dt>
<dd>When a volume is created on a storage system, that volume is visible to other resources in the
SAN. If the volume is mapped to monitored resources, such as servers or hypervisors, it is shown on
the <wintitle>Volume Mappings</wintitle> page.</dd>
<dd>However, depending on changes to fabric and zoning configurations, or if a host connection is
not defined on the storage system, the mapped volume might no longer be visible to the host servers
or hypervisors. This situation can result in wasted storage space. Use the following values in this
column to determine the visibility of a mapped volume to a monitored host.<ul>
<li>Yes: The volume is visible to the server or hypervisor.</li>
<li>No: The volume is not visible the server or hypervisor. </li>
<li>Blank: This value is not available for agentless servers and storage virtualizers. </li>
</ul></dd>
<dd>To make a mapped volume visible to the server or hypervisor and ensure that assigned storage
space can be used, try the following actions:<ul>
<li>Configure your fabric to address any zoning or masking issues that might prevent the volume from
being visible to the host.</li>
<li>On the host, scan the host bus adapter. This action finds any new SCSI targets.</li>
<li>Run <keyword conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/> probes to collect
information about the storage system that contains the volume and the host to which it is
mapped.</li>
</ul></dd>
</dlentry>
<dlentry id="hcvmapSaaSVisible">
<dt>Visible</dt>
<dd>
<p>When a volume is created on a storage system, that volume is visible to other resources in the
SAN. If the volume is mapped to monitored resources, such as servers, it is shown on the
<wintitle>Volume Mappings</wintitle> page. If the host connection is not defined on the storage
system, the mapped volume might no longer be visible to the host servers which can result in wasted
storage space. Check the following values to determine whether the volume is visible: <dl>
<dlentry>
<dt>Yes</dt>
<dd>The volume is visible to the server.</dd>
</dlentry>
<dlentry>
<dt>No</dt>
<dd>The volume is not visible the server.</dd>
</dlentry>
<dlentry>
<dt>Blank</dt>
<dd>This value is not available.</dd>
</dlentry>
</dl></p>
<p>To make the mapped volume visible to the server and to ensure that the assigned storage space can
be used, try the following actions:<ul>
<li>Scan the host bus adapter on the host to find new SCSI targets.</li>
<li>Run probes to collect information about the storage system that contains the volume and the host
to which it is mapped.</li>
</ul></p>
</dd>
</dlentry>
<dlentry id="hcvmapVolume">
<dt>Volume</dt>
<dd>The name or label of the volume, if available. This name uniquely identifies the volume within
the storage system.</dd>
</dlentry>
<dlentry>
<dt>Volume Group</dt>
<dd id="hcvmapVolumeGroup">The name of the volume group that is associated with the host connection.
This field applies only to <keyword conref="fqz0_entities.dita#fqz0_entities/kds6000"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems.</dd>
<dd id="hcvmapSaaSVolumeGroup">The name of the volume group on the <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage system that is associated with the host
connection.</dd>
</dlentry>
<dlentry id="hcvmapVolumeID">
<dt>Volume ID</dt>
<dd>The identifier of the volume, such as a serial number or internal ID.</dd>
</dlentry>
<dlentry id="hcvmapVolumewSpaceGiB">
<dt>Volume Space (GiB)</dt>
<dd>The storage capacity of the volume, which is measured in GiB. For thin-provisioned volumes, this
value is the provisioned capacity of the volume, called the soft size on an <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/>, and not the real space. The real space is called
hard size on an <keyword conref="fqz0_entities.dita#fqz0_entities/kxss"/> and it is allocated.</dd>
</dlentry>
<dlentry id="hcvmapSaaSVolumewSpaceGiB">
<!--mmm task 119115 This is Volume Capacity on SaaS-->
<dt>Volume Capacity (GiB)</dt>
<dd>The storage capacity of the volume, which is measured in GiB. For thin-provisioned volumes, this
value is the provisioned capacity of the volume, called the soft size on an <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss"/>, and not the real space. The real space is called
hard size on an <keyword conref="fqz0_entities.dita#fqz0_entities/kxss"/> and it is allocated.</dd>
</dlentry>
</dl>
<p>
<dl>
<dlentry audience="onpremonly">
<dt>Connected Device</dt>
<dd id="hpConnectedDevice">The name of the server, hypervisor, or virtualizer that is associated
with the host definition. <ph>For the server name to be known, a Storage Resource agent for the
server must be deployed and the server must be probed. Alternatively, the server must be added
automatically as an agentless server. </ph>For a hypervisor or virtualizer name to be known, the
hypervisor or virtualizer must be discovered and probed.</dd>
</dlentry>
<dlentry id="hpSaaSAssociatedResource" audience="offpremonly">
<dt>Associated Resource</dt>
<!--mmm 12 Mar 15 119115 On SaaS, this is called 'Associated Resource'-->
<dd id="hpSaaSConnectedDevice"><ph audience="offpremonly">The name of the server or virtualizer that
is associated with the host definition. For a virtualizer name to be known, the virtualizer must be
discovered and probed.</ph><ph audience="onpremonly">The name of the server or virtualizer that is
associated with the host definition. For the server name to be known, it must be probed. For a
virtualizer name to be known, the virtualizer must be discovered and probed.</ph></dd>
</dlentry>
<dlentry audience="onpremonly">
<dt>Connected Port</dt>
<dd id="hpConnectedPort">The name of the server, hypervisor, or storage virtualizer port as
displayed on the server, hypervisor, or storage system page. If no <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra"/> is deployed or the connected resource is not
discovered and probed, the value Unknown is shown.</dd>
</dlentry>
<dlentry audience="offpremonly">
<dt>Associated Port</dt>
<dd id="hpSaaSConnectedPort">The name of the server or virtualizer port as displayed on the server
or storage system page. If the connected resource is not discovered and probed, the value
Unavailable is shown.</dd>
</dlentry>
<dlentry id="hpWWPN">
<dt>WWPN</dt>
<dd>The World Wide Port Names (WWPN) that are associated with a host, as defined on the storage
system. A WWPN is the unique 64-bit identifier for a port in a Fibre Channel fabric.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>FILE SYSTEMS</title>
<p>
<dl>
<dlentry id="ffs_AvailableInodes">
<dt>Available Inodes</dt>
<dd>The number of inodes that are available in a file system.</dd>
</dlentry>
<dlentry id="ffsAavailableSpaceGiB">
<dt>Available Capacity (GiB)</dt>
<dd>The amount of storage capacity that is available (not allocated) on a file system.</dd>
</dlentry>
<dlentry id="ffsCapacityPool">
<dt>Capacity Pool</dt>
<dd>The name of the capacity pool that the file system is assigned to. Applies to the <keyword
conref="fqz0_entities.dita#fqz0_entities/kstor_unified_short"/> storage system.</dd>
</dlentry>
<dlentry id="ffsCreationName">
<dt>Creation Name</dt>
<dd>The file system name on the <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>
cluster that owns the file system. If the file system is mounted from the current cluster, no value
is shown. This information is available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> storage systems. <p>To view information
about the file system, such as NSDs, quotas, filesets, and pools, click the name of the file
system.</p></dd>
</dlentry>
<dlentry id="ffsCluster">
<dt>Cluster</dt>
<dd>The unique name of the parent cluster that a file system is on.</dd>
</dlentry>
<dlentry>
<dt>Custom Tag 1, 2, and 3</dt>
<dd id="ffsCustomTags">Any user-defined text that is associated with the file system. This text can
be included as a report column when you generate reports for the file system. <p>You can tag file
systems to satisfy custom requirements for a service class. A service class can specify up to three
required tags. To provide a service class, storage resources must have all the same tags that are
specified by the service class. If a file system is not tagged, any tags on the containing storage
system also apply to the file system for determining whether it satisfies the custom requirements of
a service class.</p>To edit the custom tags for a file system, right-click the file system in the
list and select <uicontrol>View Properties</uicontrol>. On the properties notebook, click
<uicontrol>Edit</uicontrol>.</dd>
<dd id="ffsSaaSCustomTags">Any user-defined text that is associated with the file system. This text
can be included as a report column when you generate reports for the file system. To edit the custom
tags for a file system, right-click the file system in the list and select <uicontrol>View
Properties</uicontrol>. On the properties notebook, click <uicontrol>Edit</uicontrol>.</dd>
</dlentry>
<!--For L2 File Storage Systems page-->
<dlentry id="ffsL2ExternalPoolUsedSpaceGiB">
<dt>External Pool Used Capacity (GiB)</dt>
<dd>The capacity that is migrated to external pools for all of the file systems that are associated
with the storage system. This value represents the capacity that is used by active data (migrated
and pre-migrated) only. Active data is data that has corresponding stub files on the GPFS file system.<p>
<note type="tip">To view information about the <term>inactive data</term> in external pools, go the
details page for an <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> storage system
and click <uicontrol>File System Pools</uicontrol>. The value for <uicontrol>External Used
Capacity</uicontrol> includes active and inactive data.</note>
</p><p>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>.</note>
</p></dd>
</dlentry>
<!--For L3 File Storage Systems page-->
<dlentry id="ffsL3ExternalPoolUsedSpaceGiB">
<dt>External Pool Used Capacity (GiB)</dt>
<dd>The capacity that is migrated from the file system to external pools. This value represents the
capacity that is used by active data (migrated and pre-migrated) only. Active data is data that has
corresponding stub files on the GPFS file system.<p>
<note type="tip">To view information about the <term>inactive data</term> in external pools, go the
details page for an <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> storage system
and click <uicontrol>File System Pools</uicontrol>. The value for <uicontrol>External Used
Capacity</uicontrol> includes active and inactive data.</note>
</p><p>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>.</note>
</p></dd>
</dlentry>
<dlentry id="ffsFileSets">
<dt>Filesets</dt>
<dd>The filesets that are contained in a file system. A fileset is a hierarchical grouping of files
that is managed as a unit for balancing workload across a cluster. If a file system contains only
one fileset, the unique name of the fileset is provided. If a file system contains more than one
fileset, the number of filesets is provided.</dd>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, no value is shown if the
file system is mounted from an <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>
cluster that is not monitored.</dd>
</dlentry>
<dlentry id="ffsFileSystemType">
<!--Removed link to topic with list & link to support matrix. Need to provide a link to a topic called Supported file systems. Currently, the list of supported file systems is provided here: fqz0_r_planning_fs_to_monitor.dita. However, the topic needs to be rewritten.-->
<dt>File System Type</dt>
<dd>The type of file system.</dd>
</dlentry>
<dlentry id="ffsUsedSpaaceGiB">
<dt>Internal Used Capacity (GiB)</dt>
<dd>The amount of storage space that is unavailable (allocated) on a file system. <ph>For <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, this value does not include the space that
is used by migrated data on external pools.</ph></dd>
</dlentry>
<dlentry id="ffsLastProbeTime">
<dt>Last Probe Time</dt>
<dd>The most recent date and time when a file system was probed.</dd>
</dlentry>
<dlentry id="ffsLastScanTime">
<dt>Last Scan Time</dt>
<dd>The most recent date and time when a file system was scanned.</dd>
</dlentry>
<dlentry id="ffsName">
<dt>Name</dt>
<dd>The name of the file system.</dd>
</dlentry>
<dlentry id="ffsNSDs">
<dt>NSDs</dt>
<dd>The name of the Network Shared Disk (NSD) that contains the file system. If the file system is
on one NSD, the unique name of the NSD is shown. If the file system is on two or more NSDs, the
number of NSDs is shown.</dd>
</dlentry>
<dlentry id="ffsOwningCluster">
<dt>Owning Cluster</dt>
<dd>The name of the <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster that
owns the file system. This information is available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> storage systems. <p>The file system can be
mounted from the current <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster or
a related cluster. If the related cluster is monitored, click the name of the cluster to view
information about the <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>
cluster.</p></dd>
</dlentry>
<dlentry id="ffsPath">
<dt>Path</dt>
<dd>The file system path.</dd>
</dlentry>
<dlentry id="ffsPhysicalCapacityGiB">
<dt>Physical Capacity (GiB)</dt>
<dd>The raw capacity of the partition where a file system resides.</dd>
</dlentry>
<dlentry id="ffsPools">
<dt>Pools</dt>
<dd>The storage pool that contains the file system. If a file system is on only one storage pool,
the unique name of the storage pool is shown. If the file system is on two or more storage pools,
the number of storage pools is shown.</dd>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, no value is shown if the
file system is mounted from an <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>
cluster that is not monitored.</dd>
</dlentry>
<dlentry id="ffsQuotas">
<dt>Quotas</dt>
<dd>The quotas that are associated with the file system. Quotas restrict the number of files that
are created and the amount of file system space that is used. If only one quota is associated with
the file system, the name of the quota is shown. If there is more than one quota, the number of
quotas is shown.<p>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, no value is
shown if the file system is mounted from a related <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster that is not monitored.</p></dd>
</dlentry>
<dlentry id="ffsShortfall">
<dt>Shortfall (%)</dt>
<dd>The percentage of migrated data in external pools that does not fit into the available capacity
on the file system. The shortfall represents the relative risk of not having enough internal file
system capacity if you recall all data from the associated external pools. The higher the
percentage, the higher the amount of migrated data that does not fit.<p>
<note type="tip">Only the capacity that is used by migrated data in external pools is included in
the calculation for shortfall. Pre-migrated data is not included because it exists in both the
internal and external pools.</note>
</p><p>To calculate shortfall, the following formula is
used:</p><codeblock scale="80">[(External Pool Migrated Data − Available Capacity
) ÷ External Pool Migrated Data] × 100</codeblock><p>For example, if the available capacity on a
file system is 200 GiB, but the used capacity of migrated data on external pools is 500 GiB, the
shortfall percentage is 60% (300
GiB).</p><codeblock scale="80">[(500 − 200)÷ 500] × 100 = 60%</codeblock><p>If the file system has
sufficient available capacity to contain all the migrated data in external pools, no shortfall
exists. For example, if the available capacity on a file system is 500 GiB, and the used capacity in
external pools is only 200 GiB, the shortfall is 0%.</p></dd>
<dd>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>.</note>
</dd>
</dlentry>
<dlentry id="ffsSnapshotSpaceGiB">
<dt>Snapshot Space (GiB)</dt>
<dd>The amount of space that is used by all of the snapshots of the file system. This value includes
the snapshot space for filesets within the file system.</dd>
</dlentry>
<dlentry id="ffsSnapshots">
<dt>Snapshots</dt>
<dd>The snapshots of the file system. If there is only one snapshot of the file system, the name of
the snapshot is shown. If there is more than one snapshot of the file system, the number of
snapshots is shown.<p>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, no value
is shown if the file system is mounted from an <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> cluster that is not monitored.</p></dd>
</dlentry>
<dlentry id="ffsSVM">
<dt>Storage Virtual Machine</dt>
<dd>The storage virtual machine (SVM) to which the file system belongs. An SVM is a logical entity
that is used to serve data to clients and hosts. <note othertype="Availability" type="other"
><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/knetapp_ontap9"/> storage systems.</note></dd>
</dlentry>
<dlentry id="ffsUsedInodes">
<dt>Used Inodes</dt>
<dd>The number of used inodes on a file system. An inode is the internal structure that describes
the individual files or directories in the file system metadata. An inode contains the node, type,
owner, and location of a file or directory.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="ffsUsedSpacePercentage">
<dt>Internal Used Capacity (%)</dt>
<dd>(Previously known as Used Space) The percentage of capacity that is used on the file systems
that are associated with the resource. The following formula is used to calculate this value:</dd>
<dd>
<codeblock>[(File System Used Capacity ÷ File System Capacity) × 100]</codeblock>
</dd>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, the capacity that is used
by the snapshots of the file system is included in the Internal Used Capacity value.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>SHARES</title>
<p>
<dl>
<dlentry id="shrsCluster">
<dt>Cluster</dt>
<dd>The unique name of the cluster that a share comes from.</dd>
</dlentry>
<dlentry id="shrsDiscoveredTime">
<dt>Discovered Time</dt>
<dd>The date and time when a share was last discovered by a probe.</dd>
</dlentry>
<dlentry id="shrsName">
<dt>Name</dt>
<dd>The name of a share.</dd>
</dlentry>
<dlentry id="shrsOriginalCapacityPool">
<dt>Original Capacity Pool</dt>
<dd>The name of the capacity pool from which the share was provisioned.</dd>
</dlentry>
<dlentry id="shrsPath">
<dt>Path</dt>
<dd>The file path that is exported.</dd>
</dlentry>
<dlentry id="shrsProtocol">
<dt>Protocol</dt>
<dd>The protocols that a share supports. Examples of protocols include NFS, CIFS, HTTP, and FTP. A
share can support multiple protocols.</dd>
</dlentry>
<dlentry id="shrsServiceClass">
<dt>Service Class</dt>
<dd>The name of the file storage service class that is associated with the share.<note
othertype="Learn more" type="other">Go to <xref href="tpch_r_l2_serviceclass.dita"/>.</note></dd>
</dlentry>
<dlentry id="shrsSharedServers">
<dt>Shared Servers</dt>
<dd>The servers that have access to a share. If only one server has access to a share, the server
name is provided. If multiple servers have access, "multiple" is displayed.</dd>
</dlentry>
<dlentry id="shrsState">
<dt>State</dt>
<dd>Shows whether a share is active or inactive. An inactive share is the same as a deleted share,
with the exception that the data in an inactive share remains intact and can be viewed when the
share is reactivated.</dd>
</dlentry>
<dlentry id="shrsSVM">
<dt>Storage Virtual Machine</dt>
<dd>The storage virtual machine (SVM) to which the share belongs. An SVM is a logical entity that is
used to serve data to clients and hosts. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
<keyword conref="fqz0_entities.dita#fqz0_entities/knetapp_ontap9"/> storage systems.</note></dd>
</dlentry>
<dlentry id="shrsTicket">
<dt>Ticket</dt>
<dd>A ticket identifier that was associated with the share for tracking purposes. The ticket
identifier was specified when the share was created.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>SNAPSHOTS FOR <tm tmtype="tm" trademark="GPFS" tmclass="IGNORE">GPFS</tm> FILE SYSTEMS or
FILESETS</title>
<p>
<dl>
<dlentry id="snapFileSystem">
<dt>File System</dt>
<dd>The file system that the snapshot is created from. You can create a snapshot of an entire file
system, or one of the filesets within a file system. The fileset column is blank if the snapshot is
for the entire file system.</dd>
</dlentry>
<dlentry id="snapValid">
<dt>Valid</dt>
<dd>The status of the snapshot. A value of Yes indicates that the snapshot is valid. A value of No
indicates that the snapshot is invalid. The snapshot might be invalid because, for example, it is
being deleted. Use the status to determine the condition of a snapshot, and if any actions must be
taken.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>DISK CONTROLLERS</title>
<p>
<dl>
<dlentry id="dskctrlAssociatedDisks">
<dt>Associated Disks</dt>
<dd>The number of the disks that are controlled by the disk controller. A disk might be controlled
by more than one controller.</dd>
</dlentry>
<dlentry id="dskctrlDescription">
<dt>Description</dt>
<dd>The user-defined description of the disk controller.</dd>
</dlentry>
<dlentry id="dskctrlHBAWWN">
<dt>HBA WWN</dt>
<dd>The World Wide Name (WWN) of the disk controller. The WWN is a 64-bit string that uniquely
identifies the controller. The value is only available for host bus adapters (HBAs).</dd>
</dlentry>
<dlentry>
<dt>Name</dt>
<dd id="dskctrlName">The name of the disk controller on the server or hypervisor.</dd>
<dd id="dskctrSaaSlName">The name of the disk controller on the server.</dd>
</dlentry>
<dlentry id="dskctrlType">
<dt>Type</dt>
<dd>The type of disk controller, such as IDE, SCSI, floppy, or RAID. The type of disk controller
that Host Bus Adapters (HBAs) use is FCAL (Fibre Channel Arbitrated Loop).</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>CLUSTERS</title>
<p>
<dl>
<dlentry id="clstrID">
<dt>ID</dt>
<dd>The unique ID of the cluster.</dd>
</dlentry>
<dlentry id="clstrName">
<dt>Name</dt>
<dd>The name of the cluster that uniquely identifies it within the storage system.</dd>
</dlentry>
<dlentry id="clstrType">
<dt>Type</dt>
<dd>The type of a cluster such as an interface cluster or storage cluster, or both.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>FILE NODES</title>
<p>
<dl>
<dlentry id="fnodeCacheGatewayNode">
<dt>Cache Gateway Node</dt>
<dd>Shows whether the interface node is enabled to work as a caching gateway node that can exchange
data with other systems.</dd>
</dlentry>
<dlentry id="fnodeCluster">
<dt>Cluster</dt>
<dd>The name of the cluster that the node belongs to or is a member of. A cluster is a group of
application servers or nodes that collaborate for workload balancing and failover. A failover is an
automatic operation that switches to a redundant or standby system in the event of a software,
hardware, or network interruption.</dd>
</dlentry>
<dlentry id="fnodeIPAddress">
<dt>IP Address</dt>
<dd>The IP address or fully qualified domain name of the node.</dd>
</dlentry>
<dlentry id="fnodeName">
<dt>Name</dt>
<dd>The unique name of the node.</dd>
</dlentry>
<dlentry id="fnodeRole">
<dt>Role</dt>
<dd>The server can have multiple roles, such as management, interface, and storage.</dd>
</dlentry>
<dlentry id="fnodeStatus">
<dt>Status</dt>
<dd>The status of the node. Statuses include Normal, Warning, Error, and Unknown. Use the status to
determine the condition of a node, and if any actions must be taken. For example, if a node has an
Error status, take immediate action to correct the problem.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>FILE SYSTEM POOLS</title>
<p>
<dl>
<dlentry id="fpoolAvailableSpaceGiB">
<dt>Available Capacity (GiB)</dt>
<dd>The amount of unallocated storage capacity in the pool. Available capacity usually comprises the
space that can be used for storage. However, if the pool is not formatted, the amount of overhead
capacity might also be included in the calculation.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
<dlentry id="fpoolCapacityPer">
<dt>Capacity (%)</dt>
<dd>The percentage of used capacity in the file system pool. Hover over the bar for a specific pool
to view statistics about its used and available capacity.<p>For external pools that are connected to
cloud services or storage providers, only the amount of used capacity is shown. Cloud services and
other storage providers might include <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_protect"/>, <keyword
conref="fqz0_entities.dita#fqz0_entities/kspectrum_archive"/>, Amazon Simple Storage Service (S3),
or OpenStack Swift.</p><p>For external pools that are connected to vaults in <keyword
conref="fqz0_entities.dita#fqz0_entities/kcloudobject_short"/>, the percentage of used capacity
includes active and inactive data. Space values are determined from the usable vault capacity that
is associated with the cloud account and the hard quota value (if a hard quota is configured). Hover
over the bar for a specific pool to view statistics about its used active space, used inactive
space, and available capacity. </p><dl>
<dlentry>
<dt>Active and inactive data in external pools</dt>
<dd>Active data is data that has corresponding stub files on the GPFS file system. Inactive data is
data that exists in the external pool but is not accessible in the GPFS file system. Data is
classified as inactive under these conditions: <ul>
<li>When you migrate a file off a GPFS file system, its data is still visible and accessible through
its stub file. When the stub file is deleted, the data in the external pool is no longer accessible
and is considered inactive. </li>
<li>When you modify a migrated file, it's recalled from the external pool and modified locally. The
previous data for the file remains in the external pool but is no longer accessible through the GPFS
file system and is considered inactive. </li>
</ul><note type="tip">To keep the files in sync between an external pool and a GPFS file system and
clean up inactive data, you can run the <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st_short"/>
<codeph>mmcloudgateway files reconcile</codeph> command. For more information, see <image
href="sout.gif" placement="inline">
<alt>External link icon</alt>
</image><xref
href="http://www.ibm.com/support/knowledgecenter/STXKQY_4.2.1/com.ibm.spectrum.scale.v4r21.doc/bl1adm_reconcile.htm"
format="html" scope="external"><text>Reconciling files between IBM Spectrum Scale file system and
cloud storage tier</text></xref>.</note></dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="fpoolCluster">
<dt>Cluster</dt>
<dd>The unique name of the cluster that the pool belongs to.</dd>
</dlentry>
<dlentry id="fpoolExternalUsedSpace">
<dt>External Used Capacity (GiB)</dt>
<dd>The amount of used capacity on an external pool. This value includes active data and inactive
data. Active data is data that has corresponding stub files on the GPFS file system. Inactive data
is data that exists in the external pool but is not accessible in the GPFS file system. </dd>
<dd>
<note othertype="Availability" type="other">External pools that are provided by <keyword
conref="fqz0_entities.dita#fqz0_entities/kcloudobject_short"/>. </note>
</dd>
</dlentry>
<dlentry id="fpoolFileSystem">
<dt>File System</dt>
<dd>The name of the file system that is associated with the pool.</dd>
</dlentry>
<dlentry id="fpoolInactiveUsedSpace">
<dt>Inactive Used Capacity</dt>
<dd>The amount of inactive used capacity on an external pool. Inactive data is data that exists in
the external pool but is not accessible in the GPFS file system. Available for external pools that
are provided by Cloud Object Storage only. </dd>
<dd>
<note othertype="Availability" type="other">External pools that are provided by <keyword
conref="fqz0_entities.dita#fqz0_entities/kcloudobject_short"/>. </note>
</dd>
</dlentry>
<dlentry id="fpoolName">
<dt>Name</dt>
<dd>The unique name of a pool that identifies it within the storage system.</dd>
</dlentry>
<dlentry id="fpoolNSDs">
<dt>NSDs</dt>
<dd>The unique names of the Network Shared Disks (NSDs) that the pool is on.</dd>
</dlentry>
<dlentry id="fpoolStorageSystem">
<dt>Storage System</dt>
<dd>The name of the storage system that contains the pool. The name of the storage system was
defined when the storage system was added for monitoring. If a name was not defined, the ID of the
storage system is shown.</dd>
</dlentry>
<dlentry>
<dt>Capacity (GiB)</dt>
<dd id="fpoolTotalCapacityGiB">(Previously known as Total Capacity) The total amount of storage
space in the storage pool.</dd>
<dd id="fpoolTotalCapacityGiB2">For external pools that are provided by <keyword
conref="fqz0_entities.dita#fqz0_entities/kcloudobject_short"/>, this value represents External Used
Capacity + Available Capacity .</dd>
</dlentry>
<dlentry id="fpoolUsedSpace">
<dt>Used Capacity</dt>
<dd>The amount of capacity in the file system pool that is being used. For external pools, used
capacity includes active data only. Active data is data that has corresponding stub files on the
GPFS file system.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>NSDs</title>
<p>
<dl>
<dlentry id="fnsdAcknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of an NSD as acknowledged. An acknowledged status
indicates that the status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of the NSD is Error, then the status of the related storage system is
also Error. If the Error status of the NSD is acknowledged, then its status is not used to determine
the overall status of the related storage system. In this case, if the other internal resources of
the storage system are Normal, then the status of the storage system is also Normal. </dd>
</dlentry>
<dlentry id="fnsdAvailableSpaceGiB">
<dt>Available Capacity (GiB)</dt>
<dd>The amount of storage capacity that is available, but not allocated, on the NSD.</dd>
</dlentry>
<!--<dlentry id="fnsdCapacityPool"><dt>Capacity Pool</dt><dd>The name of the capacity pool that the NSD is assigned to. This field applies only to <keyword conref="fqz0_entities.dita#fqz0_entities/ksonas_short"/> storage systems.</dd></dlentry>-->
<dlentry>
<dt>Cluster</dt>
<dd>The unique name of the cluster in which the NSD is a member.</dd>
</dlentry>
<dlentry id="fnsdCorellatedStorageSystem">
<dt>Correlated Storage Systems</dt>
<dd>Correlated storage systems are storage volumes that are configured to use the information that
is contained on an NSD. If only one storage system is correlated to an NSD, the name of the storage
volume is provided. If multiple storage volumes are correlated, the number of storage volumes is
provided.</dd>
</dlentry>
<dlentry id="fnsdCorellatedStorageVolume">
<dt>Correlated Storage Volume</dt>
<dd>The name of the volume that provides storage to the NSD. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, the correlated volume is on a back-end
storage system.</dd>
</dlentry>
<dlentry>
<dt>Custom Tag 1, 2, and 3</dt>
<dd id="fnsdCustomTags">Any user-defined text that is associated with the NSD. This text can be
included as a report column when you generate reports for the NSD. <p>You can tag an NSD to satisfy
custom requirements for a service class. A service class can specify up to three required tags. To
provide a service class, storage resources must have all the same tags that are specified by the
service class. If an NSD is not tagged, any tags on the containing storage system also apply to the
NSD for determining whether it satisfies the custom requirements of a service class.</p>To edit the
custom tags for an NSD, right-click the NSD in the list and select <uicontrol>View
Properties</uicontrol>. On the properties notebook, click <uicontrol>Edit</uicontrol>.</dd>
<dd id="fnsdSaaSCustomTags">Any user-defined text that is associated with the NSD. The text that is
added can be included in reports that are generated about the NSD. To edit the custom tags for the
NSD, right-click the NSD in the list and click <uicontrol>View Properties</uicontrol>. In the
properties notebook, click <uicontrol>Edit</uicontrol>.</dd>
</dlentry>
<dlentry id="fnsdFailureGroup">
<dt>Failure Group</dt>
<dd>The failure group ID for NSDs that share a common point of failure. </dd>
<!--06FEB 2015 PL Rewrote TPC content for SI. IF TPC wants to use this definition they can otherprop it: <dd>The ID of the failure group to which the NSD belongs. A failure
group is a group of NSDs that share a common point of failure. The
group information is used to ensure that replicas of the same block
are not written in such a way as to become unavailable due to a single
failure. The value None indicates that the NSD does not share a common
point of failure with any other disk.</dd>-->
</dlentry>
<dlentry id="fnsdFileSystems">
<dt>File System</dt>
<dd>The name of the file system on the NSD or the number of file systems on the NSD.</dd>
</dlentry>
<dlentry id="fnsdID">
<dt>ID</dt>
<dd>The unique ID of the NSD.</dd>
</dlentry>
<dlentry id="fnsdName">
<dt>Name</dt>
<dd>The name of the NSD that uniquely identifies it within a storage system.</dd>
</dlentry>
<dlentry id="fnsdNSDServers">
<dt>NSD Servers</dt>
<dd>This information is available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> storage systems. An NSD server is a cluster
node that is physically connected to the NSD and provides access to the NSD for other cluster nodes.
<p>The number of cluster nodes that are configured as NSD servers for the NSD is shown. If only one
NSD server is configured, the name of the cluster node is shown. If the column is blank, the NSD
does not have an NSD server and all the cluster nodes must be physically connected to the
NSD.</p><p>If the NSD server is disconnected from the NSD, the cluster nodes that access the NSD
through the NSD server might lose their connection unless redundant NSD servers are configured. </p>
</dd>
</dlentry>
<dlentry id="fnsdPool">
<dt>Pool</dt>
<dd>The unique name of the pool where the NSD is.</dd>
</dlentry>
<dlentry id="fnsdProbeTime">
<dt>Probe Time</dt>
<dd>The most recent date and time when the NSD was probed.</dd>
</dlentry>
<dlentry id="fnsdStatus">
<dt>Status</dt>
<dd>The status of the NSD such as normal, warning, error, or unknown. Use the status to determine
the condition of the NSD, and if any actions must be taken. For example, if the NSD has an error
status, take immediate action to correct the problem.</dd>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, an Error or Warning status
might be due to a connection problem between one of the NSD servers and the NSD. To check the status
of the NSD server connections, click the NSD row, select <uicontrol>View Properties</uicontrol>, and
click the <uicontrol>NSD Servers</uicontrol> tab. The Connectivity column shows the status of the
connection. Alternatively, if the NSD does not have an NSD server, the Error or Warning status might
be due to a connection problem between the NSD and one of the cluster nodes.</dd>
</dlentry>
<dlentry id="fnsdStorageSystem">
<dt>Storage System</dt>
<dd>The unique name of the storage system where the NSD is located.</dd>
</dlentry>
<dlentry>
<dt>Capacity (GiB)</dt>
<dd id="fnsdTotalDiskCapacityGiB">(Previously known as Total Disk Capacity) The total size of the
storage capacity on the NSD.</dd>
</dlentry>
<dlentry id="fnsdType">
<dt>Type</dt>
<dd>The type of information that is stored on the NSD. The following values might be shown:<ul>
<li>Data</li>
<li>Metadata</li>
<li>Data, Metadata. The NSD contains both data and metadata.</li>
<li>File System Descriptor. The NSD contains only a copy of the file system descriptor.</li>
<li>Local Cache. The NSD is used by the cluster node as a local cache to enable efficient access to
data.</li>
</ul></dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>FILESETS</title>
<p>
<dl>
<dlentry id="fsFileSets">
<dt>Cache Role</dt>
<dd>Specifies whether the cached fileset is a <systemoutput>Cache target</systemoutput> or a
<systemoutput>Cache source</systemoutput>. Cache target is shown when the fileset contains the
cached data of the source filesets. Cache source is shown for filesets that have copies of target
filesets. If the cached fileset is neither a cache target nor a cache source, the field is left
blank.</dd>
</dlentry>
<dlentry id="fsFileSystem">
<dt>File System</dt>
<dd>The name of the file system that is associated with the fileset.</dd>
</dlentry>
<dlentry id="fsHostSystemName">
<dt>Home System Name</dt>
<dd>Shows the name of the share or export on the home fileset that is enabled for caching.</dd>
</dlentry>
<dlentry id="fsIndependent">
<dt>Independent</dt>
<dd>This information is available only for <keyword
conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/> storage systems. If the column is marked,
the fileset is an independent fileset that has its own inode space. If the column is blank, the
fileset is a dependent fileset.<p>An inode space is a collection of inode number ranges that are
reserved for an independent fileset. A dependent fileset shares the inode space of an independent
fileset.</p></dd>
</dlentry>
<dlentry id="fsName">
<dt>Name</dt>
<dd>The name of the fileset.</dd>
</dlentry>
<dlentry id="fsPath">
<dt>Path</dt>
<dd>The path for the fileset. The path is displayed only if <uicontrol>Linked</uicontrol> is
displayed in the <uicontrol>State</uicontrol> field.</dd>
</dlentry>
<dlentry id="fsQuotas">
<dt>Quotas</dt>
<dd>The quotas that are associated with the fileset. Quotas restrict the number of files that are
created and the amount of file system space that is used. If there is only one quota that is
associated with the fileset, the name of the quota is shown. If there is more than one quota, the
number of quotas is shown.</dd>
</dlentry>
<dlentry id="fsSnapshots">
<dt>Snapshots</dt>
<dd>The snapshots of the fileset. If there is only one snapshot of the fileset, the name of the
snapshot is shown. If there is more than one snapshot of the fileset, the number of snapshots is
shown.</dd>
</dlentry>
<dlentry id="fsState">
<dt>State</dt>
<dd>The state of the fileset. Valid values are listed. <dl>
<dlentry>
<dt>Linked</dt>
<dd>The fileset is linked to the file system that is displayed in the <uicontrol>File
System</uicontrol> field.</dd>
</dlentry>
<dlentry>
<dt> Unlinked</dt>
<dd>The fileset is not linked to the file system that is displayed in the <uicontrol>File
System</uicontrol> field. </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry id="fsUsedInodes">
<dt>Used Inodes</dt>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, the number of inodes that
are used by the fileset. An inode is the internal structure that describes the individual files in
the file system metadata. An inode contains the node, type, owner, and location of a file.</dd>
</dlentry>
<dlentry id="fsUsedInodesPercentage">
<dt>Used Inodes (%)</dt>
<dd>For <keyword conref="fqz0_entities.dita#fqz0_entities/kelastic_st"/>, the percentage of the
total inodes in the inode space that is already used by the fileset. <p>The total number of inodes
in the inode space is represented by a bar. The percentage of inodes that is used by the fileset are
represented by the blue section of the bar and as a percentage figure on the bar. The inodes that
are used by other filesets that share the inode space are represented by the dark gray section of
the bar. The available inodes in the inode space are represented by the light gray section of the
bar.</p></dd>
</dlentry>
<dlentry id="fsUsedSpaceGiB">
<dt>Used Capacity (GiB)</dt>
<dd>The amount of storage capacity that is used by the fileset. Used capacity is not provided for
filesets that are cache targets.</dd>
<dd>
<note othertype="Availability" type="other">All storage systems.</note>
</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>Reusable definitions for
Quotas<!--3 FEB 2015 PL Hover help is provided at table-view level so not much to add here.--></title>
<p>
<dl>
<dlentry id="quotas_CapacityPercent">
<dt>Capacity (%)</dt>
<dd><image href="quotas_capacity_bar.svg" placement="break">
<alt>Illustrates used capacity and soft limits and hard limits.</alt>
</image><!--TVT Instructions: This is a diagram, not a screen capture, so there are no instructions for how to make it appear in Storage Insights. The source file for this diagram is quotas_capacity_bar.svg.-->The
percentage of the disk space quota that is already used by the user, group of users, or fileset.
</dd>
</dlentry>
<dlentry id="quotas_Type">
<dt>Type</dt>
<dd>The type of quota on the <tm trademark="IBM Spectrum" tmtype="reg">IBM Spectrum</tm> Scale
cluster such as user, group, or fileset quota. For group quotas, the capacity and inode usage values
for all of the users of the operating system group are calculated. For fileset quotas, the capacity
and inode usage values for the users who use the fileset are calculated.</dd>
</dlentry>
<dlentry id="quotas_UsedInodesPercent">
<dt>Used Inodes (%)</dt>
<dd>The percentage of the inode quota that is already used by the user, group of users, or
fileset.</dd>
<dd>The capacity of the quota is represented by a bar, and the Inodes Soft Limit and Inodes Hard
Limit values are shown as lines on the bar. If the Used Inodes value is less than the soft limit,
the percentage of the quota that is used is shown in blue on the capacity bar. If the Used Inodes
value exceeds the soft limit, the percentage that is used is shown in yellow. If the Used Inodes
value reaches or exceeds the hard limit, the percentage that is used is shown in red.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>FILE NODES</title>
<!--5 FEB 2015 PL As hover help is provided in the table view only 2 of the column headings require definitions.-->
<p>
<dl>
<dlentry id="ffsCacheGatewayNode">
<dt>Cache Gateway Node</dt>
<dd>The interface node is enabled or is not enabled as a cache gateway node that can exchange data
with other systems.</dd>
</dlentry>
<dlentry id="ffsNodeCluster">
<dt>Cluster</dt>
<dd>The node is a member of the specified cluster. A cluster is a group of application servers or
nodes that work together to provide workload balancing and failover services.</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>Reusable definitions for Charts</title>
<p>
<dl>
<dlentry id="bchtAssignedVolumes">
<dt>Assigned (Volumes)</dt>
<dd>Total volume space within monitored storage systems that is mapped or assigned to host
systems.</dd>
</dlentry>
<dlentry id="bchtPoolSpace">
<dt>Pool Space</dt>
<dd>Total amount of storage space in the storage pools or pools that are associated with monitored
storage systems. For <keyword conref="fqz0_entities.dita#fqz0_entities/ksvc"/>, this value
represents the amount of managed disk space that is assigned to the managed disk group. For <keyword
conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>, this value represents the physical capacity of
the pool, not the provisioned capacity. </dd>
</dlentry>
<dlentry id="bchtReserved">
<dt>Reserved</dt>
<dd>Amount of pool space that is reserved by provisioning tasks. Pool space is reserved when a
provisioning task is created, and assigned when a provisioning task is run.</dd>
</dlentry>
<dlentry id="bchtUnassignedVolumes">
<dt>Unassigned (Volumes)</dt>
<dd>Total volume space within monitored storage systems that is not mapped or assigned to host
systems.</dd>
</dlentry>
<dlentry id="bchtUnreserved">
<dt>Unreserved</dt>
<dd>Amount of capacity in storage system pools that is not used by volumes, and is not reserved by
pending or scheduled provisioning tasks.</dd>
</dlentry>
<dlentry>
<dt>Volumes</dt>
<dd id="bchtVolumes">Total amount of unique storage system volume capacity or provisioned storage
volume capacity on monitored storage systems. For thin-provisioned volumes, this value is the
physical capacity of the volume. In an <keyword conref="fqz0_entities.dita#fqz0_entities/kxss"/>,
this value is referred to as the real capacity, or physical ("hard") capacity, and not the
potentially much larger provisioned ("soft") capacity, that is allocated. For <keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> devices, this value represents the total amount
of storage capacity on the Fibre Channel (FC) LUNs that are associated with the devices.</dd>
<dd id="bchtSaaSVolumes">Total amount of unique storage system volume capacity or provisioned
storage volume capacity on monitored storage systems. For thin-provisioned volumes, this value is
the physical capacity of the volume. In an <keyword conref="fqz0_entities.dita#fqz0_entities/kxss"
/>, this value is referred to as the real capacity, or physical ("hard") capacity, and not the
potentially much larger provisioned ("soft") capacity, that is allocated.</dd>
</dlentry>
</dl>
</p>
<p>
<dl>
<dlentry id="fchtExportedFS">
<dt>Exported FS (File Systems)</dt>
<dd>The amount of storage space from file systems on storage systems that is exported to servers. An
export or share is a file system that is made available to remote servers over a network.<dl>
<dlentry>
<dt>Exported Used</dt>
<dd>The amount of exported file system space that is used by objects in the file systems, such as
files and directories.</dd>
</dlentry>
<dlentry>
<dt>Exported Available</dt>
<dd>The amount of exported file system space that is unused and available for use by objects in the
file systems.</dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="fchtFileSystemCapacity">
<dt>File System Capacity</dt>
<dd>Total amount of storage space on all the file systems on a storage system or filer.</dd>
</dlentry>
<dlentry id="fchtUnexportedFS">
<dt>Unexported FS (File Systems)</dt>
<dd>The amount of storage space in file systems on storage systems that is not exported to remote
servers. <note type="restriction">For a <keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
filer, <keyword conref="fqz0_entities.dita#fqz0_entities/ktpc_short"/> uses a <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra"/> to determine the file systems that are exported
from the filer. Depending on the operating system of the computer where the <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra"/> is deployed, some exported file systems might not
be detected during a probe. Specifically, a <tm tmtype="reg" trademark="UNIX">UNIX</tm>
<keyword conref="fqz0_entities.dita#fqz0_entities/ksra"/> might not detect the file systems that are
exported with CIFS; a <tm tmtype="tm" trademark="Windows">Windows</tm>
<keyword conref="fqz0_entities.dita#fqz0_entities/ksra"/> might not detect the file systems that are
exported with NFS.<p>Because of this limitation, the value for <uicontrol>Unexported FS</uicontrol>
might include both the space of unexported file systems and the space of exported file systems. To
work around this limitation, set up multiple <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra_pl"/> on different operating systems to probe the
<keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> filer. For example, deploy a <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra"/> to a <tm tmtype="tm" trademark="Windows"
>Windows</tm> system and another <keyword conref="fqz0_entities.dita#fqz0_entities/ksra"/> to a <tm
tmtype="reg" trademark="UNIX">UNIX</tm> system. Then, use both <keyword
conref="fqz0_entities.dita#fqz0_entities/ksra_pl"/> to probe the <keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> filer.</p>
</note><dl>
<dlentry>
<dt>Unexported Used</dt>
<dd>The amount of unexported file system space that is used by objects in the file systems, such as
files and directories. </dd>
</dlentry>
<dlentry>
<dt>Unexported Available</dt>
<dd>The amount of unexported file system space that is unused and available for use by objects in
the file systems. </dd>
</dlentry>
</dl></dd>
</dlentry>
<dlentry id="fchtSaaaSUnexportedFS">
<dt>Unexported FS (File Systems)</dt>
<dd>The amount of storage space in file systems on storage systems that is not exported to remote
servers. <dl>
<dlentry>
<dt>Unexported Used</dt>
<dd>The amount of unexported file system space that is used by objects in the file systems such as
files and directories. </dd>
</dlentry>
<dlentry>
<dt>Unexported Available</dt>
<dd>The amount of unexported file system space that is unused and available for use by objects in
the file systems. </dd>
</dlentry>
</dl></dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>Other</title>
<p>
<dl>
<dlentry id="OverheadSpace">
<dt>Overhead Space</dt>
<dd>For storage systems other than <keyword conref="fqz0_entities.dita#fqz0_entities/kxss_pl"/>,
overhead space is the space that is not available for user data and is used by the storage system
for formatting pools, or redundancy, or managing disks.</dd>
<dd>The formula that is used to calculate overhead space is as
follows:<codeblock>[Capacity − (Available Capacity
 + Used Capacity)]</codeblock></dd>
</dlentry>
<dlentry id="sbackupVolumeCapacity">
<dt>Backup Volume Capacity (GiB)</dt>
<dd>The total capacity of all of the volumes that are target volumes or that are source and target
volumes in <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> relationships or remote copy
relationships.</dd>
</dlentry>
<dlentry id="sProtectedVolumeCapacity">
<dt>Protected Volume Capacity (GiB)</dt>
<dd>The total capacity of all of the volumes that are source volumes in <tm tmtype="reg"
trademark="FlashCopy">FlashCopy</tm> relationships and remote copy relationships.</dd>
</dlentry>
<dlentry id="sUnprotectedVolumeCapacity">
<dt>Unprotected Volume Capacity (GiB)</dt>
<dd>The total capacity of all of the volumes that are not source volumes, or target volumes, or
source and target volumes in <tm tmtype="reg" trademark="FlashCopy">FlashCopy</tm> relationships or
remote copy relationships.</dd>
</dlentry>
<dlentry id="sVDiskMirroringCapacity">
<dt>VDisk Mirroring Volume Capacity (GiB)</dt>
<dd>The total capacity of the target volumes or secondary volumes of mirrored volumes in storage
virtualizer pools. Applies to mirrored volumes in pools in <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kstorwize_short"/> family storage systems that are
configured with block storage.</dd>
</dlentry>
</dl>
</p>
</section>
<section otherprops="5215_161230new">
<title><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> Network Interfaces</title>
<p>
<dl>
<dlentry id="ni_acknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a network interface as acknowledged. An acknowledged
status indicates that the status was reviewed and is either resolved or can be ignored. An
acknowledged status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a network interface is Error, the status of the storage system
that contains it is also Error. If the Error status for the network interface is acknowledged, then
its status is not used to determine the overall status of the associated storage system. In this
case, if the other internal resources of the storage system are Normal, then the status of the
storage system is also Normal. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="ni_current_node">
<dt><tm trademark="Current" tmtype="reg">Current</tm> Node</dt>
<dd>The node on which the network logical interface (LIF) currently resides. After a LIF migration,
the <tm trademark="Current" tmtype="reg">Current</tm> Node might be different from the Home Node.
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="ni_current_port">
<dt><tm trademark="Current" tmtype="reg">Current</tm> Port</dt>
<dd>The port on which the network logical interface (LIF) currently resides. After a LIF migration,
the <tm trademark="Current" tmtype="reg">Current</tm> Port might be different from the Home Port.
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="ni_data_protocol">
<dt>Data Protocol Access</dt>
<dd>Specifies the protocols that a network logical interface (LIF) uses to access data from the
Storage Virtual Machine. By default the protocols are NFS, CIFS, and FlexCache. Other supported
protocols are iSCSI and FCP. You can configure a LIF to not support any data protocols by specifying
None. You cannot combine protocol values of None, iSCSI or FCP with any other data protocol. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="ni_home_node">
<dt>Home Node</dt>
<dd>The node that was specified when the network logical interface (LIF) was created. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="ni_home_port">
<dt>Home Port</dt>
<dd>The port that was specified when the network logical interface (LIF) was created. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="ni_ip_address_wwpn">
<dt>IP Address or WWPN</dt>
<dd>The IP address of the network interface for all data protocols except FCP. For network
interfaces that use FCP, the WWPN is shown.<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="ni_ipspace">
<dt>IPspace</dt>
<dd>The name of the IP address space associated with the network interface. IPspaces enable you to
configure a single storage system to be accessed by clients from more than one disconnected network,
even if those clients are using the same IP address. </dd>
<dd>Default and Cluster IPspaces are created automatically when the cluster is initialized. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="ni_name">
<dt>Name</dt>
<dd>The name that was specified when the network logical interface (LIF) was created. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="ni_role">
<dt>Role</dt>
<dd>Determines the kind of traffic that is supported over the network logical interface (LIF), along
with the failover rules that apply and the firewall restrictions that are in place. Possible values
are listed. </dd>
<dd>
<dl>
<dlentry>
<dt>Node Management</dt>
<dd>A LIF that provides a dedicated IP address for managing a particular node in a cluster.</dd>
</dlentry>
<dlentry>
<dt>Cluster Management</dt>
<dd>A LIF that provides a single management interface for the entire cluster.</dd>
</dlentry>
<dlentry>
<dt>Cluster</dt>
<dd>A LIF that is used to carry intracluster traffic between nodes in a cluster. </dd>
</dlentry>
<dlentry>
<dt>Intercluster</dt>
<dd>A LIF that is used for cross-cluster communication, backup, and replication. </dd>
</dlentry>
<dlentry>
<dt>Data</dt>
<dd>A LIF that is associated with a Storage Virtual Machine (SVM) and is used for communicating with
clients. </dd>
</dlentry>
</dl>
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note>
</dd>
</dlentry>
<dlentry id="ni_status">
<dt>Status</dt>
<dd>The status of a network interface. Statuses include Normal, Warning, Error, and Unknown. Use the
status to determine the condition of a network interface, and if any actions must be taken. For
example, if a network interface has an Error status, take immediate action to correct the problem.
</dd>
</dlentry>
<dlentry id="ni_svm">
<dt>Storage Virtual Machine</dt>
<dd>The storage virtual machine to which the network interface belongs. An SVM is a logical entity
that is used to serve data to clients and hosts. <note othertype="Availability" type="other"
><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="ni_subnet">
<dt>Subnet</dt>
<dd>The subnet that the network logical interface is allocated to.<note othertype="Availability"
type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage
systems.</note></dd>
</dlentry>
</dl>
</p>
</section>
<section otherprops="5215_161230new">
<title><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> Qtrees</title>
<p>
<dl>
<dlentry id="qt_acknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a qtree as acknowledged. An acknowledged status
indicates that the status was reviewed and is either resolved or can be ignored. An acknowledged
status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of a qtree is Error, the status of the storage system that contains
it is also Error. If the Error status for the qtree is acknowledged, then its status is not used to
determine the overall status of the associated storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also
Normal.</dd>
</dlentry>
<dlentry id="qt_export_policy">
<dt>Export Policy</dt>
<dd>In Clustered Data ONTAP, an export policy is used to specify the level of client access that is
allowed to a volume or the qtrees that it contains. <note othertype="Availability" type="other"
><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="qt_mount_path">
<dt>Mount Path</dt>
<dd>In Clustered Data ONTAP, specifies the mount path of the volume that contains the qtree. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="qt_name">
<dt>Name</dt>
<dd>The name that was specified when the qtree was created. <note othertype="Availability"
type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage
systems.</note></dd>
</dlentry>
<dlentry id="qt_oplocks">
<dt>Oplocks</dt>
<dd>Specifies whether clients are able to lock files and cache content locally. The default value is
Enabled. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="qt_security_style">
<dt>Security Style</dt>
<dd>The security style defines the permissions that are used to control data access and the clients
that can modify permissions. Possible values are <tm trademark="UNIX" tmtype="reg">UNIX</tm>, NTFS,
or Mixed. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="qt_status">
<dt>Status</dt>
<dd>The status of a qtree. Statuses include Normal, Warning, Error, and Unknown. Use the status to
determine the condition of a qtree, and if any actions must be taken. For example, if a qtree has an
Error status, take immediate action to correct the problem. <note othertype="Availability"
type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage
systems.</note></dd>
</dlentry>
<dlentry id="qt_svm">
<dt>Storage Virtual Machine</dt>
<dd>The storage virtual machine (SVM) to which the qtree belongs. An SVM is a logical entity that is
used to serve data to clients and hosts. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="qt_volume">
<dt>Volume</dt>
<dd>The volume that contains the qtree.<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
</dl>
</p>
</section>
<section otherprops="5215_161230new">
<title><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/> Storage Virtual Machines</title>
<p>
<dl>
<dlentry id="svm_acknowledged">
<dt>Acknowledged</dt>
<dd>Shows whether a user marked the status of a storage virtual machine (SVM) as acknowledged. An
acknowledged status indicates that the status was reviewed and is either resolved or can be ignored.
An acknowledged status is not used when the status of related, higher-level resources is determined. </dd>
<dd>For example, if the status of an SVM is Error, the status of the storage system that contains it
is also Error. If the Error status for the qtree is acknowledged, then its status is not used to
determine the overall status of the associated storage system. In this case, if the other internal
resources of the storage system are Normal, then the status of the storage system is also
Normal.</dd>
</dlentry>
<dlentry id="svm_allowed_protocols">
<dt>Allowed Protocols</dt>
<dd>Specifies the data access protocols that are allowed on the SVM. By default the protocols are
NFS, CIFS, and FlexCache. Other supported protocols are NFS, CIFS, ISCSI, FCP, and NDMP.<note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="svm_condition">
<dt>Condition</dt>
<dd>The condition of the SVM. Possible values are Critical, Error, Warning, Normal, or Unknown.
<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="svm_ipspace">
<dt>IPspace</dt>
<dd>The type of the storage virtual machine (SVM). A Data SVM serves data to clients and hosts. The
special SVM types for administrative purposes are Administrator, Node, and System. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="svm_name">
<dt>Name</dt>
<dd>The name of the SVM. <note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
<dlentry id="svm_snapshot_policy">
<dt>Snapshot Policy</dt>
<dd>The policy that specifies the Snapshot backup and retention settings for the SVM.<note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="svm_subtype">
<dt>Subtype</dt>
<dd>The subtype of the storage virtual machine (SVM). A Data SVM serves data to clients and hosts.
The special SVM types for administrative purposes are Administrator, Node, and System. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="svm_type">
<dt>Type</dt>
<dd>The type of the storage virtual machine (SVM). A Data SVM serves data to clients and hosts. The
special SVM types for administrative purposes are Administrator, Node, and System. <note
othertype="Availability" type="other"><keyword conref="fqz0_entities.dita#fqz0_entities/knetapp"/>
storage systems.</note></dd>
</dlentry>
<dlentry id="svm_volume_type">
<dt>Volume Type</dt>
<dd>The type of volume that can be configured on the SVM. Allowed values are FlexVol and Infinite
Volume.<note othertype="Availability" type="other"><keyword
conref="fqz0_entities.dita#fqz0_entities/knetapp"/> storage systems.</note></dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>OBJECT STORAGE SYSTEMS</title>
<p id="access_objects">The accounts and the containers that you can monitor in the GUI are
determined by the role that is associated with your login account for the storage system.</p>
<p>
<dl>
<dlentry id="obj_available_space">
<dt>Available Capacity (GiB)</dt>
<dd>The amount of file system capacity that is available for storing object data. </dd>
</dlentry>
<dlentry id="obj_total_capacity">
<dt>Capacity (GiB)</dt>
<dd>(Previously known as Total Capacity) The total capacity is calculated by adding the space in the
file system that can be used for storing object data to the space that is already used for object
data.</dd>
<!--26 FEB 2016 PL Awaiting confirmation for replacing this text: For example, the capacity of a GPFS file system is 500 GiB. If 100 GiB is used for file storage and 200 GiB is used for object storage, then the total capacity value is 400 GiB. Provisional new text added.-->
<dd>For example, the capacity of the file system is 500 GiB. Of the 500 GiB, 100 GiB is used for
file storage and 200 GiB is used for object data storage. In this example, the total capacity for
object data is <tm trademark="400" tmtype="reg">400</tm> GiB, which is calculated by adding the
remaining capacity for the file system (200 GiB) to the space that is being used for storing the
object data (200 GiB).</dd>
</dlentry>
</dl>
</p>
</section>
<section>
<title>APPLICATION CAPACITY METRICS</title>
<dl>
<dlentry id="app_block_alloc_space">
<dt>Block Allocated Space (GiB)</dt>
<dd>The total amount of block storage that is used by the application.</dd>
</dlentry>
<dlentry id="app_file_alloc_space">
<dt>File Allocated Space (GiB)</dt>
<dd>The total amount of file storage that is used by the application.</dd>
</dlentry>
</dl>
</section>
<section>
<title>IMAGE MAPS</title>
<p>
<note id="image_map_tip" type="tip">To learn more, click an element in the image.</note>
</p>
</section>
<section>
<title>Thresholds</title>
<p>
<table>
<tgroup cols="2">
<colspec colname="col1" colwidth="69*"/>
<colspec colname="col2" colwidth="163*"/>
<thead>
<row>
<entry align="center" colname="col1">Threshold</entry>
<entry align="center" colname="col2">Explanation</entry>
</row>
</thead>
<tbody>
<row>
<entry colname="col1">
<p>Back-end Read Queue Time Threshold</p>
</entry>
<entry colname="col2">
<p id="p_backend_read_queue_time_kibop_ss_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_backend_read_queue_time_kibop"/> Specify
a threshold to receive alerts about potential performance problems on an MDisk. Slow response times
might be caused by a hardware or software-related problem, or if the workload on an MDisk is too
high.</p>
<p id="p_backend_read_queue_time_kibop_ss_2">
<note type="tip">A queue algorithm on a storage system determines the number of concurrent I/O
operations that are sent to an MDisk. If there is any queuing (other than during a backup process),
then performance might be improved if you resolve the queuing issue.</note>
</p>
<p id="p_backend_read_queue_time_kibop_ss_3">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 3 milliseconds per operation</li>
<li>Critical boundary: greater than 5 milliseconds per operation</li>
</ul>To reduce the number of unnecessary alerts, you can set a filter to ignore boundary violations
if the Back-end Read I/O Rate is less than a specified amount.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Back-end Write Queue Time Threshold</p>
</entry>
<entry colname="col2">
<p id="p_backend_write_queue_time_kibop_ss_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_backend_write_queue_time_kibop"/>
Specify a threshold to receive alerts about potential performance problems on an MDisk. Slow
response times might be caused by a hardware or software-related problem, or if the workload on an
MDisk is too high.</p>
<p id="p_backend_write_queue_time_kibop_ss_2">
<note type="tip">A queue algorithm on a storage system determines the number of concurrent I/O
operations that are sent to an MDisk. If there is any queuing (other than during a backup process),
then performance might be improved if you resolve the queuing issue.</note>
</p>
<p id="p_backend_write_queue_time_kibop_ss_3">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 3 milliseconds per operation</li>
<li>Critical boundary: greater than 5 milliseconds per operation</li>
</ul>To reduce the number of unnecessary alerts, you can set a filter to ignore boundary violations
if the Back-end Read I/O Rate is less than a specified amount.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Back-end Read Response Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_backend_read_response_time_msop_1"/></p>
<p id="p_backend_read_response_time_msop_ss_1">Specify a threshold to receive alerts about potential
performance problems on a storage system MDisk or array. Slow response times might be caused by a
hardware or software-related problem, or if the workload on an MDisk or array is too high.</p>
<p id="p_backend_read_response_time_msop_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 25 milliseconds per operation</li>
<li>Critical boundary: greater than 35 milliseconds per operation</li>
</ul>To reduce the number of unnecessary alerts, you can set a filter to ignore boundary violations
if the Back-end Read I/O Rate is less than a specified amount.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Back-end Write Response Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_backend_write_response_time_msop_1"/>
</p>
<p id="p_backend_write_response_time_msop_ss_1">Specify a threshold to receive alerts about
potential performance problems on a storage system MDisk or array. Slow response times might be
caused by a hardware or software-related problem, or if the workload on an MDisk or array is too
high.</p>
<p id="p_backend_write_response_time_msop_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 80 milliseconds per operation</li>
<li>Critical boundary: greater than 120 milliseconds per operation</li>
</ul>To reduce the number of unnecessary alerts, you can set a filter to ignore boundary violations
if the Back-end Write I/O Rate is less than a specified amount.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Cache Holding Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_cache_holding_time_s"/></p>
<p id="p_cache_holding_time_ss_1">Specify a threshold to receive alerts about potential performance
problems on a node. Slow response times might occur if the amount of cache that is installed on a
node is insufficient to manage its workload, or if the workload on a node is too high.</p>
<p id="p_cache_holding_time_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: less than 60 seconds</li>
<li>Critical boundary: less than 30 seconds</li>
</ul>
</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Class-3 Receive Timeout Frame Rate Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_class_3_receive_timeout_frame_rate_1"
/></p>
<p id="p_class_3_receive_timeout_frame_rate_sw">Specify a threshold to receive alerts about
conditions on a switch port that might slow the performance of the resources to which those ports
are connected. This threshold is available only for Brocade switches.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Class-3 Send Timeout Frame Rate Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_class_3_send_timeout_frame_rate_1"/></p>
<p id="p_class_3_send_timeout_frame_rate_sw">Specify a threshold to receive alerts about conditions
on a switch port that might slow the performance of the resources to which those ports are
connected. This threshold is available only for Brocade switches.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>CPU Utilization Threshold</p>
</entry>
<entry colname="col2">
<p id="p_processor_utilization_percentage_ss_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_processor_utilization_percentage"/>
Specify a threshold to receive alerts about potential performance problems on the nodes in a storage
system. Slow response times might occur if a node is experiencing a hardware or software-related
problem that causes an increase in CPU usage, or if the workload on the node is too high.</p>
<p id="p_processor_utilization_percentage_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than .01 counts per second</li>
<li>Critical boundary: greater than .033 counts per second</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>CRC Error Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p id="p_crc_error_rate_counts_ss_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_crc_error_rate_counts"/> CRC errors
suggest accidental modification of a frame's data during transit, and can be used as an indicator of
the relative quality of a port's connection. </p>
<p id="p_crc_error_rate_counts_ss_2">Specify a threshold to receive alerts about conditions on a
port that might slow the performance of the resources to which those ports are connected. This
threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than .01 counts per second</li>
<li>Critical boundary: greater than .033 counts per second</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Credit Recovery Link Reset Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_credit_recovery_link_reset_rate"
/></p>
<p id="p_credit_recovery_link_reset_rate_sw">Specify a threshold to receive alerts about conditions
on a port that might slow the performance of the resources to which those ports are connected. This
threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than .02 counts per second</li>
<li>Critical boundary: greater than 0.04 counts per second</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Discarded Class-3 Frame Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_discarded_class_3_frame_rate_counts_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_discarded_class_3_frame_rate_counts"/>
Frames can be discarded due to many reasons, such as invalid addressing of header contents or a
timeout that is caused by lack of buffer credits. Specify a threshold to receive alerts about
potential hardware or configuration problems on a switch port or for the connected fabric.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Discarded Frame Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_discarded_frame_rate_counts_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_discarded_frame_rate_counts"/> Frames
can be discarded due to many reasons, such as invalid addressing of header contents or a timeout
that is caused by lack of buffer credits. Specify a threshold to receive alerts about potential
hardware or configuration problems on a switch port or for the connected fabric.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Disk Utilization Percentage Threshold</p>
</entry>
<entry colname="col2">
<p id="p_disk_utilization_percentage_ss_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_disk_utilization_percentage_1"/> Specify
a threshold to receive alerts about potential performance problems on a storage system array. Slow
response times might occur if an array is experiencing a hardware or software-related problem, or if
the workload on the array is too high.</p>
<p id="p_disk_utilization_percentage_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 50%</li>
<li>Critical boundary: greater than 80%</li>
</ul></p>
<note id="p_disk_utilization_percentage_ss_3" othertype="Tips" type="other">
<ul>
<li>For <keyword conref="fqz0_entities.dita#fqz0_entities/kds6000"/> and <keyword
conref="fqz0_entities.dita#fqz0_entities/kds8000"/> storage systems, this threshold applies only to
those ranks that are the only ranks in their associated extent pool.</li>
<li>Some highly sequential workloads such as batch or backup processing might continually exceed the
threshold because they drive the arrays to high utilization percentages. For these types of
workloads, a high utilization indicates that the work is being performed very efficiently and is not
a cause for concern. If this situation occurs for your workload, consider enabling the Sequential
I/O Percentage filter. Use this filter to ignore any violations of the Disk Utilization Percentage
threshold for highly sequential workloads.</li>
</ul>
</note>
</entry>
</row>
<row>
<entry colname="col1">
<p>Error Frame Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p id="p_error_frame_rate_counts_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_error_frame_rate_counts"/> Specify a
threshold to receive alerts about conditions on a switch port or storage system port that might slow
the performance of the resources to which those ports are connected.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Extreme I/O Concurrency Percentage Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_extreme_io_concurrency_percentage"/></p>
<p id="p_extreme_io_concurrency_percentage_ss">Specify a threshold to receive alerts when the
physical capability of a port on the <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> is
being exceeded. If a port reaches its maximum number of open exchanges, it responds to any
additional I/O requests with a 'Busy' status. When a "Busy" status is returned by the port,
typically the host server needs to redrive the I/O, which negatively affects the performance of the
I/O for the host.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>I/O Busy Percentage Threshold</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_io_busy_percentage"/></p>
<p id="p_io_busy_percentage_ss"> Specify a threshold to receive alerts when the physical capability
of a port on the <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> is being exceeded.
When a "Busy" status is returned by a port, typically the requesting host server needs to redrive
the I/O, which negatively affects the performance of the I/O for the host.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>I/O Overrun Percentage Threshold</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_io_overrun_percentage"/></p>
<p id="p_io_overrun_percentage_ss">Specify a threshold to receive alerts when the physical
capability of a port on the <keyword conref="fqz0_entities.dita#fqz0_entities/kds8000"/> is being
exceeded. When an I/O operation is discarded, typically the requesting host server needs to redrive
the I/O, which negatively affects the performance of the I/O for the host.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Invalid Link Transmission Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p id="p_invalid_link_transmission_rate_cnts_ss_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_invalid_link_transmission_rate_cnts"/>
Transmission words are primitive elements that are used by the Fibre Channel protocol for
transmission of data. A frame consists of 10 - 537 transmission words.</p>
<p id="p_invalid_link_transmission_rate_cnts_ss_2">Specify a threshold to receive alerts about
potential hardware or configuration problems on a switch port, a storage system port, or for the
connected port or cable. This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than .01 counts per second</li>
<li>Critical boundary: greater than .033 counts per second</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Invalid Transmission Word Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p id="p_invalid_transmission_word_rate_counts_ss_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_invalid_transmission_word_rate_counts"/>
Transmission words are primitive elements that are used by the Fibre Channel protocol for
transmission of data. A frame consists of 10 - 537 transmission words.</p>
<p id="p_invalid_transmission_word_rate_counts_ss_2">Specify a threshold to receive alerts about
potential hardware or configuration problems on a switch port, storage system port, for the
connected port or cable. This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than .01 counts per second</li>
<li>Critical boundary: greater than .033 counts per second</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Link Failure Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_link_failure_rate_counts_ss"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_link_failure_rate_counts"/> Specify a
threshold to receive alerts about potential hardware or configuration problems on a switch port or
storage system port, the connected port, or for fibre link or GBICs.</p>
<p id="p_link_failure_rate_counts_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than .01 counts per second</li>
<li>Critical boundary: greater than .033 counts per second</li>
</ul>
</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Link Quality Percentage Threshold</p>
</entry>
<entry colname="col2">
<p id="p_link_quality_percentage_sw">The percentage is based on whether the port is an expansion
port (E_port) or a fabric port (F_port), and on the numbers and types of errors that are detected by
the port. Specify a threshold to receive alerts about the estimated relative quality of the
connection of a switch port or the connected port or cable.</p>
<p id="p_link_quality_percentage_sw_2">The default critical stress boundary for this threshold is
set to greater than 75%.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Link Reset Received Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p id="p_link_reset_received_rate_counts_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_link_reset_received_rate_counts"/>
Specify a threshold to receive alerts about potential hardware or configuration problems on a switch
port, a storage system port, or for the connected fabric.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Link Reset Transmitted Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_link_reset_transmitted_rate_counts_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_link_reset_transmitted_rate_counts"/>
Specify a threshold to receive alerts about potential hardware or configuration problems on a switch
port or for the connected fabric.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Loss of Signal Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_loss_of_signal_rate_counts"/></p>
<p id="p_loss_of_signal_rate_counts_sw">Specify a threshold to receive alerts about potential
hardware problems on a switch port, a storage system port, or for the connected port. <note
type="tip">If the threshold that you set is violated, it might mean that the connected port was
reset, perhaps because the host that contains the HBA was rebooted. Therefore, isolated errors can
be ignored in most cases, but a rate that remains consistently high over time might need to be
investigated.</note></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Loss of Sync Rate Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_loss_of_sync_rate_counts"/></p>
<p id="p_loss_of_sync_rate_counts_sw">Specify a threshold to receive alerts about potential hardware
or configuration problems on a switch port, a storage system port, or for the connected port.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Non-preferred Node Usage Percentage Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_nonpreferred_node_usage_percentage"
/></p>
<p id="p_nonpreferred_node_usage_percentage_ss">Under normal conditions, the non-preferred node
usage percentage should be zero for all I/O groups. Specify a threshold to receive alerts about the
following issues: <ul>
<li>Potential configuration problems for some of the host servers that use storage from the storage
system.</li>
<li>Data paths from the host servers to the storage system through the fabric are not
operational.</li>
</ul></p>
<p id="p_nonpreferred_node_usage_percentage_ss_2">To reduce the number of unnecessary alerts, you
can set a filter to ignore boundary violations if the Total I/O Rate of an I/O group is less than a
specified amount. </p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Overall Back-end Response Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_overall_backend_response_time_msop_1"
/></p>
<p id="p_overall_backend_response_time_msop_ss_1">Specify a threshold to receive alerts about
potential performance problems on an MDisk. Slow response times might occur if an MDisk is
experiencing a hardware or software-related problem, or if the workload on the MDisk is too
high.</p>
<p id="p_overall_backend_response_time_msop_ss_2">To reduce the number of unnecessary alerts, you
can set a filter to ignore boundary violations if the Total Back-end I/O Rate is less than a
specified amount.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Overall Port Response Time Threshold</p>
</entry>
<entry colname="col2">
<p id="p_overall_port_response_time_msop_ss"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_overall_port_response_time_msop"/>
Specify a threshold to receive alerts about potential performance problems on a storage system port.
Slow response times might occur if a port is experiencing a hardware or software-related problem, or
if the workload on the port is too high.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port Congestion Index Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_congestion_index"/></p>
<p id="p_port_congestion_index_sw_1">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 80 counts per second</li>
<li>Critical boundary: greater than 100 counts per second</li>
</ul>
</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port Receive Bandwidth Percentage Threshold</p>
<p>switches, storage systems</p>
</entry>
<entry colname="col2">
<p id="p_port_receive_bandwidth_percentage_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_receive_bandwidth_percentage"/>
Because fibre channel protocols are full-duplex, the sum of the send and receive bandwidth
percentages can theoretically reach a maximum of 200%.</p>
<p id="p_port_receive_bandwidth_percentage_sw_2">Specify a threshold to receive alerts about
potential performance problems on a port. Slow response times might occur if the workload for a port
is too high, if it is experiencing a hardware or software-related problem, or if there are
congestion or configuration issues in the fabric. This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 75%</li>
<li>Critical boundary: greater than 85%</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port Receive Utilization Percentage Threshold</p>
</entry>
<entry colname="col2">
<p id="p_port_receive_utilization_percentage_ss"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_receive_utilization_percentage"/>
Specify a threshold to receive alerts about potential performance problems on a storage system port.
Slow response times might occur if the workload for a port is too high, if it is experiencing a
hardware or software-related problem, or if there are congestion or configuration issues in the
fabric.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port Send Bandwidth Percentage Threshold</p>
</entry>
<entry colname="col2">
<p id="p_port_send_bandwidth_percentage_sw_1"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_send_bandwidth_percentage"/>
Because fibre channel protocols are full-duplex, the sum of the send and receive bandwidth
percentages can theoretically reach a maximum of 200%.</p>
<p id="p_port_send_bandwidth_percentage_sw_2">Specify a threshold to receive alerts about potential
performance problems on a switch port or storage system port. Slow response times might occur if the
workload for a port is too high, if it is experiencing a hardware or software-related problem, or if
there are congestion or configuration issues in the fabric. This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 75%</li>
<li>Critical boundary: greater than 85%</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port Send Utilization Percentage Threshold</p>
</entry>
<entry colname="col2">
<p id="p_port_send_utilization_percentage_ss"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_send_utilization_percentage"/>
Specify a threshold to receive alerts about potential performance problems on a storage system port.
Slow response times might occur if the workload for a port is too high, if it is experiencing a
hardware or software-related problem, or if there are congestion or configuration issues in the
fabric.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port State Change Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_port_state_change_rate_1_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_state_change_rate_1"/> You can also
use this threshold to help identify flapping ports.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port to Local Node Receive Queue Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_to_local_node_receive_queue_time_msop"
/></p>
<p id="p_port_to_local_node_receive_queue_time_msop_ss_1">Specify a threshold to receive alerts
about potential performance problems on a storage system node. A violation of this threshold means
that a node is waiting too long to receive data from other nodes and suggests that there is
congestion around the ports on the fabric.</p>
<p id="p_port_to_local_node_receive_queue_time_msop_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 1 millisecond per operation</li>
<li>Critical boundary: greater than 2 millisecond per operation</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port to Local Node Send Queue Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_to_local_node_send_queue_time_msop"
/></p>
<p id="p_port_to_local_node_send_queue_time_msop_ss_1">Specify a threshold to receive alerts about
potential performance problems on a storage system node. A violation of this threshold means that a
node is waiting too long to send data to other nodes and suggests that there is congestion around
the ports on the fabric.</p>
<p id="p_port_to_local_node_send_queue_time_msop_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 1 millisecond per operation</li>
<li>Critical boundary: greater than 2 milliseconds per operation</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port to Local Node Receive Response Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_to_local_node_receive_response_time_msop"
/></p>
<p id="p_port_to_local_node_receive_response_time_msop_ss_1">Specify a threshold to receive alerts
about potential performance problems on a storage system node. A violation of this threshold means
that data is taking too long to be sent between nodes on the fabric, and suggests that there is
either congestion around the FC ports, or there is an internal microcode problem on the <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>.</p>
<p id="p_port_to_local_node_receive_response_time_msop_ss_2">This threshold has a default critical
boundary of greater than 1 milliseconds per operation</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Port to Local Node Send Response Time Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_port_to_local_node_send_response_time_msop"
/></p>
<p id="p_port_to_local_node_send_response_time_msop_ss_1">Specify a threshold to receive alerts
about potential performance problems on a storage system node. A violation of this threshold means
that data is taking too long to be sent between nodes on the fabric, and suggests that there is
either congestion around the FC ports, or there is an internal microcode problem on the <keyword
conref="fqz0_entities.dita#fqz0_entities/ksvc"/>.</p>
<p id="p_port_to_local_node_send_response_time_msop_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 1 millisecond per operation</li>
<li>Critical boundary: greater than 3 milliseconds per operation</li>
</ul></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Primitive Sequence Protocol Error Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_primitive_sequence_protocol_error_rate_counts_1_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_primitive_sequence_protocol_error_rate_counts_1"
/><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_primitive_sequence_protocol_error_rate_counts_2"
/> Specify a threshold to receive alerts about conditions on a switch port or storage system that
might slow the performance of the resources to which those ports are connected.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>RDY Priority Override Rate Threshold</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_rdy_priority_override_rate_1"/>
The threshold is available only for Brocade switches.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Total Back-end Data Rate Threshold</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_total_backend_data_rate_mibs"
/></p>
<p id="p_total_backend_data_rate_mibs_ss">Specify a threshold to receive alerts about storage system
ranks, RAID arrays, MDisks, or pools (MDisk groups) that might have too high a workload. </p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Total Back-end I/O Rate Threshold</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_total_backend_io_rate_opss"/></p>
<p id="ph_total_backend_io_rate_opss">Specify a threshold to receive alerts about storage system
ranks, RAID arrays, MDisks, pools, nodes, or I/O groups that might have too high a workload. </p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Total Data Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_total_data_rate_mibs_ss"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_total_data_rate_mibs"/> Specify a
threshold to receive alerts about storage system volumes, RAID arrays, pools, nodes, I/O groups, or
host connections that might have too high a workload.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Total I/O Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_total_normal_io_rate_opss_ss"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_total_normal_io_rate_opss"/> Specify a
threshold to receive alerts about storage system volumes, RAID arrays, pools, nodes, or host
connections that might have too high a workload.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Total Port Data Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_total_port_data_rate_mibs_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_total_port_data_rate_mibs"/> Specify a
threshold to receive alerts about switch ports or storage system ports that might have too high a
workload.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Total Port Frame Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_total_port_frame_rate_frames_sw"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_total_port_frame_rate_frames"/> Specify
a threshold to receive alerts about switch ports or storage system ports that might have too high a
workload.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Total Port I/O Rate Threshold</p>
</entry>
<entry colname="col2">
<p id="p_total_port_io_rate_opss_ss"><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_total_port_io_rate_opss"/> Specify a
threshold to receive alerts about storage system ports that might have too high a workload.</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Write-cache Delay Percentage Threshold</p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_write_cache_delay_percentage_1"/>
<ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_write_cache_delay_percentage_2"
/></p>
<p id="p_write_cache_delay_percentage_ss">Specify a threshold to receive alerts about potential
performance problems on a storage system node. For example, the following issues might be causing
performance problems: <ul>
<li>The amount of cache that is installed on the node is insufficient to manage its workload.</li>
<li>The node that is experiencing a hardware or software-related problem that causes cache slots to
be occupied longer than necessary. This problem occurs when there is a drop in write performance, or
the node is being overwhelmed with too high a write workload.</li>
</ul></p>
<p id="p_write_cache_delay_percentage_ss_2">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 3%</li>
<li>Critical boundary: greater than 10%</li>
</ul>To reduce the number of unnecessary alerts, you can set a filter to ignore boundary violations
if the Back-end Read I/O Rate is less than a specified amount. The pre-populated filter value is 10
I/Os per second. </p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Zero Buffer Credit Percentage Threshold<sup>3</sup></p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_zero_buffer_to_buffer_credit_percent"
/></p>
<p id="p_zero_buffer_to_buffer_credit_percent_ss_1">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 25%</li>
<li>Critical boundary: greater than 66%</li>
</ul>
</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Zero Buffer Credit Rate Threshold<sup>2</sup></p>
</entry>
<entry colname="col2">
<p><ph conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_zero_buffer_credit_rate_1"/></p>
<p id="p_zero_buffer_credit_rate_ss_1">This threshold has the following default boundaries:<ul>
<li>Warning boundary: greater than 75,000,000 milliseconds</li>
<li>Critical boundary: greater than 198,000,000 milliseconds</li>
</ul>
</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Zero Buffer Credit Timer Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_zero_buffer_to_buffer_credit_timer_microseconds_1"
/><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_zero_buffer_to_buffer_credit_timer_microseconds_2"
/></p>
<p id="p_zero_buffer_to_buffer_credit_timer_microseconds_ss_1">This threshold has the following
default boundaries:<ul>
<li>Warning boundary: greater than 75,000,000 milliseconds</li>
<li>Critical boundary: greater than 198,000,000 milliseconds</li>
</ul>
</p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Zero Receive Buffer Credit Percentage Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_zero_receive_buffer_credit_percentage"
/></p>
</entry>
</row>
<row>
<entry colname="col1">
<p>Zero Send Buffer Credit Percentage Threshold</p>
</entry>
<entry colname="col2">
<p><ph
conref="fqz0_reports_entities.dita#fqz0_reports_entities/ph_zero_send_buffer_credit_percentage"
/></p>
</entry>
</row>
</tbody>
</tgroup>
</table>
</p>
</section>
<section>
<title>Alerting</title>
<simpletable id="alerts_definition_status_icons">
<sthead>
<stentry>Icon</stentry>
<stentry>Status of the alert definition</stentry>
</sthead>
<strow>
<stentry><image href="alerts_enable_disable.jpg" placement="inline">
<alt>Alert is disabled icon</alt></image></stentry>
<stentry>The alert definition is disabled.</stentry>
</strow>
<strow>
<stentry><image href="alerts_enabled.jpg" placement="inline"><alt>Alert is enabled
icon</alt></image></stentry>
<stentry>The alert definition is enabled.</stentry>
</strow>
</simpletable>
<ul>
<li id="alerts_tab_actions">To view a list of actions for managing alerts, see <xref
href="tpch_r_alert_log.dita#tpch_r_alert_log/alert_actions">alert actions</xref>.</li>
<li id="alerts_tab_information">To view descriptions of the information that is shown for alerts,
see <xref href="tpch_r_alert_log.dita#tpch_r_alert_log/alert_columns">information about
alerts</xref>.</li>
</ul>
</section>
<section>
<title>Reusable definitions for Capacity</title>
<p>
<dl>
<dlentry id="ssysPoolOverheadCapacity">
<dt>Overhead Capacity (GiB)</dt>
<dd>The amount of usable capacity that is occupied by metadata in a pool or system and other data
that is used for system operation.</dd>
</dlentry>
</dl>
</p>
</section>
</refbody>

</reference><?tm 1617467417976 113 XIV||Windows||UNIX||Storwize||Solid||Power||IBM Spectrum||IBM FlashCore||IBM||GPFS||FlashSystem||FlashCopy||Easy Tier||DS8000||Current||400 ?>
